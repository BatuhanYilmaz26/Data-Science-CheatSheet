{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bizarre-shakespeare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clinical-association",
   "metadata": {},
   "source": [
    "#### Some of the problems you could run into when training a deep DNN: \n",
    "- You may be faced with the tricky **vanishing gradients problem** or the related **exploding gradients problem**. \n",
    "    - **This is when the gradients grow smaller and smaller, or larger and larger, when flowing backward through the DNN during training.** \n",
    "    - Both of these problems make lower layers very hard to train.\n",
    "- You might not have enough training data for such a large network, or it might be too costly to label.\n",
    "- Training may be extremely slow.\n",
    "- A model with millions of parameters would severely risk overfitting the training set, especially if there are not enough training instances or if they are too noisy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-generic",
   "metadata": {},
   "source": [
    "## The Vanishing/Exploding Gradients Problems\n",
    "- The backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient along the way. \n",
    "    - Once the algorithm has computed the gradient of the cost function with regard to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step.\n",
    "- **Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers.** \n",
    "    - As a result, the Gradient Descent update leaves the lower layers’ connection weights virtually unchanged, and training never converges to a good solution.\n",
    "    - **We call this the vanishing gradients problem.**\n",
    "- In some cases, the opposite can happen: **the gradients can grow bigger and bigger until layers get insanely large weight updates and the algorithm diverges.**\n",
    "    - This is the **exploding gradients problem**, which surfaces in recurrent neural networks. \n",
    "- More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds.\n",
    "***\n",
    "- This unfortunate behavior was empirically observed long ago, and it was one of the reasons deep neural networks were mostly abandoned in the early 2000s. \n",
    "- It wasn’t clear what caused the gradients to be so unstable when training a DNN, but some light was shed in a 2010 paper by Xavier Glorot and Yoshua Bengio. \n",
    "    - The authors found a few suspects, including the combination of the popular logistic sigmoid activation function and the weight initialization technique that was most popular at the time (i.e., a normal distribution with a mean of 0 and a standard deviation of 1).\n",
    "    - In short, they showed that with this activation function and this initialization scheme, the variance of the outputs of each layer is much greater than the variance of its inputs.\n",
    "    - Going forward in the network, the variance keeps increasing after each layer until the activation function saturates at the top layers. \n",
    "    - This saturation is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daily-edgar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "perceived-frost",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "violent-burning",
   "metadata": {},
   "source": [
    "- Looking at the logistic activation function above, you can see that when inputs become large (negative or positive), the function saturates at 0 or 1, with a derivative extremely close to 0. \n",
    "- Thus, when backpropagation kicks in it has virtually no gradient to propagate back through the network; and what little gradient exists keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-smart",
   "metadata": {},
   "source": [
    "## Glorot and He Initialization\n",
    "- In their paper, Glorot and Bengio propose a way to significantly alleviate the unstable gradients problem. \n",
    "    - They point out that we need the signal to flow properly in both directions: in the forward direction when making predictions, and in the reverse direction when backpropagating gradients.\n",
    "    - We don’t want the signal to die out, nor do we want it to explode and saturate. \n",
    "    - For the signal to flow properly, the authors argue that we need the variance of the outputs of each layer to be equal to the variance of its inputs, and we need the gradients to have equal variance before and after flowing through a layer in the reverse direction.\n",
    "- It is actually not possible to guarantee both unless the layer has an equal number of inputs and neurons (these numbers are called the fan-in and fan-out of the layer), but Glorot and Bengio proposed a good compromise that has proven to work very well in practice: the connection weights of each layer must be initialized randomly as described in Equation 11-1, where. <img src=\"1.png\">\n",
    "    - This initialization strategy is called **Xavier initialization** or **Glorot initialization**, after the paper’s first author.\n",
    "<img src=\"11-1.png\">\n",
    "\n",
    "- If you replace fan with $fan_in$ in Equation 11-1, you get an initialization strategy that Yann LeCun proposed in the 1990s. He called it LeCun initialization. \n",
    "- Genevieve Orr and Klaus-Robert Müller even recommended it in their 1998 book Neural Networks: Tricks of the Trade (Springer).\n",
    "- LeCun initialization is equivalent to Glorot initialization when $fan_in$ = $fan_out$ . \n",
    "    - It took over a decade for researchers to realize how important this trick is. Using Glorot initialization can speed up training considerably, and it is one of the tricks that led to the success of Deep Learning.\n",
    "- Some papers have provided similar strategies for different activation functions. \n",
    "    - These strategies differ only by the scale of the variance and whether they use $fan_avg$ or $fan_in$ , as shown in Table 11-1.\n",
    "- **The initialization strategy for the ReLU activation function** (and its variants, including the ELU activation described shortly) is sometimes called **He initialization**, after the paper’s first author.\n",
    "<img src=\"t1.png\">\n",
    "\n",
    "- By default, Keras uses **Glorot initialization** with a uniform distribution.\n",
    "    - When creating a layer, you can change this to He initialization by setting kernel_initializer=\"he_uniform\" or kernel_initializer=\"he_normal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "governing-feelings",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Constant',\n",
       " 'GlorotNormal',\n",
       " 'GlorotUniform',\n",
       " 'HeNormal',\n",
       " 'HeUniform',\n",
       " 'Identity',\n",
       " 'Initializer',\n",
       " 'LecunNormal',\n",
       " 'LecunUniform',\n",
       " 'Ones',\n",
       " 'Orthogonal',\n",
       " 'RandomNormal',\n",
       " 'RandomUniform',\n",
       " 'TruncatedNormal',\n",
       " 'VarianceScaling',\n",
       " 'Zeros',\n",
       " 'constant',\n",
       " 'deserialize',\n",
       " 'get',\n",
       " 'glorot_normal',\n",
       " 'glorot_uniform',\n",
       " 'he_normal',\n",
       " 'he_uniform',\n",
       " 'identity',\n",
       " 'lecun_normal',\n",
       " 'lecun_uniform',\n",
       " 'ones',\n",
       " 'orthogonal',\n",
       " 'random_normal',\n",
       " 'random_uniform',\n",
       " 'serialize',\n",
       " 'truncated_normal',\n",
       " 'variance_scaling',\n",
       " 'zeros']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[name for name in dir(keras.initializers) if not name.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "competent-conservation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1b9101ae9c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=\"he_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eleven-pioneer",
   "metadata": {},
   "source": [
    "If you want He initialization with a uniform distribution but based on $fan_avg$ rather than $fan_in$ , you can use the VarianceScaling initializer like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "animal-african",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1b91133b948>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = keras.initializers.VarianceScaling(scale=2., mode=\"fan_avg\", distribution=\"uniform\")\n",
    "keras.layers.Dense(10, activation=\"relu\", kernel_initializer=init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-composer",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions\n",
    "- One of the insights in the 2010 paper by Glorot and Bengio was that the problems with unstable gradients were in part due to a poor choice of activation function. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice.\n",
    "- But it turns out that other activation functions behave much better in deep neural networks—in particular, the **ReLU** activation function, mostly because it does not saturate for positive values (and because it is fast to compute).\n",
    "- Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively “die,” meaning they stop outputting anything other than 0. \n",
    "    - In some cases, you may find that half of your network’s neurons are dead, especially if you used a large learning rate.\n",
    "    - A neuron dies when its weights get tweaked in such a way that the weighted sum of its inputs are negative for all instances in the training set. \n",
    "    - When this happens, it just keeps outputting zeros, and Gradient Descent does not affect it anymore because the gradient of the ReLU function is zero when its input is negative.\n",
    "    - To solve this problem, you may want to use a variant of the ReLU function, such as the **leaky ReLU**.\n",
    "    \n",
    "### Leaky ReLU\n",
    "- The hyperparameter α(alpha) defines how much the function “leaks”: it is the slope of the function for z < 0 and is typically set to 0.01. This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "excess-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rapid-productivity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWsAAAEMCAYAAAACt5eaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAn2UlEQVR4nO3deXxU5b3H8c+PBDBhRy5xo8RarFsFNVJ3g4p4Qa0tLoii1GLUilpfuHAtFC7KVRQXioiKIIKAa33VrbZSGyotRSJiFauoCIIiixBIICyZPPePZ0KGkJDJMnNm+b5fr3lxcubknO+cOfPjyTPPOcecc4iISGJrFnQAERGpm4q1iEgSULEWEUkCKtYiIklAxVpEJAmoWIuIJAEV6yRiZs7MLg46RzIzs8FmVhqnbcXl/TKzU83s32a208wKY729OrLkhl93XpA5UpGKdRMxs+lm9nrQOerDzEaHP1jOzCrM7Fszm2VmXeq5nkIze7SW51aY2W21bPvjhmaPMldNxfJ54IdNvJ3a3vsDgdeaclu1mAB8CBwG/CIO2wNqfd9X4V/3knjlSBcq1vIZ/sN1CHAZ8BPghUATxZBzrsw5ty5O2/rOObcjDpv6EfCOc26Vc25jHLZXK+dcKPy6y4PMkYpUrOPEzI4yszfMrMTM1pnZHDM7IOL5E83sL2a2wcy2mNl8Mzu5jnXeGV7+zPDvXFzt+d5mtsvMcvaxmvLwh+tb59y7wBTgJDNrG7GeC8zsfTPbbmZfmdlYM2vRwF0RFTPLMLOp4e2VmdnnZnaHmTWrttzVZvaRme0ws7VmNj08f0V4kRfDLewV4fm7u0HM7PDwcz+pts6C8H5tXlcOMxsNXA30i/grJT/83B4tezP7iZnNDa9nY7hF3i7i+elm9rqZ3WJm35jZJjN72syya9lHuWbmgHbAtPD2BptZfni6U/VlK7snIpY528wWmtk2Mysys+OrbeMkM3vHzLaa2WYz+6uZHRTez2cCN0a87tyaukHM7IzwNraH36OHI4+fcAv9MTP7v/B+X2dm46u/1+lOOyMOzOxA4O/Ax0BP4BygNfBqxAHZBpgJnB5eZgnwZuQHLmJ9ZmbjgZuAM51z84A5wDXVFr0GeN05tzbKnAfg/4wOhR+YWR9gFvAocHR4nRcD/xfNOhuhGfANcClwJPBb4C7glxF5rwOeAJ4GjgX6AkvDT58Y/vda/F8OlT/v5pxbBhQBV1R76grgeefcrihyjMf/JTI3vJ0DgX9W31a44L4FlOLf358DpwDTqi16OnAM/hi5LLzcLdXXF1bZ5bAN+E14+vlalq3NvcBw4Hjge2CWmVk4c3fgb8AXwKnASeHXmhnOtAC/7ytf96oaXvfBwJ+AD4DjgF8Bl4e3G+kKoBy/T4aGX89l9Xwtqc05p0cTPIDp+MJY03NjgL9Wm9cBcEDPWn7HgDXAlRHzHP4AfhpYBuRGPJeHP9gPjlh/GXD+PjKPxhflUvwH3oUfEyKW+TswstrvXRT+HQv/XAg8Wss2VgC31bLtj+u5j+8D5kb8vBq4bx/LO+DiavMGA6URP98CrIx4LV2ACuDkeuSo8b2P3D7+P43NQJuI5/PDy/woYj2rgMyIZaZEbquWPKXA4BrW2yliXm54Xl61ZfpELHNqeN4h4Z9nAf/ax3b3et9r2M5YfLFvVu092AFkR6xnQbX1vA081ZjPZKo91LKOjxOAM8ystPJBVSvkMAAz62xmT5jZMjPbDJQAnYEfVFvXePwH7TTn3IrKmc65IuAj/J/kAAOBTfhWzb58CfTAtzx/CyzGtxwjs/+2WvbZQCvgAGLIzK4P/2m+PrzdWwnvDzPrDBwM/LWRm5kDHIRv0YLfb8udcwuiyVEPRwL/ds6VRMz7J/4/hqMi5n3i9uzv/RZ/HMTKv6tti4jtHUfj9++R+EJcETFvPtAC39deU47KLLF83UlHxTo+mgFv4Iti5KMbUDmK4Bl8wbwV/6dgD3zLsXrf8Nv4Itm3hu08RdWf59cA051zoTqy7XTOfeGcW+qc+z/8h2ZStez/Wy33seHs6+tYN8AWfJ9qde3xLc0amdllwCP41maf8HYfo2p/WBTbrpPzXzbOpaor5Ap8izLaHNEyfIuzxhgR07tqeK6+n9PKwhi5j5rXsmzk9ipzVG6vKfZxPF93SssMOkCaWIzv81zpfD9oTU4DbnbOvQFg/kvBA2tY7k3gD4S/OHPOPRPx3LPAA2Y2FN8HOaABWe8GPjOzic6598PZj3DOfdGAdYEfbXJCDfOPDz9Xm9OAhc653UPDzOywymnn3Foz+wY4G/8fWE12ARlRZHwWmGhmT+JHw/SPNkfYzii28wlwjZm1iWhdn4IvSP+JImN9VP4nemDEdI8GrGcxcNY+no/2dV9qZs0iWtenhX/3ywZkSlv6n6tptTWzHtUeufiWajvgeTP7qZn90MzOMbMnzaxN+HeXAVeaHzVyIvAc/oDei3PudeAS4HEzuypi/mbgReBB4O/Ouc/r+wKcc8uBV/FFG3x/+0AzG2Nmx5jZEWZ2sZndX+1XO9Xw2g8CHgb6mNnI8Gs72szGAifjW6y1WQYcb2b/bWbdzGwkfvRBpLHAb8zsVvMjO3qY2bCI51cAZ5vZAWbWYR/begXf8pwKvFdtv0WTYwVwjJn92Mw6mVlNrdhZwFZghvlRIWfgvxz9QyP+I6zNF/huttHh/XIuMKIB63kAOC58nHYPv74hZlbZBbQC6BkeAdKpltEbj+G7mR4zsyPNrB++z/9R59y2BmRKX0F3mqfKA/9nsqvh8VL4+W7AS/h+5DJ8q3Ii0CL8fHdgYfi5L4FB+NEjoyO2sccXZsAF4eWviph3Rni5q6LIPJoavuTDt/gccEr453OBd/FfQm7Bj6AYGrF8YS2vfXy139+IH3FQCJxRR7YW+OK5CSgOT/8OWFFtuV/hW287ge+AadX2z+f4FvaK8LzBRHzBGLHsjHDmm+qbA/gv4C/47xkckF/L+/UTfB9wWXh904F21Y6h16ttv8b3qNoye3zBGPEeLglvawHQj5q/YKz1S8jwvNPwXzKXhV//XODA8HOHh9dd+eV0bi3rOAN/bO8A1uL/A29Z7fip/kXlXvsi3R+V34BLigj3sT4BHOTUchFJGeqzThHhcby5+JEcU1SoRVKL+qxTxx3460NspKq/WURShLpBRESSgFrWIiJJIGZ91p06dXK5ubmxWn1Utm7dSqtWrQLNkCi0L7zPPvuMUCjEUUcdVffCaUDHRZWa9sW338KaNdC8ORx1FGTG4Vu+999/f4Nz7r+qz4/ZpnNzcykqKorV6qNSWFhIfn5+oBkShfaFl5+fT3FxceDHZqLQcVGl+r54913IzwczeOstOGtfpwc1ITNbWdN8dYOIiFSzaRNccQVUVMCdd8avUO+LirWISATn4NprYdUq6NkTxowJOpGnYi0iEuGpp+Dll6FNG5gzx/dXJwIVaxGRsP/8B265xU9Pngw/bNK7dTZOvYp1+EI2283s2VgFEhEJws6dzbj8cigrg0GDfJ91Iqlvy3oSsCgWQUREgvTkkz/kww/hRz+CSZPqXj7eoi7WZjYAf9Wtxt45QkQkobzxBrz88iFkZsLs2b6/OtFEVazN3+l6DDCsrmVFRJLJmjUweLCfHjsWTtzr1sqJIdqTYu4GpjrnVoVvfFwjMysACgBycnIoLCxsdMDGKC0tDTxDotC+8IqLiwmFQtoXYel+XFRUwB13HMuGDR3p0WM9eXlLSdTdUWexNrMewDn4m2fuk3PuSeBJgLy8PBf0mVE6O6uK9oXXvn17iouLtS/C0v24eOABeP996NQJRoz4nLPOyg86Uq2iaVnn46+T/HW4Vd0ayDCzo5xzx8cumohI7CxaBHfd5aenT4dWrWq8i17CiKbP+kngMKrubP04/k7dfWKWSkQkhkpK4PLLobwcbr4Z+vULOlHd6mxZh+84svuuI2ZWCmx3zq2v/bdERBLX0KHw5ZfQvTuMGxd0mujU+6p7zrnRMcghIhIXs2bBjBmQleVPJ99vv6ATRUenm4tI2li+HG64wU9PmABHHhlsnvpQsRaRtLBrl++nLimB/v1hyJCgE9WPirWIpIVRo+C996BLF5gyxd9UIJmoWItIynvnHbjvPmjWzPdZd+gQdKL6U7EWkZS2YQNceaW/qcDIkXD66UEnahgVaxFJWc7BNdf463+ceiqMGBF0ooZTsRaRlPXYY/Daa9C+ve/+iMfdyWNFxVpEUtJHH8Gw8HVCp0yBrl2DzdNYKtYiknK2bYMBA2DHDj9E7+KLg07UeCrWIpJyhg2DTz6BI46ARx4JOk3TULEWkZTyyivw+OPQooU/nbxVq6ATNQ0VaxFJGatWwa9+5afvvx969Ag0TpNSsRaRlBAK+buSb9oEffv6S5+mEhVrEUkJ994L8+ZBTg48/XTynU5eFxVrEUl6//wnjB7tp2fOhM6dA40TEyrWIpLUioth4EDfDXL77dC7d9CJYkPFWkSSlnNw/fWwciXk5cE99wSdKHZUrEUkaU2fDs8/74fnzZ7th+ulKhVrEUlKn30GN93kpx97DLp1CzZPrKlYi0jS2bHD3/Vl61bfXz1oUNCJYk/FWkSSzl13wQcfwKGHwuTJqTdMryYq1iKSVN56Cx56CDIyfD9127ZBJ4oPFWsRSRpr18LVV/vpu++Gk04KNk88qViLSFKoqPCFet066NUL7rgj6ETxpWItIknhkUfgz3+G/ff3ZylmZASdKL5UrEUk4S1eDMOH++mpU+Hgg4PNEwQVaxFJaKWlfpjerl1w443ws58FnSgYKtYiktBuvhmWLYNjjoEHHgg6TXBUrEUkYT3/vL/c6X77wXPPQVZW0ImCo2ItIglpxQooKPDTDz8MRx8daJzAqViLSMIpL/enkW/ZAhddBNddF3Si4KlYi0jC+d//hQUL/KiPp55Kj9PJ66JiLSIJZd48GDvWF+hnn/XjqkXFWkQSyMaNcOWV/qYCv/0t5OcHnShxqFiLSEJwDoYMgdWr4eSTYdSooBMlFhVrEUkITzwBr7zir6I3ezZkZgadKLGoWItI4JYuhVtv9dNPPAG5uYHGSUhRFWsze9bM1pjZFjNbZmZDYh1MRNJDWZk/nXz7dvjlL2HAgKATJaZoW9b3ArnOubbAhcA9ZnZC7GKJSLq4/Xb46CM4/HD4/e+DTpO4oirWzrmlzrkdlT+GH4fFLJWIpIVXX4VJk6B5c5gzB1q3DjpR4oq6C9/MHgMGA1nAB8CbNSxTABQA5OTkUFhY2CQhG6q0tDTwDIlC+8IrLi4mFAppX4QFeVysX9+CIUNOBJozZMgXbNmymiDflkT/jJhzLvqFzTKAk4F8YJxzbldty+bl5bmioqJGB2yMwsJC8jVQE9C+qJSfn09xcTFLliwJOkpCCOq4CIWgd2/429+gTx94801oFvBwh0T5jJjZ+865vOrz67V7nHMh59x84BDghqYKJyLp5f77faHu3BmeeSb4Qp0MGrqLMlGftYg0wMKFMHKkn37mGcjJCTZPsqizWJtZZzMbYGatzSzDzPoAlwPvxD6eiKSSLVv8ML1QyI+rPu+8oBMlj2i+YHT4Lo/H8cV9JfAb59wfYxlMRFKLc3DDDfDVV3DccXDvvUEnSi51Fmvn3HrgzDhkEZEUNnOmP408O9sP02vZMuhEyUXd+iISc1984W92CzBxIvz4x8HmSUYq1iISUzt3+n7q0lK49FJ/SrnUn4q1iMTUyJFQVARdu/qLNOmuLw2jYi0iMfP2235MdUaG769u3z7oRMlLxVpEYmL9erjqKj89ahScckqweZKdirWINDnnfN/0d9/BGWfAXXcFnSj5qViLSJObOBHeeAM6dPA3vc3ICDpR8lOxFpEmtWSJv0Y1wNSp0KVLoHFShoq1iDSZrVv9ML2dO+G66+DnPw86UepQsRaRJnPrrfDpp3DUUfDQQ0GnSS0q1iLSJF56CaZM8aeRP/ecP61cmo6KtYg02tdfw7XX+unx4+EnPwk2TypSsRaRRikvhyuugOJiuOCCqmuASNNSsRaRRhk7FubPhwMPhGnTdDp5rKhYi0iDzZ8PY8b4Av3ss9CpU9CJUpeKtYg0yKZNMHAgVFTAnXfCWWcFnSi1qViLSL05BwUFsGoV9OzpW9cSWyrWIlJvU6f6oXpt2vir6TVvHnSi1KdiLSL18p//wM03++nJk+Gww4LNky5UrEUkatu3+9PJy8pg0CA/ZE/iQ8VaRKI2fDh8+KFvTU+aFHSa9KJiLSJReeMNmDABMjP93cnbtAk6UXpRsRaROq1ZA4MH++mxY+HEEwONk5ZUrEVknyoq/O25NmyAc86B224LOlF6UrEWkX168EGYO9efnThjBjRT1QiEdruI1GrRoqr7J06f7q//IcFQsRaRGpWU+GF65eV+XHW/fkEnSm8q1iJSo6FD4csvoXt3GDcu6DSiYi0ie5k92/dPZ2X5YXr77Rd0IlGxFpE9LF8O11/vpydMgCOPDDaPeCrWIrLbrl2+n7qkBPr3hyFDgk4klVSsRWS3UaPgvfegSxd/81vd9SVxqFiLCADvvAP33efHUc+aBR06BJ1IIqlYiwgbNvir6DkHI0fC6acHnUiqU7EWSXPOwTXXwLffwqmnwogRQSeSmqhYi6S5xx6D116Ddu1890dmZtCJpCZ1Fmsza2lmU81spZmVmNkHZvbf8QgnIrG1fHkrhg3z01OmQNeuweaR2kXTss4EVgFnAu2AkcALZpYbw1wiEmPbtsHddx/Fjh1+iN4llwSdSPalzj94nHNbgdERs143s6+AE4AVsYklIrE2bBisWNGKI46ARx4JOo3Upd69U2aWAxwOLK3huQKgACAnJ4fCwsLG5muU0tLSwDMkCu0Lr7i4mFAolPb74t13O/H448eQmRli2LAPWLSoNOhIgUv0z4g556Jf2Kw58CfgS+fcdftaNi8vzxUVFTUyXuMUFhaSn58faIZEoX3h5efnU1xczJIlS4KOEpjVq/3FmTZuhBtv/JxHH+0WdKSEkCifETN73zmXV31+1KNBzKwZMBPYCQxtwmwiEiehEFx5pS/UfftC//7fBB1JohRVsTYzA6YCOUB/59yumKYSkZi4916YNw9ycuDpp3U6eTKJtmU9GTgSuMA5VxbDPCISIwsWwOjRfnrGDOjcOdA4Uk/RjLPuClwH9AC+M7PS8OOKWIcTkaZRXOyvphcKwe23w7nnBp1I6iuaoXsrAf2xJJKknPPXp165EvLy4J57gk4kDaHTzUVS3PTp8Pzz0KqVvwNMixZBJ5KGULEWSWHLlsFNN/npSZOgm0bpJS0Va5EUtWMHDBgAW7fCwIFw1VVBJ5LGULEWSVF33QUffACHHgqTJ2uYXrJTsRZJQW+9BQ89BBkZvp+6bdugE0ljqViLpJi1a+Hqq/303XfDSScFm0eahoq1SAqpqIDBg2HdOujVC+64I+hE0lRUrEVSyCOP+C6Q/feHmTN9N4ikBhVrkRSxeDEMH+6np06Fgw8ONo80LRVrkRRQWupPJ9+1C268EX72s6ATSVNTsRZJAbfc4k+AOeYYeOCBoNNILKhYiyS555+HadNgv/3guecgKyvoRBILKtYiSWzFCigo8NMPPQRHHx1oHIkhFWuRJFVe7k8j37IFLrrIX1lPUpeKtUiSGjPG31Dg4IPhqad0OnmqU7EWSULz5vnrUpvBs8/6cdWS2lSsRZLMxo3+prfO+Ys1JcANuSUOVKxFkohzMGQIrF4NJ58Mo0YFnUjiRcVaJIk8+SS88oq/it7s2dC8edCJJF5UrEWSxNKl8Jvf+OknnoDc3CDTSLypWIskge3b/enk27f7q+oNGBB0Iok3FWuRJHD77fDRR/4eihMnBp1GgqBiLZLgXnsNHn3U908/9xy0bh10IgmCirVIAvvmG/jlL/30vffC8ccHm0eCo2ItkqBCIX9H8u+/h3PPhVtvDTqRBEnFWiRBPfAAvPMOdO4MzzwDzfRpTWt6+0US0MKFMGKEn37mGTjggGDzSPBUrEUSzJYtfpheKOS7Ps47L+hEkghUrEUSzK9/DV99Bccd579UFAEVa5GEMnMmzJoF2dkwZw60bBl0IkkUKtYiCeKLL3yrGvyJLz/+cbB5JLGoWIskgJ07fT91aSlcemnV2GqRSirWIglg5EgoKoKuXf1FmnTXF6lOxVokYG+/DfffDxkZ/rKn7dsHnUgSkYq1SIDWr/dnKYK/kcAppwSbRxKXirVIQJzzfdPffQdnnOFv0SVSm6iKtZkNNbMiM9thZtNjnEkkLUycCG+8AR06+JveZmQEnUgSWWaUy30L3AP0AbJiF0ckPXz4ob9GNcDUqdClS7B5JPFFVaydc38AMLM84JCYJhJJcVu3+ju97NwJ110HP/950IkkGUTbso6KmRUABQA5OTkUFhY25errrbS0NPAMiUL7wisuLiYUCgW6L8aPP5xPPz2Irl23ctFF71NYWBFYFh0XVRJ9XzRpsXbOPQk8CZCXl+fy8/ObcvX1VlhYSNAZEoX2hde+fXuKi4sD2xcvveT7qVu2hFdfbcWxx54RSI5KOi6qJPq+0GgQkTj5+mu49lo/PX48HHtssHkkuahYi8RBeTlccQUUF8MFF8CNNwadSJJNVN0gZpYZXjYDyDCz/YBy51x5LMOJpIqxY2H+fDjwQJg2TaeTS/1F27IeAZQBw4Erw9MjYhVKJJXMnw9jxvgCPXMmdOoUdCJJRtEO3RsNjI5pEpEUtGmT7/6oqIDhw+Hss4NOJMlKfdYiMeIcFBT4LxZ79vSta5GGUrEWiZGpU/1QvTZt/NX0mjcPOpEkMxVrkRj49FO45RY/PXkyHHZYsHkk+alYizSx7dv96eTbtsGgQb7PWqSxVKxFmtjw4f5CTYcdBpMmBZ1GUoWKtUgTevNNmDABMjP93cnbtAk6kaQKFesmYGa89NJLQceQgK1ZA4MH++mxY+HEEwONIykmLYr14MGDOf/884OOISmsosLfnmv9ejjnHLjttqATSapJi2ItEmsPPghz5/qzE2fMgGb6ZEkTS/tD6pNPPqFfv360adOGzp07c/nll/Pdd9/tfn7RokWce+65dOrUibZt23LaaaexYMGCfa5z3LhxdOrUiYULF8Y6viSAoqKq+ydOn+6v/yHS1NK6WK9Zs4YzzjiDY445hvfee4+5c+dSWlrKhRdeSEWFvyB8SUkJgwYN4t133+W9996jR48e9O3blw0bNuy1Pucct912GxMnTmTevHn89Kc/jfdLkjgrKYHLL/dX1bv5ZujXL+hEkqqa9OYDyWby5Ml0796dcePG7Z43Y8YMOnbsSFFRET179uSss87a43cmTpzIyy+/zFtvvcWVV165e34oFOKaa67hH//4B/Pnzyc3NzdeL0MCNHQofPEFdO8OEYeRSJNL62L9/vvv8/e//53WrVvv9dyXX35Jz549WbduHSNHjuRvf/sba9euJRQKUVZWxtdff73H8rfddhuZmZksXLiQzp07x+slSIBmz/b901lZfpjefvsFnUhSWVoX64qKCvr168f48eP3ei4nJweAq6++mrVr1/Lwww+Tm5tLy5YtOfvss9m5c+cey/fu3Zs5c+bw5ptvMrhy/JakrOXL4frr/fSECXDkkcHmkdSX1sX6+OOP54UXXqBr1640r+UqO/Pnz+f3v/89/cKdkWvXrmXNmjV7Lde3b19+8YtfcMkll2BmXH311THNLsHZtQsGDvT91f37w5AhQSeSdJA2XzBu2bKFJUuW7PHo168fmzdv5rLLLmPhwoUsX76cuXPnUlBQQElJCQCHH344zz77LJ988gmLFi1iwIABtGjRosZtnH/++bz44otcf/31zJgxI54vT+Jo1ChYuBC6dIEpU3TXF4mPtGlZv/vuuxx33HF7zOvfvz//+Mc/+J//+R/OO+88tm/fzg9+8APOPfdcWrZsCcC0adMoKCjghBNO4KCDDmL06NGsX7++1u2cf/75vPDCC1x66aUAXHXVVbF7URJ377wD993nx1HPmgUdOgSdSNJFWhTr6dOnM3369Fqf39ep4t27d99rvPSgQYP2+Nk5t8fPF1xwAWVlZfUPKgltwwZ/FT3n4He/g9NPDzqRpJO06QYRaQzn4Fe/gm+/hVNPhRG6A6nEmYq1SBQeewxefRXatfPdH5lp8TepJBIVa5E6fPQRDBvmp6dMga5dg80j6UnFWmQfysr86eQ7dvghepdcEnQiSVdJXazLy8u54YYbeOqpp4KOIilq2DBYuhSOOAIeeSToNJLOkrbnraSkhH79+lFUVMQzzzzDgQceuPvEFZGm8Mor/ma3LVr408lbtQo6kaSzpGxZf/PNNxx//PG89957lJWVUVZWxmWXXcbixYuDjiYpYvXqqjMTx42DHj0CjSOSfMX63//+N927d+err75ix44du+dv3bqVPn367L60qUhDhUJw5ZWwcSP07Qu33BJ0IpEkK9Z//vOfOeWUU/j+++8JhUJ7PJeVlcWIESNoplt0SCPddx/Mmwc5OfD00zqdXBJD0vRZT5kyhVtuuaXGMwOzs7OZM2cOF154YQDJJJUsWOCv/QH+8qe62q0kioQv1s45hg8fzqOPPrpXoW7WrBlt27bl7bffJi8vL6CEkio2b/ZX0wuF4Pbb4dxzg04kUiWhi/XOnTsZOHAgf/rTn9i2bdsez7Vo0YIDDjiAefPm6a4s0mjOwXXXwYoVkJcH99wTdCKRPSVssd60aRN9+vTh448/3qtFnZWVxdFHH81f/vIXOuiyZ9IEpk+H55/3w/Nmz/bD9UQSSUJ+G7dixQp69OjBhx9+uFehzs7Opm/fvsyfP1+FWprEsmVw001+etIk6NYt2DwiNUm4Yl1UVMRxxx3H6tWr97p1VnZ2NkOHDuXFF1/cfb1pkcbYscOfTr51q++v1uXHJVEFUqw//vhjTjrpJL7//vs95v/xj3/kzDPPpLi4eK/x0llZWTz88MOMGzcO01gqaSK//S0sXgyHHurPVtShJYkqkGI9btw4Fi1aRO/evdm+fTsAjzzyCJdffvleXyQCtGrVildeeYWCgoJ4R5UU9tZb8OCDkJHh+6nbtg06kUjt4v4F4+bNm3nppZeoqKjg008/ZcCAARxyyCE8/fTTe/VPZ2Rk0L59e9555x2OPfbYeEeVFLZ2LVTe03jMGDjppGDziNQl7sV6+vTpu88yLCsr4+233wbYq0XdsmVLunTpQmFhIQcffHC8Y0qKGzwY1q2DXr3gzjuDTiNSt6i6Qcyso5m9YmZbzWylmQ1syMacc4wfP36Pwrxt27a9CnVWVhY9e/Zk8eLFKtTS5Nata8lbb8H++8PMmb4bRCTRRduyngTsBHKAHsAbZvahc25pfTZWWFhIcXHxPpfJzs6mf//+TJs2jUzdO0kaqbwciov9RZk2bfLD9NasyQJg6lRQW0CShVW/M/deC5i1AjYBxzjnloXnzQS+cc4Nr+332rRp40444YQ95n300Uds3Lix1m01a9aMnJwcunXr1iQjPoqLi2nfvn2j15MKkn1fhEK+8JaXw65dNf9b07xq1/sClgDQrVsPDjoo3q8i8ST7cdGUEmVfzJs3733n3F7Xz4im6Xo4EKos1GEfAmdWX9DMCoACgObNm+/Rit61axebNm3a54YqKipYt24d7du3p0UTnEIWCoXqbMmni0TYF85BKGQRj2aUl1f9XNt0KNSMOtoU+5SR4XY/du50NG8eIju7GB0aiXFcJIpE3xfRFOvWwOZq8zYDbaov6Jx7EngSIC8vzxUVFe1+btSoUYwbN26Pa1DXZseOHSxYsIB27dpFEa92hYWF5OfnN2odqaKp9oVz/r6Eld0KGzdGP725+lFUD1lZ0LEjdOjg/412um1biLxqbn5+PsXFxSxZsqTR+yIV6DNSJVH2RW29CtEU61Kg+gjUtkBJtBsvLy9n4sSJURXqUCjEypUr+d3vfseECROi3YTUUyjki2d9im3lz1G8jTUyg/bt61dsK3/eb78mffkiSSeaYr0MyDSzbs65z8PzugNRf7n42muvUV5eXuvzrVu3ZteuXey///706dOHvn370rt372hXn9ZqauXWVHC//PJYnKuav3kzDe5aaNnSj6Sobyu3Xbs9W7kiEr06i7VzbquZ/QEYY2ZD8KNBfgacEu1G7r//fkpKqhri2dnZOOfIzs7m7LPP5oILLqBXr15pO0yvspVb326FTZsgfAJoFDru8VNjWrlZWU29B0SkLtGOjfs1MA1YB3wP3BDtsL3ly5fzr3/9i6ysLFq0aMGZZ57JhRdeSK9evTj00ENT6jof27fXv9hu3OiHljW0lduixb5buZU/r1r1Ib16dd/9XLt2Gl8skkyiKtbOuY3ARQ3ZQIcOHZgyZQqnnnoqRxxxRMIX54qK6Fu51X+OvpW7t3bt9l1sa5vOyoru4kOFhZs48cSG5xORYMX8rJMOHTowZMiQWG9mL9u3w/fft2Dp0rr7cyOnN21qXCu3vsW2Y0ffHaFWrojsS0KfIlhRAVu2NGyYmL8mVNTd6nto27Z+xbZyOjtbl9gUkdiIS7HesaNhX55t2uQLdkM0bw6tW+/kgANa1GvUQvv2oLPcRSTRxKwsffIJdOniC28Nl6iOWtu29R8i1rGjb+XOm/fPhBjkLiLSWDEr1mVlsHp1eCOZDRsi1r69byGLiKS7mBXrI4/0d+Lo2NHfMVp9uSIiDRezYp2dDT/4QazWLiKSXnTyr4hIElCxFhFJAirWIiJJQMVaRCQJqFiLiCQBFWsRkSSgYi0ikgRUrEVEkoCKtYhIEjDX0Is317Vis/XAypisPHqdgA0BZ0gU2hdVtC+qaF9USZR90dU591/VZ8asWCcCMytyzuUFnSMRaF9U0b6oon1RJdH3hbpBRESSgIq1iEgSSPVi/WTQARKI9kUV7Ysq2hdVEnpfpHSftYhIqkj1lrWISEpQsRYRSQIq1iIiSSCtirWZdTOz7Wb2bNBZgmBmLc1sqpmtNLMSM/vAzP476FzxYmYdzewVM9sa3gcDg84UhHQ/DmqT6PUhrYo1MAlYFHSIAGUCq4AzgXbASOAFM8sNMlQcTQJ2AjnAFcBkMzs62EiBSPfjoDYJXR/Splib2QCgGPhrwFEC45zb6pwb7Zxb4ZyrcM69DnwFnBB0tlgzs1ZAf2Ckc67UOTcfeBUYFGyy+Evn46A2yVAf0qJYm1lbYAwwLOgsicTMcoDDgaVBZ4mDw4GQc25ZxLwPgXRsWe8hzY6DvSRLfUiLYg3cDUx1zq0KOkiiMLPmwCzgGefcp0HniYPWwOZq8zYDbQLIkjDS8DioSVLUh6Qv1mZWaGaulsd8M+sBnAM8HHDUmKtrX0Qs1wyYie+/HRpY4PgqBdpWm9cWKAkgS0JI0+NgD8lUHzKDDtBYzrn8fT1vZr8BcoGvzQx8CyvDzI5yzh0f63zxVNe+ADC/E6biv2Tr65zbFetcCWIZkGlm3Zxzn4fndSd9//RP1+OgunySpD6k/OnmZpbNni2q2/Bvzg3OufWBhAqQmT0O9ADOcc6VBhwnrszsOcABQ/D74E3gFOdc2hXsdD4OIiVTfUj6lnVdnHPbgG2VP5tZKbA90d6IeDCzrsB1wA7gu3BLAuA659yswILFz6+BacA64Hv8BzIdC3W6Hwe7JVN9SPmWtYhIKkj6LxhFRNKBirWISBJQsRYRSQIq1iIiSUDFWkQkCahYi4gkARVrEZEkoGItIpIE/h8nb6tVY6PaXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "popular-southwest",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deserialize',\n",
       " 'elu',\n",
       " 'exponential',\n",
       " 'gelu',\n",
       " 'get',\n",
       " 'hard_sigmoid',\n",
       " 'linear',\n",
       " 'relu',\n",
       " 'selu',\n",
       " 'serialize',\n",
       " 'sigmoid',\n",
       " 'softmax',\n",
       " 'softplus',\n",
       " 'softsign',\n",
       " 'swish',\n",
       " 'tanh']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m in dir(keras.activations) if not m.startswith(\"_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cellular-number",
   "metadata": {},
   "source": [
    "Let's train a neural network on Fashion MNIST using the Leaky ReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "portable-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255.\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "double-calculator",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.LeakyReLU(), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "improving-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "secret-alexander",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 1.6314 - accuracy: 0.5054 - val_loss: 0.8886 - val_accuracy: 0.7160\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.8416 - accuracy: 0.7247 - val_loss: 0.7130 - val_accuracy: 0.7656\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.7053 - accuracy: 0.7637 - val_loss: 0.6427 - val_accuracy: 0.7898\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.6325 - accuracy: 0.7908 - val_loss: 0.5900 - val_accuracy: 0.8066\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5992 - accuracy: 0.8021 - val_loss: 0.5582 - val_accuracy: 0.8198\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5624 - accuracy: 0.8142 - val_loss: 0.5350 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 5s 3ms/step - loss: 0.5379 - accuracy: 0.8217 - val_loss: 0.5156 - val_accuracy: 0.8304\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.5152 - accuracy: 0.8297 - val_loss: 0.5079 - val_accuracy: 0.8284\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5100 - accuracy: 0.8269 - val_loss: 0.4895 - val_accuracy: 0.8386\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 6s 3ms/step - loss: 0.4918 - accuracy: 0.8339 - val_loss: 0.4817 - val_accuracy: 0.8398\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10, \n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fuzzy-african",
   "metadata": {},
   "source": [
    "## PReLU\n",
    "- The paper evaluated the parametric leaky ReLU (PReLU), where α is authorized to be **learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter).** \n",
    "- PReLU was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set.\n",
    "Now let's try PReLU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "applicable-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]), \n",
    "    keras.layers.Dense(300, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.PReLU(), \n",
    "    keras.layers.Dense(100, kernel_initializer=\"he_normal\"), \n",
    "    keras.layers.PReLU(), \n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "seeing-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "adjusted-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 8s 4ms/step - loss: 1.6969 - accuracy: 0.4974 - val_loss: 0.9255 - val_accuracy: 0.7186\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.8706 - accuracy: 0.7247 - val_loss: 0.7305 - val_accuracy: 0.7628\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.7211 - accuracy: 0.7620 - val_loss: 0.6565 - val_accuracy: 0.7878\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.6448 - accuracy: 0.7881 - val_loss: 0.6004 - val_accuracy: 0.8046\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.6078 - accuracy: 0.8003 - val_loss: 0.5656 - val_accuracy: 0.8182\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5693 - accuracy: 0.8119 - val_loss: 0.5406 - val_accuracy: 0.8238\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5428 - accuracy: 0.8194 - val_loss: 0.5196 - val_accuracy: 0.8310\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5193 - accuracy: 0.8284 - val_loss: 0.5113 - val_accuracy: 0.8316\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 6s 4ms/step - loss: 0.5129 - accuracy: 0.8273 - val_loss: 0.4916 - val_accuracy: 0.8378\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.4941 - accuracy: 0.8314 - val_loss: 0.4826 - val_accuracy: 0.8396\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-heaven",
   "metadata": {},
   "source": [
    "## ELU\n",
    "- A 2015 paper by Djork-Arné Clevert et al. proposed a new activation function called the **exponential linear unit (ELU)** that outperformed all the ReLU variants in the authors’ experiments: training time was reduced, and the neural network performed better on the test set.\n",
    "<img src=\"11-2.png\">\n",
    "- The ELU activation function looks a lot like the ReLU function, with a few major differences:\n",
    "    - **It takes on negative values when z < 0, which allows the unit to have an average output closer to 0 and helps alleviate the vanishing gradients problem.** \n",
    "        - The hyperparameter α defines the value that the ELU function approaches when z is a large negative number. \n",
    "        - It is usually set to 1, but you can tweak it like any other hyperparameter.\n",
    "    - It has a nonzero gradient for z < 0, which avoids the dead neurons problem.\n",
    "    - **If α is equal to 1 then the function is smooth everywhere, including around z = 0, which helps speed up Gradient Descent since it does not bounce as much to the left and right of z = 0.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "distant-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "substantial-victim",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-tyler",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "automatic-discretion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1b91683dc88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"elu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "practical-worthy",
   "metadata": {},
   "source": [
    "## SELU\n",
    "- This activation function was proposed in a great paper by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, published in June 2017. \n",
    "- **During training, a neural network composed exclusively of a stack of dense layers using the SELU activation function and LeCun initialization will self-normalize: the output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem.** \n",
    "    - As a result, this activation function outperforms the other activation functions very significantly for such neural nets. \n",
    "    - **The input features must be standardized (mean 0 and standard deviation 1).**\n",
    "    - **Every hidden layer’s weights must be initialized with LeCun normal initialization.** \n",
    "        - In Keras, this means setting kernel_initializer=\"lecun_normal\".\n",
    "    - **The network’s architecture must be Sequential.** \n",
    "    - Unfortunately, if you try to use SELU in nonsequential architectures, such as recurrent networks or networks with skip connections (i.e., connections that skip layers, such as in Wide & Deep nets), self-normalization will not be guaranteed, so SELU will not necessarily outperform other activation functions.\n",
    "    - Unfortunately, the self-normalizing property of the SELU activation function is easily broken: you cannot use ℓ1 or ℓ2 regularization, regular dropout, max-norm, skip connections or other non-sequential topologies (so recurrent neural networks won't self-normalize). \n",
    "    - However, in practice it works quite well with sequential CNNs. \n",
    "    - If you break self-normalization, SELU will not necessarily outperform other activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dramatic-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erfc\n",
    "\n",
    "# alpha and scale to self normalize with mean 0 and standard deviation 1\n",
    "alpha_0_1 = -np.sqrt(2 / np.pi) / (erfc(1/np.sqrt(2)) * np.exp(1/2) - 1)\n",
    "scale_0_1 = (1 - erfc(1 / np.sqrt(2)) * np.sqrt(np.e)) * np.sqrt(2 * np.pi) * (2 * erfc(np.sqrt(2))*np.e**2 + np.pi*erfc(1/np.sqrt(2))**2*np.e - 2*(2+np.pi)*erfc(1/np.sqrt(2))*np.sqrt(np.e)+np.pi+2)**(-1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "thrown-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z, scale=scale_0_1, alpha=alpha_0_1):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "resident-naples",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-springer",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (scale and alpha) are tuned in such a way that the mean output of each neuron remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 1,000 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "entertaining-tours",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: mean -0.00, std deviation 1.00\n",
      "Layer 100: mean 0.02, std deviation 0.96\n",
      "Layer 200: mean 0.01, std deviation 0.90\n",
      "Layer 300: mean -0.02, std deviation 0.92\n",
      "Layer 400: mean 0.05, std deviation 0.89\n",
      "Layer 500: mean 0.01, std deviation 0.93\n",
      "Layer 600: mean 0.02, std deviation 0.92\n",
      "Layer 700: mean -0.02, std deviation 0.90\n",
      "Layer 800: mean 0.05, std deviation 0.83\n",
      "Layer 900: mean 0.02, std deviation 1.00\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100)) # standardized inputs\n",
    "for layer in range(1000):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1 / 100)) # LeCun initialization\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=0).mean()\n",
    "    stds = np.std(Z, axis=0).mean()\n",
    "    if layer % 100 == 0:\n",
    "        print(\"Layer {}: mean {:.2f}, std deviation {:.2f}\".format(layer, means, stds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-manner",
   "metadata": {},
   "source": [
    "Using SELU is easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "false-simple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1b915929e48>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation=\"selu\", kernel_initializer=\"lecun_normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-lodging",
   "metadata": {},
   "source": [
    "Let's create a neural net for Fashion MNIST with 100 hidden layers, using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "strange-adobe",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "chinese-delivery",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "advisory-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3), \n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-monte",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "positive-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_means = X_train.mean(axis=0, keepdims=True)\n",
    "pixel_stds = X_train.std(axis=0, keepdims=True)\n",
    "X_train_scaled = (X_train - pixel_means) / pixel_stds\n",
    "X_valid_scaled = (X_valid - pixel_means) / pixel_stds\n",
    "X_test_scaled = (X_test - pixel_means) / pixel_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "split-symposium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 50s 27ms/step - loss: 1.3254 - accuracy: 0.4924 - val_loss: 0.7693 - val_accuracy: 0.7192\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 47s 27ms/step - loss: 0.7608 - accuracy: 0.7294 - val_loss: 0.6361 - val_accuracy: 0.7780\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 45s 26ms/step - loss: 0.6213 - accuracy: 0.7806 - val_loss: 0.5794 - val_accuracy: 0.7962\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 42s 24ms/step - loss: 0.5377 - accuracy: 0.8141 - val_loss: 0.4850 - val_accuracy: 0.8306\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 43s 25ms/step - loss: 0.4936 - accuracy: 0.8289 - val_loss: 0.4789 - val_accuracy: 0.8358\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-raising",
   "metadata": {},
   "source": [
    "Now look at what happens if we try to use the ReLU activation function instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "latest-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "italic-example",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "model.add(keras.layers.Dense(300, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "for layer in range(99):\n",
    "    model.add(keras.layers.Dense(100, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "written-worth",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "trying-blackberry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 48s 26ms/step - loss: 2.0999 - accuracy: 0.1760 - val_loss: 1.7955 - val_accuracy: 0.2882\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 43s 25ms/step - loss: 1.3662 - accuracy: 0.4005 - val_loss: 0.9627 - val_accuracy: 0.5952\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 43s 25ms/step - loss: 0.9966 - accuracy: 0.5824 - val_loss: 0.9120 - val_accuracy: 0.6410\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 42s 24ms/step - loss: 0.9921 - accuracy: 0.5944 - val_loss: 0.7455 - val_accuracy: 0.7198\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 43s 25ms/step - loss: 0.7753 - accuracy: 0.6996 - val_loss: 0.8021 - val_accuracy: 0.6832\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train, epochs=5,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-beauty",
   "metadata": {},
   "source": [
    "Not great at all, we suffered from the vanishing/exploding gradients problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-encyclopedia",
   "metadata": {},
   "source": [
    "### TIP\n",
    "So, which activation function should you use for the hidden layers of your deep neural networks? \n",
    "- Although your mileage will vary, in general SELU > ELU > leaky ReLU (and its variants) > ReLU > tanh > logistic.\n",
    "- If the network’s architecture prevents it from self-normalizing, then ELU may perform better than SELU (since SELU is not smooth at z = 0).\n",
    "- If you care a lot about runtime latency, then you may prefer leaky ReLU. \n",
    "    - If you don’t want to tweak yet another hyperparameter, you may use the default α values used by Keras (e.g., 0.3 for leaky ReLU).\n",
    "- If you have spare time and computing power, you can use cross-validation to evaluate other activation functions, such as RReLU if your network is overfitting or PReLU if you have a huge training set.\n",
    "- That said, because\n",
    "- ReLU is the most used activation function (by far), many libraries and hardware accelerators provide ReLU-specific optimizations; therefore, if speed is your priority, ReLU might still be the best choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-oxide",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- Although using He initialization along with ELU (or any variant of ReLU) can significantly reduce the danger of the vanishing/exploding gradients problems at the beginning of training, it doesn’t guarantee that they won’t come back during training.\n",
    "- In a 2015 paper, Sergey Ioffe and Christian Szegedy proposed a technique called **Batch Normalization(BN)** that addresses these problems.\n",
    "- The technique consists of **adding an operation in the model just before or after the activation function of each hidden layer.**\n",
    "    - This operation simply zerocenters and normalizes each input, then scales and shifts the result using two new parameter vectors per layer: one for scaling, the other for shifting.\n",
    "    - In other words, **the operation lets the model learn the optimal scale and mean of each of the layer’s inputs.**\n",
    "    - In many cases, **if you add a BN layer as the very first layer of your neural network, you do not need to standardize your training set (e.g., using a StandardScaler); the BN layer will do it for you** (well, approximately, since it only looks at one batch at a time, and it can also rescale and shift each input feature).\n",
    "- In order to zero-center and normalize the inputs, the algorithm needs to estimate each input’s mean and standard deviation. \n",
    "    - It does so by evaluating the mean and standard deviation of the input over the current mini-batch (hence the name “Batch Normalization”). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "chemical-delhi",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation=\"relu\"), \n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "acute-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rational-glossary",
   "metadata": {},
   "source": [
    "- As you can see, each BN layer adds four parameters per input: γ, β, μ, and σ (for example, the first BN layer adds 3,136 parameters, which is 4 × 784).\n",
    "- The last two parameters, μ and σ, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable” (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total number of non-trainable parameters in this model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-heath",
   "metadata": {},
   "source": [
    "- Batch Normalization acts like a regularizer, reducing the need for other regularization techniques.\n",
    "- Batch Normalization does, however, add some complexity to the model (although it can remove the need for normalizing the input data). \n",
    "- Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. \n",
    "- Fortunately, it’s often possible to fuse the BN layer with the previous layer, after training, thereby avoiding the runtime penalty. \n",
    "    - This is done by updating the previous layer’s weights and biases so that it directly produces outputs of the appropriate scale and offset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flexible-instrumentation",
   "metadata": {},
   "source": [
    "Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "defined-trinidad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn1 = model.layers[1]\n",
    "[(var.name, var.trainable) for var in bn1.variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "requested-postcard",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fancy-nashville",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 6ms/step - loss: 1.2287 - accuracy: 0.5994 - val_loss: 0.5525 - val_accuracy: 0.8230\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5995 - accuracy: 0.7958 - val_loss: 0.4724 - val_accuracy: 0.8472\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5312 - accuracy: 0.8173 - val_loss: 0.4375 - val_accuracy: 0.8550\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4885 - accuracy: 0.8293 - val_loss: 0.4151 - val_accuracy: 0.8598\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4718 - accuracy: 0.8343 - val_loss: 0.3996 - val_accuracy: 0.8636\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4420 - accuracy: 0.8458 - val_loss: 0.3866 - val_accuracy: 0.8690\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4286 - accuracy: 0.8497 - val_loss: 0.3761 - val_accuracy: 0.8700\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4087 - accuracy: 0.8552 - val_loss: 0.3712 - val_accuracy: 0.8740\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4080 - accuracy: 0.8563 - val_loss: 0.3630 - val_accuracy: 0.8752\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.3903 - accuracy: 0.8615 - val_loss: 0.3571 - val_accuracy: 0.8762\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-favor",
   "metadata": {},
   "source": [
    "- Sometimes applying BN before the activation function works better (there's a debate on this topic).\n",
    "- **To add the BN layers before the activation functions, you must remove the activation function from the hidden layers and add them as separate layers after the BN layers.**\n",
    "- Moreover, the layer before a BatchNormalization layer does not need to have bias terms, since the BatchNormalization layer some as well, it would be a waste of parameters, so you can set use_bias=False when creating those layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "welsh-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(100, use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "logical-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 300)               235200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 100)               30000     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_217 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 270,946\n",
      "Trainable params: 268,578\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fantastic-resort",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sought-provider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1719/1719 [==============================] - 12s 6ms/step - loss: 1.3677 - accuracy: 0.5604 - val_loss: 0.6767 - val_accuracy: 0.7816\n",
      "Epoch 2/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.7136 - accuracy: 0.7701 - val_loss: 0.5566 - val_accuracy: 0.8180\n",
      "Epoch 3/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.6123 - accuracy: 0.7991 - val_loss: 0.5007 - val_accuracy: 0.8360\n",
      "Epoch 4/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5547 - accuracy: 0.8148 - val_loss: 0.4666 - val_accuracy: 0.8448\n",
      "Epoch 5/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.5254 - accuracy: 0.8231 - val_loss: 0.4434 - val_accuracy: 0.8536\n",
      "Epoch 6/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4947 - accuracy: 0.8326 - val_loss: 0.4263 - val_accuracy: 0.8546\n",
      "Epoch 7/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4736 - accuracy: 0.8388 - val_loss: 0.4130 - val_accuracy: 0.8568\n",
      "Epoch 8/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4550 - accuracy: 0.8446 - val_loss: 0.4035 - val_accuracy: 0.8606\n",
      "Epoch 9/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4495 - accuracy: 0.8438 - val_loss: 0.3943 - val_accuracy: 0.8640\n",
      "Epoch 10/10\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4333 - accuracy: 0.8495 - val_loss: 0.3875 - val_accuracy: 0.8662\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-performance",
   "metadata": {},
   "source": [
    "## Gradient Clipping\n",
    "- Another popular technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold.\n",
    "    - This is called Gradient Clipping. \n",
    "    - This technique is most often used in recurrent neural networks, as Batch Normalization is tricky to use in RNNs.\n",
    "- For other types of networks, BN is usually sufficient.\n",
    "\n",
    "In Keras, implementing Gradient Clipping is just a matter of setting the **clipvalue** or **clipnorm** argument when creating an optimizer, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "abstract-identity",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-burns",
   "metadata": {},
   "source": [
    "- This optimizer will clip every component of the gradient vector to a value between –1.0 and 1.0. This means that all the partial derivatives of the loss (with regard to each and every trainable parameter) will be clipped between –1.0 and 1.0. \n",
    "- The threshold is a hyperparameter you can tune. Note that it may change the orientation of the gradient vector. \n",
    "    - For instance, if the original gradient vector is [0.9, 100.0], it points mostly in the direction of the second axis; but once you clip it by value, you get [0.9, 1.0], which points roughly in the diagonal between the two axes. \n",
    "    - In practice, this approach works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "through-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-credit",
   "metadata": {},
   "source": [
    "- If you want to ensure that Gradient Clipping does not change the direction of the gradient vector, you should clip by norm by setting clipnorm instead of clipvalue. \n",
    "- This will clip the whole gradient if its $ℓ_2$ norm is greater than the threshold you picked. \n",
    "    - For example, if you set clipnorm=1.0, then the vector [0.9, 100.0] will be clipped to [0.00899964, 0.9999595], preserving its orientation but almost eliminating the first component.\n",
    "- If you observe that the gradients explode during training (you can track the size of the gradients using TensorBoard), you may want to try both clipping by value and clipping by norm, with different thresholds, and see which option performs best on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-component",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers\n",
    "- It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then reuse the lower layers of this network.\n",
    "    - This technique is called **transfer learning.**\n",
    "- It will not only speed up training considerably, but also require significantly less training data.\n",
    "***\n",
    "- Suppose you have access to a DNN that was trained to classify pictures into 100 different categories, including animals, plants, vehicles, and everyday objects. \n",
    "- You now want to train a DNN to classify specific types of vehicles.\n",
    "    - These tasks are very similar, even partly overlapping, so you should try to reuse parts of the first network.\n",
    "\n",
    "<img src=\"11-4.png\">\n",
    "\n",
    "### NOTE\n",
    "- If the input pictures of your new task don’t have the same size as the ones used in the original task, you will usually have to add a preprocessing step to **resize them to the size expected by the original model.** \n",
    "- More generally, **transfer learning will work best when the inputs have similar low-level features.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-syndication",
   "metadata": {},
   "source": [
    "- **The output layer of the original model should usually be replaced because it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.**\n",
    "- Similarly, the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task. \n",
    "    - You want to find the right number of layers to reuse.\n",
    "### TIP\n",
    "- **The more similar the tasks are, the more layers you want to reuse (starting with the lower layers).** \n",
    "- **For very similar tasks, try keeping all the hidden layers and just replacing the output layer.**\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experimental-resident",
   "metadata": {},
   "source": [
    "- Try freezing all the reused layers first (i.e., make their weights non-trainable so that Gradient Descent won’t modify them), then train your model and see how it performs. \n",
    "- Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. \n",
    "    - The more training data you have, the more layers you can unfreeze. \n",
    "    - **It is also useful to reduce the learning rate when you unfreeze reused layers**: this will avoid wrecking their fine-tuned weights.\n",
    "- If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again.\n",
    "    - You can iterate until you find the right number of layers to reuse. \n",
    "    - If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-ethics",
   "metadata": {},
   "source": [
    "### Reusing a Keras model\n",
    "Let's split the fashion MNIST training set in two:\n",
    "\n",
    "- X_train_A: all images of all items except for sandals and shirts (classes 5 and 6).\n",
    "- X_train_B: a much smaller training set of just the first 200 images of sandals or shirts.\n",
    "\n",
    "The validation set and the test set are also split this way, but without restricting the number of images.\n",
    "\n",
    "- We will train a model on set A (classification task with 8 classes), and try to reuse it to tackle set B (binary classification).(positive=shirt,negative=sandal).\n",
    "- We hope to transfer a little bit of knowledge from task A to task B, since classes in set A (sneakers, ankle boots, coats, t-shirts, etc.) are somewhat similar to classes in set B (sandals and shirts). \n",
    "- However, since we are using Dense layers, only patterns that occur at the same location can be reused (in contrast, convolutional layers will transfer much better, since learned patterns can be detected anywhere on the image)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "nuclear-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y):\n",
    "    y_5_or_6 = (y == 5) | (y == 6) # sandals or shirts\n",
    "    y_A = y[~y_5_or_6]\n",
    "    y_A[y_A > 6] -= 2 # class indices 7, 8, 9 should be moved to 5, 6, 7\n",
    "    y_B = (y[y_5_or_6] == 6).astype(np.float32) # binary classification task: is it a shirt (class 6)?\n",
    "    return ((X[~y_5_or_6], y_A),\n",
    "            (X[y_5_or_6], y_B))\n",
    "\n",
    "(X_train_A, y_train_A), (X_train_B, y_train_B) = split_dataset(X_train, y_train)\n",
    "(X_valid_A, y_valid_A), (X_valid_B, y_valid_B) = split_dataset(X_valid, y_valid)\n",
    "(X_test_A, y_test_A), (X_test_B, y_test_B) = split_dataset(X_test, y_test)\n",
    "X_train_B = X_train_B[:200]\n",
    "y_train_B = y_train_B[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "popular-briefs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43986, 28, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "humanitarian-integration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 28, 28)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "correct-mystery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 5, 7, 7, 7, 4, 4, 3, 4, 0, 1, 6, 3, 4, 3, 2, 6, 5, 3, 4, 5,\n",
       "       1, 3, 4, 2, 0, 6, 7, 1], dtype=uint8)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_A[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "static-supplement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_B[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "hundred-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "rental-duration",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.Sequential()\n",
    "model_A.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_A.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_A.add(keras.layers.Dense(8, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "nervous-converter",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "               metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "muslim-youth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1375/1375 [==============================] - 7s 4ms/step - loss: 0.9248 - accuracy: 0.6995 - val_loss: 0.3894 - val_accuracy: 0.8662\n",
      "Epoch 2/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3651 - accuracy: 0.8745 - val_loss: 0.3288 - val_accuracy: 0.8827\n",
      "Epoch 3/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3182 - accuracy: 0.8898 - val_loss: 0.3011 - val_accuracy: 0.8991\n",
      "Epoch 4/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.3048 - accuracy: 0.8955 - val_loss: 0.2894 - val_accuracy: 0.9023\n",
      "Epoch 5/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2804 - accuracy: 0.9029 - val_loss: 0.2772 - val_accuracy: 0.9071\n",
      "Epoch 6/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2702 - accuracy: 0.9078 - val_loss: 0.2734 - val_accuracy: 0.9071\n",
      "Epoch 7/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2627 - accuracy: 0.9092 - val_loss: 0.2715 - val_accuracy: 0.9088\n",
      "Epoch 8/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2610 - accuracy: 0.9119 - val_loss: 0.2589 - val_accuracy: 0.9138\n",
      "Epoch 9/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2559 - accuracy: 0.9109 - val_loss: 0.2563 - val_accuracy: 0.9145\n",
      "Epoch 10/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2512 - accuracy: 0.9140 - val_loss: 0.2542 - val_accuracy: 0.9168\n",
      "Epoch 11/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2431 - accuracy: 0.9172 - val_loss: 0.2496 - val_accuracy: 0.9150\n",
      "Epoch 12/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2423 - accuracy: 0.9171 - val_loss: 0.2511 - val_accuracy: 0.9131\n",
      "Epoch 13/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2360 - accuracy: 0.9181 - val_loss: 0.2446 - val_accuracy: 0.9155\n",
      "Epoch 14/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2266 - accuracy: 0.9230 - val_loss: 0.2414 - val_accuracy: 0.9173\n",
      "Epoch 15/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2225 - accuracy: 0.9241 - val_loss: 0.2447 - val_accuracy: 0.9193\n",
      "Epoch 16/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2261 - accuracy: 0.9213 - val_loss: 0.2383 - val_accuracy: 0.9195\n",
      "Epoch 17/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2191 - accuracy: 0.9252 - val_loss: 0.2405 - val_accuracy: 0.9173\n",
      "Epoch 18/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2171 - accuracy: 0.9253 - val_loss: 0.2422 - val_accuracy: 0.9150\n",
      "Epoch 19/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2180 - accuracy: 0.9249 - val_loss: 0.2329 - val_accuracy: 0.9205\n",
      "Epoch 20/20\n",
      "1375/1375 [==============================] - 6s 4ms/step - loss: 0.2113 - accuracy: 0.9271 - val_loss: 0.2333 - val_accuracy: 0.9205\n"
     ]
    }
   ],
   "source": [
    "history = model_A.fit(X_train_A, y_train_A, epochs=20, \n",
    "                   validation_data=(X_valid_A, y_valid_A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "precious-emerald",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A.save(\"my_model_A.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "smart-mozambique",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B = keras.models.Sequential()\n",
    "model_B.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
    "for n_hidden in (300, 100, 50, 50, 50):\n",
    "    model_B.add(keras.layers.Dense(n_hidden, activation=\"selu\"))\n",
    "model_B.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ahead-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "prerequisite-poland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "7/7 [==============================] - 1s 71ms/step - loss: 1.0360 - accuracy: 0.4975 - val_loss: 0.6314 - val_accuracy: 0.6004\n",
      "Epoch 2/20\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.5883 - accuracy: 0.6971 - val_loss: 0.4784 - val_accuracy: 0.8529\n",
      "Epoch 3/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4380 - accuracy: 0.8854 - val_loss: 0.4102 - val_accuracy: 0.8945\n",
      "Epoch 4/20\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.4021 - accuracy: 0.8712 - val_loss: 0.3647 - val_accuracy: 0.9178\n",
      "Epoch 5/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3361 - accuracy: 0.9348 - val_loss: 0.3300 - val_accuracy: 0.9320\n",
      "Epoch 6/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.3113 - accuracy: 0.9233 - val_loss: 0.3019 - val_accuracy: 0.9402\n",
      "Epoch 7/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2817 - accuracy: 0.9299 - val_loss: 0.2804 - val_accuracy: 0.9422\n",
      "Epoch 8/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2632 - accuracy: 0.9379 - val_loss: 0.2606 - val_accuracy: 0.9473\n",
      "Epoch 9/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2373 - accuracy: 0.9481 - val_loss: 0.2428 - val_accuracy: 0.9523\n",
      "Epoch 10/20\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.2229 - accuracy: 0.9657 - val_loss: 0.2281 - val_accuracy: 0.9544\n",
      "Epoch 11/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2155 - accuracy: 0.9590 - val_loss: 0.2150 - val_accuracy: 0.9584\n",
      "Epoch 12/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1834 - accuracy: 0.9738 - val_loss: 0.2036 - val_accuracy: 0.9584\n",
      "Epoch 13/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1671 - accuracy: 0.9828 - val_loss: 0.1931 - val_accuracy: 0.9615\n",
      "Epoch 14/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1527 - accuracy: 0.9915 - val_loss: 0.1838 - val_accuracy: 0.9635\n",
      "Epoch 15/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1595 - accuracy: 0.9904 - val_loss: 0.1746 - val_accuracy: 0.9686\n",
      "Epoch 16/20\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1473 - accuracy: 0.9937 - val_loss: 0.1674 - val_accuracy: 0.9686\n",
      "Epoch 17/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1412 - accuracy: 0.9944 - val_loss: 0.1604 - val_accuracy: 0.9706\n",
      "Epoch 18/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1242 - accuracy: 0.9931 - val_loss: 0.1539 - val_accuracy: 0.9706\n",
      "Epoch 19/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1224 - accuracy: 0.9931 - val_loss: 0.1482 - val_accuracy: 0.9716\n",
      "Epoch 20/20\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1096 - accuracy: 0.9912 - val_loss: 0.1431 - val_accuracy: 0.9716\n"
     ]
    }
   ],
   "source": [
    "history = model_B.fit(X_train_B, y_train_B, epochs=20,\n",
    "                      validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "complimentary-amateur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_7 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_229 (Dense)            (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 275,801\n",
      "Trainable params: 275,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_B.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-pharmacology",
   "metadata": {},
   "source": [
    "#### First, you need to load model A and create a new model based on that model’s layers. Let’s reuse all the layers except for the output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "religious-possibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model(\"my_model_A.h5\")\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enhanced-inspector",
   "metadata": {},
   "source": [
    "- Note that model_A and model_B_on_A now share some layers. \n",
    "    - When you train model_B_on_A, it will also affect model_A.\n",
    "- If you want to avoid that, you need to **clone model_A before you reuse its layers.**\n",
    "    - To do this, you clone model A’s architecture with **clone.model()**, then copy its weights (since clone_model() does not clone the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "determined-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-asthma",
   "metadata": {},
   "source": [
    "- Now you could train model_B_on_A for task B, but since the new output layer was initialized randomly it will make large errors (at least during the first few epochs), so there will be large error gradients that may wreck the reused weights.\n",
    "- **To avoid this, one approach is to freeze the reused layers during the first few epochs,** giving the new layer some time to learn reasonable weights. \n",
    "    - To do this, **set every layer’s trainable attribute to False and compile the model:**\n",
    "\n",
    "### NOTE\n",
    "#### You must always compile your model after you freeze or unfreeze layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "discrete-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-sweden",
   "metadata": {},
   "source": [
    "- Now you can **train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again)** and continue training to fine-tune the reused layers for task B.\n",
    "- **After unfreezing the reused layers, it is usually a good idea to reduce the learning rate**, once again to avoid damaging the reused weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "united-maximum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.6101 - accuracy: 0.6184 - val_loss: 0.5798 - val_accuracy: 0.6379\n",
      "Epoch 2/4\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 0.5503 - accuracy: 0.6783 - val_loss: 0.5427 - val_accuracy: 0.6836\n",
      "Epoch 3/4\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.4850 - accuracy: 0.7558 - val_loss: 0.5109 - val_accuracy: 0.7140\n",
      "Epoch 4/4\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 0.4852 - accuracy: 0.7455 - val_loss: 0.4824 - val_accuracy: 0.7373\n",
      "Epoch 1/16\n",
      "7/7 [==============================] - 1s 72ms/step - loss: 0.4347 - accuracy: 0.7823 - val_loss: 0.3443 - val_accuracy: 0.8651\n",
      "Epoch 2/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.2954 - accuracy: 0.9143 - val_loss: 0.2592 - val_accuracy: 0.9290\n",
      "Epoch 3/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.2023 - accuracy: 0.9777 - val_loss: 0.2103 - val_accuracy: 0.9554\n",
      "Epoch 4/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1744 - accuracy: 0.9789 - val_loss: 0.1786 - val_accuracy: 0.9696\n",
      "Epoch 5/16\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 0.1341 - accuracy: 0.9809 - val_loss: 0.1558 - val_accuracy: 0.9757\n",
      "Epoch 6/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.1166 - accuracy: 0.9973 - val_loss: 0.1390 - val_accuracy: 0.9797\n",
      "Epoch 7/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.1133 - accuracy: 0.9931 - val_loss: 0.1264 - val_accuracy: 0.9838\n",
      "Epoch 8/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0996 - accuracy: 0.9931 - val_loss: 0.1162 - val_accuracy: 0.9858\n",
      "Epoch 9/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0831 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9888\n",
      "Epoch 10/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0773 - accuracy: 1.0000 - val_loss: 0.0999 - val_accuracy: 0.9899\n",
      "Epoch 11/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0687 - accuracy: 1.0000 - val_loss: 0.0939 - val_accuracy: 0.9899\n",
      "Epoch 12/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0717 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9899\n",
      "Epoch 13/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0565 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9899\n",
      "Epoch 14/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0802 - val_accuracy: 0.9899\n",
      "Epoch 15/16\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 0.0543 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9899\n",
      "Epoch 16/16\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.0470 - accuracy: 1.0000 - val_loss: 0.0738 - val_accuracy: 0.9899\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=4,\n",
    "                           validation_data=(X_valid_B, y_valid_B))\n",
    "\n",
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_B_on_A.compile(loss=\"binary_crossentropy\",\n",
    "                     optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "                     metrics=[\"accuracy\"])\n",
    "history = model_B_on_A.fit(X_train_B, y_train_B, epochs=16,\n",
    "                           validation_data=(X_valid_B, y_valid_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-winning",
   "metadata": {},
   "source": [
    "So, what's the final verdict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "pleasant-store",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1408 - accuracy: 0.9705\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.1408407986164093, 0.9704999923706055]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "occupational-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0682 - accuracy: 0.9935\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06818398833274841, 0.9934999942779541]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test_B, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-superintendent",
   "metadata": {},
   "source": [
    "Great! We got quite a bit of transfer: the error rate dropped by a factor of 8.2!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "affiliated-editing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.249999999999911"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100 - 98.35) / (100 - 99.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-equivalent",
   "metadata": {},
   "source": [
    "- Are you convinced? You shouldn’t be: I cheated! I tried many configurations until I found one that demonstrated a strong improvement. If you try to change the classes or the random seed, you will see that the improvement generally drops, or even vanishes or reverses. What I did is called “torturing the data until it confesses.”\n",
    "- When a paper just looks too positive, you should be suspicious: perhaps the flashy new technique does not actually help much (in fact, it may even degrade performance), but the authors tried many variants and reported only the best results (which may be due to sheer luck), without mentioning how many failures they encountered on the way.\n",
    "    - Most of the time, this is not malicious at all, but it is part of the reason so many results in science can never be reproduced.\n",
    "- Why did I cheat? It turns out that **transfer learning does not work very well with small dense networks,** presumably because small networks learn few patterns, and dense networks learn very specific patterns, which are unlikely to be useful in other tasks. \n",
    "- **Transfer learning works best with deep convolutional neural networks,** which tend to learn feature detectors that are much more general (especially in the lower layers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "motivated-voluntary",
   "metadata": {},
   "source": [
    "## Faster Optimizers\n",
    "- Training a very large deep neural network can be painfully slow. \n",
    "- So far we have seen four ways to speed up training (and reach a better solution):\n",
    "    - applying a good initialization strategy for the connection weights, \n",
    "    - using a good activation function, \n",
    "    - using Batch Normalization, \n",
    "    - and reusing parts of a pretrained network (possibly built on an auxiliary task or using unsupervised learning).\n",
    "- Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer. \n",
    "- In this section we will present the most popular algorithms: \n",
    "    - **Momentum Optimization**, \n",
    "    - **Nesterov Accelerated Gradient**, \n",
    "    - **AdaGrad**, \n",
    "    - **RMSProp**, \n",
    "    - and finally **Adam and Nadam optimization**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-radiation",
   "metadata": {},
   "source": [
    "### Momentum Optimization\n",
    "- Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance). \n",
    "- This is the very simple idea behind momentum optimization, proposed by Boris Polyak in 1964. \n",
    "- In contrast, regular Gradient Descent will simply take small, regular steps down the slope, so the algorithm will take much more time to reach the bottom.\n",
    "***\n",
    "- Gradient Descent updates the weights θ by directly subtracting the gradient of the cost function with regard to the weights multiplied by the learning rate η.\n",
    "    - It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
    "- Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multiplied by the learning rate η), and it updates the weights by adding this momentum vector.\n",
    "- In other words, the gradient is used for acceleration, not for speed. \n",
    "- To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a **new hyperparameter β, called the momentum**, \n",
    "    - which must be set between **0 (high friction)** and **1 (no friction)**. \n",
    "    - **A typical momentum value is 0.9.**\n",
    "- You can easily verify that if the gradient remains constant, the terminal velocity (i.e., the maximum size of the weight updates) is equal to that gradient multiplied by the learning rate η multiplied by 1/1−β (ignoring the sign).\n",
    "    - For example, if β = 0.9, then the terminal velocity is equal to 10 times the gradient times the learning rate, so **momentum optimization ends up going 10 times faster than Gradient Descent!** \n",
    "    - **This allows momentum optimization to escape from plateaus much faster than Gradient Descent.**\n",
    "***\n",
    "- Gradient Descent goes down the steep slope quite fast, but then it takes a very long time to go down the valley. \n",
    "- In contrast, momentum optimization will roll down the valley faster and faster until it reaches the bottom (the optimum).\n",
    "- **In deep neural networks that don’t use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using momentum optimization helps a lot.** \n",
    "    - It can also help roll past local optima.\n",
    "\n",
    "Implementing momentum optimization in Keras;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "aware-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "criminal-amount",
   "metadata": {},
   "source": [
    "- The one drawback of momentum optimization is that it adds yet another hyperparameter to tune. \n",
    "    - However, the momentum value of 0.9 usually works well in practice and almost always goes faster than regular Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-luxury",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "- One small variant to momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla momentum optimization.\n",
    "- The Nesterov Accelerated Gradient (NAG) method, **also known as Nesterov momentum optimization**, measures the gradient of the cost function not at the local position θ but slightly ahead in the direction of the momentum, at θ + βm.\n",
    "- This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than the gradient at the original position.\n",
    "\n",
    "NAG is generally faster than regular momentum optimization. To use it, simply set **nesterov=True** when creating the SGD optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "alert-thought",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-dallas",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "- Gradient Descent starts by quickly going down the steepest slope, which does not point straight toward the global optimum, then it very slowly goes down to the bottom of the valley. \n",
    "- It would be nice if the algorithm could correct its direction earlier to point a bit more toward the global optimum. \n",
    "    - The AdaGrad algorithm achieves this correction by scaling down the gradient vector along the steepest dimensions.\n",
    "- this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. \n",
    "    - This is called an **adaptive learning rate**. \n",
    "    - It helps point the resulting updates more directly toward the global optimum.\n",
    "    - One additional benefit is that it requires much less tuning of the learning rate hyperparameter η.\n",
    "- **AdaGrad frequently performs well for simple quadratic problems, but it often stops too early when training neural networks.** \n",
    "    - The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum.\n",
    "- So even though Keras has an Adagrad optimizer, **you should not use it to train deep neural networks** (it may be efficient for simpler tasks such as Linear Regression, though).\n",
    "- Still, understanding AdaGrad is helpful to grasp the other adaptive learning rate optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "inside-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adagrad(lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-conflict",
   "metadata": {},
   "source": [
    "### RMSProp\n",
    "- As we’ve seen, AdaGrad runs the risk of slowing down a bit too fast and never converging to the global optimum. \n",
    "- The RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training). It does so by using exponential decay in the first step.\n",
    "- **The decay rate β is typically set to 0.9.**\n",
    "- Yes, it is once again a new hyperparameter, but this default value often works well, so you may not need to tune it at all.\n",
    "- Except on very simple problems, **this optimizer almost always performs much better than AdaGrad.** \n",
    "    - In fact, it was the preferred optimization algorithm of many researchers until **Adam optimization** came around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "supreme-illness",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-declaration",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimization\n",
    "### Adam\n",
    "- **Adam**, which stands for **adaptive moment estimation**, combines the ideas of momentum optimization and RMSProp: just like momentum optimization, it keeps track of an exponentially decaying average of past gradients; and just like RMSProp, it keeps track of an exponentially decaying average of past squared gradients.\n",
    "- The momentum decay hyperparameter **$β_1$ is typically initialized to 0.9**, \n",
    "    - while the scaling decay hyperparameter **$β_2$ is often initialized to 0.999**.\n",
    "- Since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter η.\n",
    "    - You can often use the default value **η = 0.001**, making Adam even easier to use than Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "oriental-encounter",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-jonathan",
   "metadata": {},
   "source": [
    "### Adamax\n",
    "- This is just one more optimizer you can try if you experience problems with Adam on some task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "outstanding-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adamax(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imported-questionnaire",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "- Nadam optimization is Adam optimization plus the Nesterov trick, so it will often converge slightly faster than Adam.\n",
    "- In his report introducing this technique, the researcher Timothy Dozat compares many different optimizers on various tasks and finds that Nadam generally outperforms Adam but is sometimes outperformed by RMSProp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "hired-efficiency",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affecting-thanksgiving",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "- Adaptive optimization methods (including RMSProp, Adam, and Nadam optimization) are often great, converging fast to a good solution. \n",
    "- However, a 2017 paper by Ashia C. Wilson et al. showed that they can lead to solutions that generalize poorly on some datasets. \n",
    "- So when you are disappointed by your model’s performance, try using plain Nesterov Accelerated Gradient instead: your dataset may just be allergic to adaptive gradients. Also check out the latest research, because it’s moving fast.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-rebel",
   "metadata": {},
   "source": [
    "- All the optimization techniques discussed so far only rely on the first-order partial derivatives (Jacobians).\n",
    "- The optimization literature also contains amazing algorithms based on the second-order partial derivatives (the Hessians, which are the partial derivatives of the Jacobians).\n",
    "    - Unfortunately, these algorithms are very hard to apply to deep neural networks because there are n² Hessians per output (where n is the number of parameters), as opposed to just n Jacobians per output.\n",
    "- Since DNNs typically have tens of thousands of parameters, the second-order optimization algorithms often don’t even fit in memory, and even when they do, computing the Hessians is just too slow.\n",
    "\n",
    "Table 11-2 compares all the optimizers we’ve discussed so far (* is bad, ** is average, and *** is good).\n",
    "<img src=\"t2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structural-hostel",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "- Finding a good learning rate is very important. \n",
    "    - If you set it much too high, training may diverge (as we discussed in “Gradient Descent”). \n",
    "    - If you set it too low, training will eventually converge to the optimum, but it will take a very long time.\n",
    "    - If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never really settling down.\n",
    "    - If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n",
    "<img src=\"11-8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "primary-parts",
   "metadata": {},
   "source": [
    "- You can find a good learning rate by training the model for a few hundred iterations, exponentially increasing the learning rate from a very small value to a very large value, and then looking at the learning curve and picking a learning rate slightly lower than the one at which the learning curve starts shooting back up. \n",
    "    - You can then reinitialize your model and train it with that learning rate.\n",
    "- But you can do better than a constant learning rate: **if you start with a large learning rate and then reduce it once training stops making fast progress,** you can reach a good solution faster than with the optimal constant learning rate.\n",
    "- There are many different strategies to reduce the learning rate during training. \n",
    "    - It can also be beneficial to start with a low learning rate, increase it, then drop it again.\n",
    "    - These strategies are called **learning schedules.**\n",
    "\n",
    "These are the most commonly used learning schedules:\n",
    "### Power scheduling\n",
    "#### lr = lr0 / (1 + steps / s)**c\n",
    "- The initial learning rate lr0 , the power c (typically set to 1), and the steps s are hyperparameters.\n",
    "- The learning rate drops at each step.\n",
    "- After s steps, it is down to lr0/ 2\n",
    "- After s more steps, it is down to lr0/ 3, then it goes down to lr0/ 4, then lr0/ 5, and so on.\n",
    "    - **As you can see, this schedule first drops quickly, then more and more slowly.** \n",
    "    - Of course, power scheduling requires tuning lr0 and s (and possibly c).\n",
    "- The decay is the inverse of s (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that c is equal to 1.\n",
    "Keras uses c=1 and s = 1 / decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "adverse-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "healthy-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "anonymous-newspaper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.5980 - accuracy: 0.7933 - val_loss: 0.4031 - val_accuracy: 0.8596\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3829 - accuracy: 0.8637 - val_loss: 0.3715 - val_accuracy: 0.8724\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3491 - accuracy: 0.8769 - val_loss: 0.3746 - val_accuracy: 0.8738\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3277 - accuracy: 0.8817 - val_loss: 0.3500 - val_accuracy: 0.8796\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.3172 - accuracy: 0.8858 - val_loss: 0.3450 - val_accuracy: 0.8782\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2922 - accuracy: 0.8940 - val_loss: 0.3416 - val_accuracy: 0.8820\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2870 - accuracy: 0.8973 - val_loss: 0.3358 - val_accuracy: 0.8870\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2720 - accuracy: 0.9032 - val_loss: 0.3410 - val_accuracy: 0.8842\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2730 - accuracy: 0.9002 - val_loss: 0.3294 - val_accuracy: 0.8868\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2585 - accuracy: 0.9067 - val_loss: 0.3266 - val_accuracy: 0.8888\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2528 - accuracy: 0.9100 - val_loss: 0.3275 - val_accuracy: 0.8882\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2484 - accuracy: 0.9103 - val_loss: 0.3338 - val_accuracy: 0.8826\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2419 - accuracy: 0.9143 - val_loss: 0.3262 - val_accuracy: 0.8902\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2373 - accuracy: 0.9145 - val_loss: 0.3293 - val_accuracy: 0.8894\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2363 - accuracy: 0.9154 - val_loss: 0.3250 - val_accuracy: 0.8884\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2309 - accuracy: 0.9178 - val_loss: 0.3214 - val_accuracy: 0.8910\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 8s 5ms/step - loss: 0.2233 - accuracy: 0.9212 - val_loss: 0.3245 - val_accuracy: 0.8906\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2247 - accuracy: 0.9195 - val_loss: 0.3198 - val_accuracy: 0.8934\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2234 - accuracy: 0.9214 - val_loss: 0.3239 - val_accuracy: 0.8904\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.2227 - accuracy: 0.9224 - val_loss: 0.3219 - val_accuracy: 0.8906\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2194 - accuracy: 0.9229 - val_loss: 0.3217 - val_accuracy: 0.8918\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2163 - accuracy: 0.9229 - val_loss: 0.3193 - val_accuracy: 0.8938\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2127 - accuracy: 0.9250 - val_loss: 0.3206 - val_accuracy: 0.8904\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2077 - accuracy: 0.9275 - val_loss: 0.3224 - val_accuracy: 0.8896\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 7s 4ms/step - loss: 0.2103 - accuracy: 0.9254 - val_loss: 0.3224 - val_accuracy: 0.8916\n"
     ]
    }
   ],
   "source": [
    "n_epochs=25\n",
    "history = model.fit(X_train_scaled,  y_train, epochs=n_epochs,\n",
    "                   validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "anonymous-taiwan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEeCAYAAAC30gOQAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAzlklEQVR4nO3deXxU9b3/8dcnCSQhECBsQlhdQFBURMS6FUUv3vaqVKy/XpdKtbW19ra1vSruuLT2qrX2Xq2trZSqaOuCuFWxKrhvKAqyKrLIIvsWyEby+f1xTnCYTJITzMwkmffz8ZgHM+d8z/d85hjzyfec72LujoiISFPLSncAIiLSOinBiIhIUijBiIhIUijBiIhIUijBiIhIUijBiIhIUijBiLQgZjbezEqSVPfHZjaxkccsM7P/ruuzZDYlGGlxzGyymXn4qjSzz8zsdjMrSHdsDTGzAWb2oJmtNLNyM1ttZs+a2bB0x9ZERgB/SHcQ0jzkpDsAkb30InAe0AY4DvgLUABcnM6gaphZG3evjN8G/AtYApwFrAKKgZOBopQHmQTuvj7dMUjzoRaMtFTl7v6Fu3/u7g8BU4CxAGaWa2Z3mtlaMyszs7fN7NiaA83sHTO7IubzlLA1tE/4uZ2ZVZjZMeFnM7PLzWyJmZWa2VwzOzfm+P7h8f9pZi+bWSnwwwQxHwTsB1zi7m+6+/Lw3xvc/aWY+grN7B4zWxPGv8DM/l9sRWY2OryltcPMZpjZgLj9p5rZ++HxS83sV2bWNmZ/dzN7Mvw+y83sgvhgw+90Zty2em+BJbhl5mZ2kZk9Gsb6Wey1C8uMNLMPwlhnm9k3wuNG1XUeaRmUYKS1KCVozQDcCvw/4AJgGDAXeN7Meob7ZwInxBz7dWADMCr8fAxQCbwbfr4ZuBC4BBgC3AL8ycy+GRfDLQS3h4YA0xLEuB6oBsaZWcK7B2ZmwHNhTN8L6/oFUBFTLBe4Mvx+XwM6AX+MqWMMQcK9iyCpXQCcCfw6po7JwP7ASQSJ+btA/0QxNYHrgCeBQ4F/AJPMrF8Ya3vgGWAhMBy4HLgtSXFIqrm7Xnq1qBfBL8dnYj4fSZAg/kFwm6wC+G7M/myC21I3h5//HSghuEV8ALAd+BXwp3D/r4B/he8LCJLXcXEx3An8M3zfH3DglxFivwTYEZ7/FeAm4KCY/ScTJKHBdRw/PjzXoJht54TfOSv8/CpwbdxxY8NzGjAwrOOYmP39gCpgYsw2B86Mq2cZ8N+N+OzALTGfc4CdwLnh5x8Cm4D8mDJnh8eNSvfPml5f7aUWjLRUp5hZiZmVAW8R/FL9L4JbUG2AN2oKuntVWGZIuOk1glbACIJWy2sEz3RGhftHEbRyCI/JI2gBldS8CJ717BcX06yGgnb3u4F9CH6Jvg6cDnxoZueFRYYBa9x9QT3VlLv7opjPq8Pv3Cn8PBy4Oi7ehwiS5T7AYIIkVtNCw92Xh/Ukw5yY8+wiaMl1DzcdCHzs7qUx5d9JUhySYnrILy3Vq8BFBLeyVnv4QD3mNliiacKDP6ndS8zsA4LbZAcBMwgSUD8zO4Ag8VweHlPzR9ipwIq4+irjPu+IEri7bweeAp4ys2uA6QQtmQcIWhgN2RVfZVysWcANwKMJjl0f8Rw19caXbZOoYAPir5PzZaxG4v9W0goowUhLtdPdP02w/VOC20XHAp8BmFk2wbOKh2LKzSRIMIOBO929zMzeAa5mz+cv84FyoJ+7v9zUX8Ld3cwWAoeHmz4AeprZ4AZaMfX5ADiwjuuDmS0g+AU/Angz3NYX6BVXdD3QM+a4HrGfm8gC4Ltmlh/Tijmyic8haaIEI62Ku+8ws3uA35jZBmApcCnQgz3HZ8wEfknQ6vggZtvVwIyaFpG7bzez24HbwwfwrwLtgaOAane/N2psZnYYQcviAYLEVUHwMP8C4OGw2EsEt4geN7NLgcUED+ML3H1axFPdCDxjZsuBRwhaPAcDR7r75e6+yMyeJ+iocBHBM6Y7wn9jvQxcYmZvEjyf+TVQFvX7RjSFoBPFn83s1wRJ7qpwn1o2LZyewUhrdAXBL9a/Ah8ChwCnuPuamDKvEfwCey18RgPBrbJsvnz+UuNaYCLw38A8grEs4wiSV2OsJGhVXQe8Hcb2S+B2gudHuHs1QSeEN4AHCf7C/z3QtnZ1ibn7dOCbBC20d8PXBPa8xTc+jP9l4GmC1t2yuKp+GcY7E3iMYKzRuqhxRIy1hOD240HAbIIeZBPD3U2dzCTFzF1/JIhI82FmpwNPAN3dfUO645G9p1tkIpJWZnY+QUvpc4JbeXcCTyu5tHwpvUVmZkVm9kQ4one5mZ1dT9lLzewLM9tqZpPMLDdm30/MbJYFczlNTnDsaDNbaGY7w1HO/ZL0lUTkq+tB8FxqEXA3wUDTc+s9QlqElN4iM7OHCZLahcBhwLPA0e4+L67cGOB+4ESCvvlPAG+7+4Rw/xkE/fjHEAzQGh9zbFeCQXXfJ7i3fBPBILmjkvndRERkTylLMBbMdLsZONjdF4fbHgBW1SSOmLIPAcvc/arw82hgirvvE1fuZqB3XIK5CBjv7kfHnHcDMMzdFybr+4mIyJ5S+QxmIFBVk1xCHxF004x3EMHcRbHlephZF3ff2MB5DgrLA7u7rS4Jt++RYMJkdBFAVn7h8JyO3Xfv61+oDnYA1dXVZGXpWsTTdUlM16W21n5NFi9evMHduyXal8oE0x7YGrdtK9AhQtma9x2AhhJMe4IBYg2eJxzDcC9Abs8DvOf5dwJQ3CmfNyac2MBpMsPMmTMZNWpUusNodnRdEtN1qa21X5NwvFVCqUyrJUBh3LZCgokGGypb8z5R2a9ynlpyc7K4bMygKEVFRKQeqUwwi4GccK6nGocSDFyLNy/cF1tubYTbY7WODZ/B7FfHefZgwNDiQsYOK45wGhERqU/KEoy77wCmAjeaWYEFizmdTtA9Md79wIVmNsTMOgPXEEzRDoCZ5ZhZHsGo62wzy4tZX+MJ4GAzGxeWuQ6Y09AD/v6FWXzvmAF8tHIra7dpALGIyFeV6idPPwbyCaabeBi42N3nmVnfcFrxvgDu/jzBolEzgOXh6/qYeq4hmDdpAkF/+dJwGx4s2TqOYE2PzcBI4DtRght/dH92VTsPvl3nLUUREYkopSP53X0T4bK2cdtXEDycj912B8EEfInqmciX8xUl2v8iwToTjdK3SztOGtyDKe+s4JIT9ievTXZjqxARkVDr7Tu3ly44ZgCbdlTw5Ier0h2KiEiLpgQT56h9ixjcs5BJry9DE4GKiOw9JZg4ZsYFx/Rn0drtvLkkSqc1ERFJRAkmgVMP7UXX9m2Z9Hpjl/sQEZEaSjAJ5LXJ5pyR/Xhp4TqWboi0zLqIiMRRgqnDOUf1pW12FpPfUCtGRGRvKMHUoXuHPE49tBePvr+SraWV6Q5HRKTFUYKpx/eO6c/Oiioeee/zdIciItLiKMHU4+DijowcUMTkN5exq6o63eGIiLQoSjANuODYAazaUsq/5q9NdygiIi2KEkwDThrcgz5F+UzSw34RkUZRgmlAdpYx/ugBvLdsM3NWbkl3OCIiLYYSTARnHdGb9rk5/PWNZekORUSkxVCCiaBDXhu+fURvnpmzWmvFiIhEpAQTkdaKERFpHCWYiPp1Kdi9VkxZZVW6wxERafaUYBpBa8WIiESnBNMIWitGRCQ6JZhG0FoxIiLRKcE0ktaKERGJRgmmkbRWjIhINEowe0FrxYiINCwn3QG0RN075HFI70Luf2s597+1nF6d8rlszCDGDitOd2giIs2GEsxemDZ7FXNXbaOmH9mqLaVcOXUugJKMiEhIt8j2wm3TF1G+a8/1YUorq7ht+qI0RSQi0vwoweyF1VtKG7VdRCQTKcHshV6d8hu1XUQkEynB7IXLxgwiv032Htuys4zLxgxKU0QiIs2PHvLvhZoH+bdNX8TqLaW0y81mR3kV+3dvn+bIRESaDyWYvTR2WPHuRLO1tJITbp/JDU/P45Effg0zS3N0IiLpp1tkTaBjfhsuHzOI95Zt5qmPVqc7HBGRZkEJpol8+4g+DC3uyK//uYAd5bvSHY6ISNopwTSR7Cxj4mkHsXZbOXfN+DTd4YiIpJ0STBMa3q8zZxxezH2vLdVEmCKS8VKaYMysyMyeMLMdZrbczM6up+ylZvaFmW01s0lmlhu1HjM7y8wWmNl2M5tvZmOT+LX2MOGUA2mbk8VNz8xP1SlFRJqlVLdg7gYqgB7AOcA9ZnZQfCEzGwNMAEYD/YF9gRui1GNmxcCDwC+AQuAy4CEz656cr7Sn7oV5/HT0/ry8cB0vL1ybilOKiDRLKUswZlYAjAOudfcSd38deAo4L0Hx84H73H2eu28GbgLGR6ynN7DF3Z/zwLPADmC/JH69PYw/egD7divgxqfnU76rKlWnFRFpVlI5DmYgUOXui2O2fQR8PUHZg4An48r1MLMuQN8G6pkFLDCz04BngVOBcmBO/EnM7CLgIoBu3boxc+bMvfhaiX2r7y5++34519z/Et/ct22T1ZtqJSUlTXpdWgtdl8R0XWrL5GuSygTTHtgat20r0CFC2Zr3HRqqx92rzOx+4CEgj+BW2rfdvdZTd3e/F7gXYNCgQT5q1KhGfJ36jQLmlM7i2U838MtxR7FPx7wmqzuVZs6cSVNel9ZC1yUxXZfaMvmapPIZTAnBM5FYhcD2CGVr3m9vqB4zOwm4leB3fFuCls1fzOywvQ9971z7zSHsqnZ+89yCVJ9aRCTtUplgFgM5ZnZAzLZDgXkJys4L98WWW+vuGyPUcxjwqrvPcvdqd38PeAc4qWm+RnR9u7Tjh8fvy7QPV/Pesk2pPr2ISFqlLMGEt6imAjeaWYGZHQOcDjyQoPj9wIVmNsTMOgPXAJMj1vMecFxNi8XMhgHHkeAZTCpcPGo/enbM4/on51FV7Q0fICLSSqS6m/KPgXxgHfAwcLG7zzOzvmZWYmZ9Adz9eYLbXDOA5eHr+obqCY99BZgIPGZm24HHgV+7+wsp+H61tGubw1XfGMz8Ndv4+3sr0hGCiEhapHQ2ZXffBIxNsH0FwcP72G13AHc0pp6Y/XcBd32FUJvUfxzSkwffXs7t0xfxzaE96dSu5fYqExGJSlPFpIBZME/Z1tJK7vjX4oYPEBFpBZRgUmRwz0LOO6ofD769nPmrt6U7HBGRpNOCYyl06ckDeXTW54y9+w0qq6rp1Smfy8YM2r1wmYhIa6IEk0IzF62nstqprAp6k63aUsqVU+cCKMmISKujW2QpdNv0RbuTS43Syipum74oTRGJiCSPEkwKrd5S2qjtIiItmRJMCvXqlN+o7SIiLZkSTApdNmYQ+W2ya20/c3jvNEQjIpJcSjApNHZYMbecMZTiTvkY0LNjHkUFbXhk1uds2lGR7vBERJqUepGl2NhhxXv0GJu7civj7nmTXzzyIZPOH0FWlqUxOhGRpqMWTJoN7d2R604dwsxF6/nDzE/THY6ISJNRgmkGzhnZl9MO7cUd/1rMm0s2pDscEZEmETnBmNm/m9kzZjbfzPqE275vZqOTF15mMDNuOWMoA7oW8NOHP2TdtrJ0hyQi8pVFSjBmdg7wCPAJMABoE+7KBi5PTmiZpSA3h3vOHU5JeSX/9fBsdlVVpzskEZGvJGoL5nLgB+5+KbArZvvbBCtIShMY2KMDvxo7lHeWbuJ3L2rWZRFp2aImmAOAtxJsLwEKmy4cGTe8N98Z0Ye7ZyxhxsJ16Q5HRGSvRU0wq4GBCbYfDyxpunAEYOJpBzG4ZyGXPvIhqzSNjIi0UFETzL3A/5rZMeHnPmZ2PsGyxvckJbIMltcmmz+cczi7qpxLpnxAxS49jxGRlidSgnH3W4GpwL+AAmAG8Efgj+5+d/LCy1wDuhZw65mH8OHnW/jNcwvTHY6ISKNF7qbs7lcDXYEjgaOAbu5+bbICE/jG0J6MP7o/k95YynNz16Q7HBGRRok0VYyZTQJ+5u7bgVkx2wuA/3P3C5IUX8a76huDmf35Fn7+99lMfHoe67aVayVMEWkRorZgzgcSzSmfD3y36cKReG1zsjj90J6UVzlrt5XjfLkS5rTZq9IdnohInepNMGZWZGZdAAM6h59rXt2A/wDWpiLQTHbf68tqbdNKmCLS3DV0i2wD4OFrfoL9Dlzf1EHJnrQSpoi0RA0lmBMIWi8vA+OATTH7KoDl7r46SbFJqFen/ITjYbQSpog0Z/UmGHd/BcDMBgCfu7sGZKTBZWMGceXUuZRWVu2x/ej9u6QpIhGRhkXqRebuywHMrBfQF2gbt//Vpg9NatT0Frtt+iJWbymlZ8c8OrVrw2Pvr+To/brwrWFacllEmp+o3ZR7AQ8RTA3jBLfNPKZI7YXmpUnFr4RZVlnFBZPf478fnUN+m2xOObhnGqMTEaktajflO4EqYAiwEzgO+DawADglKZFJvfLaZPPn7x7BYX068V8Pz2bGIk2MKSLNS9QE83XgCndfSNByWe/uU4ErgJuSFZzUryA3h0njRzBonw786IH3eWvJxnSHJCKyW9QEk0/QZRmCnmTdw/fzgUOaOiiJrmN+G+6/YCR9i9px4d/e4/3lm9MdkogIED3BLAQODN9/CPzIzPoBlwAaTp5mRQVtmfL9kXTvkMv4v77Lx6u2pjskEZHICeb3wD7h+xuBfwM+A34MXJWEuKSRuhfmMeUHR1GY14bvTnqXT9ZuT3dIIpLhok7XP8XdJ4fvPwD6AyOAvu7+aNSThVPMPGFmO8xsuZmdXU/ZS83sCzPbamaTzCw3aj1m1s7M/mBmG8LjM6IbdXGnfKZ8fyTZWcY5f3mH5Rt3pDskEclgkafrj+XuO8NEs8PMJjTi0LsJZgDoAZwD3GNmB8UXMrMxwARgNEEy2xe4oRH13AsUAYPDfy9tRIwtWv+uBUz5/kgqq6o5+8/vaDoZEUmbBsfBmFlXYCRQCbzk7lVm1obg+cuVBGNgfhOhngKC6WYOdvcS4HUzewo4jyCZxDofuM/d54XH3gRMASY0VI+ZDQJOA3q7+7awvvcbiq81GdijAw9cOJL/vPdtTrvrdXKysli7rUzT/ItIStWbYMzsaOBZoCNB9+T3zGw88ATQhqCL8qSI5xoIVLn74phtHxF0gY53EPBkXLke4czOfRuoZySwHLjBzM4D1gAT3f3xBN/vIuAigG7dujFz5syIX6VlOLE3PLmkYvfnVVtKufzRD5m/YD5H92oTqY6SkpJWd12agq5LYroutWXyNWmoBXMTMB24GbgA+DnwDMGD/gfc3es+tJb2QHz3pq1Ahwhla953iFBPb+Bg4HGgF/A14Fkzm+/uC2IPcvd7CW6nMWjQIB81alQjvk7zd/XbLwO79thWUQ3PrsjmqrNHRapj5syZtLbr0hR0XRLTdaktk69JQ89gDgVucvePgWsIWjFXuvv9jUwuACVAYdy2QiBRd6f4sjXvt0eop5Tgdt7N7l4RTtg5g6DnW0bRNP8ikk4NJZgiYD0ED/YJpomZvZfnWgzkmNkBMdsOBeYlKDsv3Bdbbq27b4xQz5y9jK/VqWs6/6KCtgm3i4g0pSi9yGpWsuxC0IIpjFvZsijKidx9BzAVuNHMCszsGOB04IEExe8HLjSzIWbWmaD1NDliPa8CK4ArzSwn3D+K4FZfRrlszCDy2+w5D6kBm3ZUMOWd5ekJSkQyRpQEM5+gFbOO4PnHe+Hn9QTTx6xvxPl+TDDtzDrgYeBid59nZn3NrMTM+gK4+/PArQS3tpaHr+sbqic8tpIg4XyD4NnMn4HvhvOoZZSxw4q55YyhFHfKxwjGydxyxlBGDerG1U98zM3PzKequrF3OkVEoomyomWTcfdNwNgE21cQJK/YbXcAdzSmnpj98wge7me8+Gn+Ac4c3pubn13AX15fyvJNO/n9dw6jXdtIKzeIiEQWaUVLaV1ysrOYeNpB9O/Sjhufmc9Zf3qL+84fQY/CvHSHJiKtyF6N5JfWYfwxA/jL+UewdP0Oxt79BvNXb2v4IBGRiJRgMtyJB/bg0R8dDcC3//gmLy9cm+aIRKS1UIIRhvQqZNolxzCgWwHf/9ssJr+xNN0hiUgroCe7AkCPwjwe+eHX+NnfP2Ti0/N5acFalmzYweotZRS//bLmMBORRlMLRnZr1zaHP547nBMGdeW1TzeyeksZEMxhduXUuUybrbXlRCS6SC0YM6trQksHyoBPgX+4++qmCkzSIzvLWLy29joypZVV3DZ9kVoxIhJZ1Ftk3YDjgGrg43DbwQQDw98HziAYWX+cu3/Y1EFKamkOMxFpClFvkb0BPEewxsrx7n48wazF/wReAPoRTOv/26REKSlV1xxmbXOyWL+9PMXRiEhLFTXB/Ay4MZzwEtg9+eWvgEvdvQL4H+CwJo9QUi7RHGZtso1dVdWccuervLRAXZlFpGFRE0x7oGeC7fvw5RQv21CvtFYhdg4zCOYwu+3MQ3n+58fTvTCPC/82i2umzaW0oirNkYpIcxY1ITwB3GdmlxNMdunAkQQTUk4NyxxJMJW+tAI1c5jFL5Y07ZKj+e0Li7n31c94a8lGfv+dYRxc3DF9gYpIsxW1BfMjgunuHwSWAJ+F758nmNkYYAHwg6YOUJqX3JxsrvrGYB68cCQl5bv41h/e4E+vLKFaszKLSJxICcbdd7r7jwgWIBsGHA4UufvF4fosuPuH6kGWOY49oCvP/+x4Rh/Yg1ueW8i5973Dmq3qZSYiX2rUM5MwmWjFSAGgc0Fb7jn3cB6dtZKJT8/jlDtfY+xhvXhxwTpWbymlV6d8zQAgksGiDrTMI+hJNhroTlzLx90PafrQpCUwM84a0YcRA4r47qR3+NtbX66UWTMDAKAkI5KBorZg/gB8C3gUeJPgIb/IbgO6FiRcHVMzAIhkrqgJZizwbXd/MYmxSAu3Jpy7LJ5mABDJTFF7ke0EPk9mINLy1TUDgAO/fWGRxs2IZJioCeZW4BdmptmXpU6JZgDIy8ni8L6d+L+XP+WkO15h+rwvcNcdVpFMEPUW2ckEk12eYmbzgcrYne5+WlMHJi1PzXOW26YvqtWL7O3PNnL9k/P44QPv8/WB3Zh42kEM6FqQ5ohFJJmiJpgNBKP5RepVMwNAvKP27cIzPz2W+99azu/+tZgxv3uVHxw/gEtO2J92bTXDkEhrFOn/bHf/XrIDkdavTXYWFx47gFMP6cktzy3k7hlLmDZ7Ndf+x2BKK6q4/YXFGj8j0oroT0dJue6Fefzu/x3Gfx7Zl+ue/JgfPfgBWQY1vZw1fkakdajzob2ZzTGzzuH7ueHnhK/UhSutyZEDinjmv46lY34O8UNoasbPiEjLVV8L5nGgZnWpx1IQi2SgnOwstpXuSrhP42dEWrY6E4y735DovUhT69Upn1V1JJO7Xv6E8ccMoH2u7uaKtDQa1yJpl2j8TG5OFoN7duD2FxZz/K0z+NMrSzRQU6SFiTrZZRHB8sh1TXZZ2PShSaaob/zM7BWb+d2Ln3DLcwv582tL+fGo/Th7ZF/y4hKSiDQ/Ue873EewDsy9wGo02aU0sbrGzwzr25n7LziS95Zt4o4XFnPjM/O599XP+MmJ+3PWEX3459w1CROTiKRf1AQzGjjZ3d9JZjAidRnRv4iHLzqKNz/dwG//tZhrpn3Mb19YREn5Liqrgr931L1ZpHmJ+gxmHVCSzEBEojh6/6489qOvMfl7I/ZILjXUvVmk+YiaYK4GbjSz9skMRiQKM2PUoO7sqkp8p1bdm0Wah6gJ5hrg34B1ZrZAAy2lOahveYArp87l03XbUxuQiOwhaoJ5DLgd+B/g7wSDMGNfkZhZkZk9YWY7zGy5mZ1dT9lLzewLM9tqZpPMLLex9ZjZ9WbmZnZS1Bil5aire/PX9i1i6gcrOemOVxn/13d57ZP1WiJAJA0afMhvZm2AAuBud1/eUPkG3A1UAD2Aw4Bnzewjd58Xd84xwATgRIJea08AN4TbItVjZvsBZwJrvmLM0kzV1715Y0k5U95Zwf1vLee8+95lUI8OXHjsAE47rBd5bbKZNnuVep+JJFmDCcbdK83sYuAPX+VEZlYAjAMOdvcS4HUzewo4jy8TR43zgftqEoaZ3QRMASY0op67gCu+atzSvNXVvblL+1x+OvoAfvj1fXn6ozX85bXPuPzxOdw6fSFH9OvMzMXrKausBtT7TCRZonZTfoGgNTHpK5xrIFDl7otjtn0EfD1B2YOAJ+PK9TCzLkDfhuoxs28DFe7+TzOrMyAzuwi4CKBbt27MnDmzUV8oE5SUlLT469IVuOJQZ0GfPKYvq+T5eWtrlSmtrOKmJz+i09ZPItXZGq5LMui61JbJ1yRqgnkJ+LWZHQK8D+yI3enuUyPU0R7YGrdtK9AhQtma9x0aqifs6fZrgk4J9XL3ewkGjzJo0CAfNWpUQ4dknJkzZ9JarssJwI+BAROeTThSeFOZR/6urem6NCVdl9oy+ZpETTB3hf/+NME+B6LM21ECxE8pUwgk6uoTX7bm/fYI9dwAPODuSyPEJBmorsk1HfjFPz7krBF9GDmgiPpavyLSsEi9yNw9q55X1EmhFgM5ZnZAzLZDgXkJys4L98WWW+vuGyPUMxr4adgD7QugD/CImV0RMU5p5erqfXb0fkX8a/5avnPv25xw+0zunvEpa7eVpSlKkZYvZXOgu/sOM5tKMGDz+wS9v04Hjk5Q/H5gsplNIegFdg0wOWI9o4E2MXW9B/wCeK6Jv5K0UPX1PiutqOKfc9fwj1mfc9v0Rfz2hUWcMKg7Z43ow4kHdufZOcHcZ6u2lFL89svqfSZSj8gJJpxR+RSCh+xtY/e5+40Rq/kxQUeBdcBG4GJ3n2dmfYH5wBB3X+Huz5vZrcAMIJ9grM31DdUTxrIxLu4qYHPY40wEqLv3WX7bbMYN78244b1ZumEHj8z6nMffX8lLC9fRPjebsspqdlVr7jORKKJO138U8CzBCpfdgFVAz/DzMiBSgnH3TcDYBNtXEDy8j912B3BHY+qpo2z/KOVE4g3oWsAVpxzIL08eyMxF6/nJQx/sTi41auY+U4IRqS3qSP7bCMahFANlBF2W+wKzCEb3i7RaOdlZnDSkB+W7qhPuX7WllAfeWsaGkvKE+0UyVdQEcwhwlwfzbVQBue6+lmAg48QkxSbSrNQ191lOlnHtk/M48lcvcs5f3ubv765gy86KFEcn0vxEfQYT+3/LWqAfsICgy3Cvpg5KpDm6bMwgrpw6l9LKL5duzm+TzS1nDGVwz0KembOaZ+asYcLUuVwz7WOOPaArpx7Si5MP6sHLC9ZpahrJOFETzAfACIIuwjOBm82sB3AuoNmUJSPE9j5btaWU4rhEMWifQfzi5IHMW72Np+es5pmP1vDLRz8i+7FgjE3N4xt1DpBMETXBXM2XI+6vIehG/H8ECed7SYhLpFmq6X1W1+hsM+Pg4o4cXNyRCaccyIefb+Hc+95hR3nVHuVKK6u45bkFSjDSqkVKMO4+K+b9euDfkxaRSCthZgzr25mdccmlxtpt5Xzj969x0pAenDS4Owf36khWlmYPkNajUQMtzewIYD/gmXDAYwFQ7u67khKdSCtQ19Q0hXk5tM/N4a6XP+F/X/qEHoW5nHhgkGyO2b+rlhWQFi/qOJgewFMEz2EcOAD4jGCcShnws2QFKNLS1dU54MbTD2bssGI276hgxqJ1vLhgLU9/tJqH311BXpss9uvWnsVrt1NZpYGd0jJFbcH8DvgC6AKsiNn+KMGzGBGpQ31T0wB0LmjLGYf35ozDe1O+q4p3PtvESwvW8uDbK6jy2gM7b31+oRKMtAhRE8xoYLS7b46bYXYJwYBLEalHXVPTxMvNyeb4gd04fmA37n8r8QKyq7eWcf6kdznugK4ce0BXBvXooJmfpVmKmmDy2XMsTI1uBLfIRKSJ1fXspiA3m5Wbd3LzswsA6NYhl2P37xoknP270r0wT89upFmImmBeBcYDV4Wf3cyyCUbyv5SEuEQyXl3Pbn41dihjhxWzekspr3+ygdc+3cAri9fzxOxVAOxTmMv6kgqqNCmnpFnUBHM58IqZjQBygd8SLGvcETgmSbGJZLSGnt306pTPWSP6cNaIPlRXO/PXbOO1TzZw54uLdyeXGqWVVdz0zHxOOLA7HfPb1DqXSDJEHQcz38yGAhcTzKCcR/CA/253X5PE+EQyWtRnN1lZXw7wvPX5hQnLbNxRwWE3vsCB+xQyckARIwcUMWJAEV3b5+4uo1tr0pQij4Nx9y/Yc00WzKyfmT3i7mc1eWQislfqenbTtX1bzjuqP+8u28jf31vB5DeXAbBftwJG7tuFbINHZ62kLJw1WrfW5Kv6qitadgLGNUEcItJE6np2c803h4SJ4gAqdlUzd9VW3l26iXeXbuTpD1ezvbz2eOnSyipuna5u0bJ3UrZksoikRkPPbgDa5mQxvF9nhvfrzMWj9qOq2tn/qn/iCepbvaWMs/74FsP6dWJYn84c3q8T3Tvk7VGm5taalpKWWEowIq1Q1Gc3NbKzrN5u0RVV1Ux6fSmVVZ8BUNwpn2F9O3F4385sK6vkj68soaxSt9ZkT0owIgI03C26rLKKeau3MXvFZmZ/voXZK7bwzJzEfXx0a02ggQRjZk81cHxhE8YiImnU0K21vDbZu2+r1Vi7rYyRv048FG71ljLG3fMmQ8PebUOLO7JftwJysr9cSFe91lq3hlowGyPsX9pEsYhImjX21lqPwjyK67m1lm3GI7M+391jLa9NFkN6FjK0uCMVVdVM/WAV5eq11mrVm2DcXYuJiUi9Grq1VlXtLN1QwtxVW5m7chsfr9rKY++vZEdF7XVySiuruPnZhgeEquXTMugZjIh8JQ0tJZ2dZezfvQP7d+/At4YFx9TXa21DSQWH3vACxZ3yGdyzA4N7FnLgPoUM7tmBfl0KePqj1XskNLV8mi8lGBH5yhpaSjpefb3WuhS05cLjBrBwzXYWrNnGjEXrd099k98mm13V1bvXyKlRWlnFbdMXKcE0M0owIpIWdd1au/Y/huyRKMoqq/h0XQnz12xj4ZrtTHoj8WPfVVtKuXbaxwzs0Z79u3dgYI/2dImZBgd0ay3VlGBEJC2iDAiFoPdazTxrANPnfZGw5dMm25g2e9UeMxJ0KWjL/t3bM7BHB0ordvHUnDVUqFNByijBiEjaNLbXGtTd8rnljKGcflgv1m4rZ/Ha7Sxeu51P15WweO32WomnRmllFdc9+TH5bbPZr1t7+ha1o21OVq1yavnsHSUYEWlRGmr57NMxj3065nH8wG67j3F39r0ycaeCbWW7+OED7wPBs6G+Re3Yt2sB+3Vvz75dC1i1pZQ/v/aZZirYC0owItLiNLblY1Z3p4KeHfP447nDWbK+hM/W7+CzDSUsWbeD1z7dsPt2WrzSyipueHoefbu0o3+XAjq3a1Nr2WrNz6YEIyIZoq5ba1ecciCH9unEoX067VG+qtpZvaWU426dkbC+zTsrOeMPbwLQIS+H/l0K6N+1gP5d2rGhpJzHP1iV8c97lGBEJCNE7VRQIzvL6FPUrs6ZCrp3yOWWM4aybONOlm/cwbKNO5mzcgv/nLum1oqiELR6rp42l62llfQpyqdvUTt6d25HXpvsPcq1puc9SjAikjGaslPBVd8YzOjBPWqVr6yqZuDVzyV83rOjvIrrn5q3x7buHXLpU9SOvkXt2FleycuL1u8e59OYlk9zTExKMCIi9Whsy6dNdladz3uKO+Ux7ZJjWbFpJys372TFxp18vnknKzbt5N2lmxIeU1pZxeWPzeHVT9bTu1M+xZ3zKe7UjuLO+fTqlEduTjbTZq9qlrMbpDTBmFkRcB/wb8AG4Ep3f6iOspcCVwD5wOPAxe5e3lA9ZnYUcBMwHKgCZgI/dffE84qLiDSgsS2fulo9l405kG4dcunWIXePWalrDJjwbMKWT0VVNW8t2cjabWXE333r3iGXzTsrEs5u8D/PL+T0w3rV6oAQK5ktn1S3YO4GKoAewGHAs2b2kbvv0WY0szHABOBEYDXwBHBDuK2hejoD9wLTgV3AXcBfgVOS+cVERGo0ND9bXepu+eTzxoQTqayq5outZazcXMqqLaWs3LyTVZtLefT9lQnrW7O1jMHXPU+vjvn07JRHz4759OqYR89O+fTsmMeCNdv4/Uuf7FUX7JrE1Haf/YfXVSZlCcbMCoBxwMHuXgK8Hq43cx5fJo4a5wP31SQeM7sJmAJMaKged38u7rx3Aa8k8auJiNTS2PnZoL6WzyAguP3Wp6gdfYra7XHcm0s2JkxMHfNzOHN4H9ZsLWX1ljJe/2QD67bXbgXFKq2s4tppH7O9fBf7FOaxT2EePTrm0rUgl6ysoCUUf0uuLuZez5makJkNA9509/yYbf8NfN3dT40r+xHwa3f/R/i5K7Ae6Ar0jVpPuO/nwHfc/agE+y4CLgLo1q3b8EceeeQrf8/WpqSkhPbt26c7jGZH1yUxXZfaGntN3lxdyeOLK9lY5nTJM8YNbMPRvepeuqDmmMkfV1ARM2ynbRaMP7htrWN3VTtby51NZc6v3imLHFe2Qcdco3Ou8fn26t3nWvO3n1O+5pOE9+BSeYusPbA1bttWoEOEsjXvOzSmHjM7BLgOOD1RQO5+L8HtNAYNGuRR/8rIJI356yuT6LokputSW2OvySjgqkaeYxQwZC+epUxe9HLClk+vTnlMvfgYvthWxhdby1i7rYwvtpWxdmvw75KtDa1FGUhlgimh9hLLhcD2CGVr3m+PWo+Z7Q88B/zM3V/by5hFRFqEpuyCffmYA3dPuUOf2scd85vEiSle7VndkmcxkGNmB8RsOxSYl6DsvHBfbLm17r4xSj1m1g94EbjJ3R9oovhFRFqVscOKueWMoRR3yscIOhPccsbQBhPVZWMGkR83QDSRlLVg3H2HmU0FbjSz7xP0/jodODpB8fuByWY2BVgDXANMjlKPmRUDLwN3u/sfk/mdRERaur1p+cT2kqtv/EcqWzAAPyYY17IOeJhgbMs8M+trZiVm1hfA3Z8HbgVmAMvD1/UN1RPu+z6wL3B9WGeJmZWk4LuJiGSMscOKeWPCiVR88en7dZVJ6TgYd98EjE2wfQXBw/vYbXcAdzSmnnDfDQRjZkREJI1S3YIREZEMoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJoQQjIiJJkdIEY2ZFZvaEme0ws+VmdnY9ZS81sy/MbKuZTTKz3Kj1mNloM1toZjvNbIaZ9Uvm9xIRkdpS3YK5G6gAegDnAPeY2UHxhcxsDDABGA30B/YFbohSj5l1BaYC1wJFwCzgH8n5OiIiUpeUJRgzKwDGAde6e4m7vw48BZyXoPj5wH3uPs/dNwM3AeMj1nMGMM/dH3X3MmAicKiZHZi8byciIvFyUniugUCVuy+O2fYR8PUEZQ8Cnowr18PMugB9G6jnoPAzAO6+w8yWhNsXxp7EzC4CLgo/lpvZx43+Vq1fV2BDuoNohnRdEtN1qa21X5M6H0GkMsG0B7bGbdsKdIhQtuZ9hwj1tAfWRzmPu98L3AtgZrPc/Yj6v0Lm0XVJTNclMV2X2jL5mqTyGUwJUBi3rRDYHqFszfvtEeppzHlERCRJUplgFgM5ZnZAzLZDgXkJys4L98WWW+vuGyPUs8ex4TOb/eo4j4iIJEnKEoy77yDo3XWjmRWY2THA6cADCYrfD1xoZkPMrDNwDTA5Yj1PAAeb2TgzywOuA+a4+8L4k8S596t9w1ZL1yUxXZfEdF1qy9hrYu6eupOZFQGTgJOBjcAEd3/IzPoC84Eh7r4iLPsL4AogH3gc+JG7l9dXT8x5TgLuInj49A4w3t2XpeRLiogIkOIEIyIimUNTxYiISFIowYiISFJkfIJpzPxomcTMZppZmZmVhK9F6Y4pHczsJ2Y2y8zKzWxy3L6MnPOurmtiZv3NzGN+ZkrM7No0hppSZpZrZveFv0e2m9lsM/v3mP0Z9/OS8QmGiPOjZaifuHv78DUo3cGkyWrgZoJOJbtl+Jx3Ca9JjE4xPzc3pTCudMsBPieYVaQjwc/GI2Hizcifl1SO5G92YuY1O9jdS4DXzaxmXrMJaQ1OmgV3nwpgZkcAvWN27Z7zLtw/EdhgZgdG6BLfotVzTTJaOIRiYsymZ8xsKTAc6EIG/rxkegumrvnR1IIJ3GJmG8zsDTMble5gmplac94BNXPeZbrlZrbSzP4a/uWekcysB8HvmHlk6M9LpieYxsyPlmmuIFgmoZhgoNjTZrZfekNqVvSzU9sGYATB+LPhBNdiSlojShMza0Pw3f8WtlAy8ucl0xOM5i2rg7u/4+7b3b3c3f8GvAF8I91xNSP62YkTLp8xy913ufta4CfAv5lZ/HVq1cwsi2BmkQqCawAZ+vOS6QmmMfOjZToHLN1BNCOa865hNaO4M+bnxswMuI+g09A4d68Md2Xkz0tGJ5hGzo+WMcysk5mNMbM8M8sxs3OA44Hp6Y4t1cLvnwdkA9k114S9n/OuxavrmpjZSDMbZGZZ4dpN/wvMdPf4W0Ot2T3AYOBUdy+N2Z6ZPy/untEvgi6D04AdwArg7HTHlO4X0A14j6D5vgV4Gzg53XGl6VpMJPhLPPY1Mdx3EsEidqXATKB/uuNN5zUB/hNYGv6/tIZg0tp90h1vCq9Lv/BalBHcEqt5nZOpPy+ai0xERJIio2+RiYhI8ijBiIhIUijBiIhIUijBiIhIUijBiIhIUijBiIhIUijBiLRS4dosZ6Y7DslcSjAiSWBmk8Nf8PGvt9Mdm0iqZPR6MCJJ9iLB2kKxKtIRiEg6qAUjkjzl7v5F3GsT7L599RMzezZcQne5mZ0be7CZDTWzF82s1Mw2ha2ijnFlzjezueHyxWvjl3UGiszs0XBJ8M/izyGSTEowIulzA/AUcBjBmjv3h6tEYmbtgOcJ5rI6EvgWcDQxyxSb2Q+BPwF/BQ4hWE4hfnbe64AnCWby/QcwKRPWgpfmQXORiSRB2JI4l2Diw1h3u/sVZubAX9z9BzHHvAh84e7nmtkPgNuB3u6+Pdw/CpgBHODun5rZSuBBd0+4vHd4jt+4+5Xh5xxgG3CRuz/YdN9WJDE9gxFJnleBi+K2bYl5/1bcvreAb4bvBxNM5x67INWbQDUwxMy2Eaw2+lIDMcypeePuu8xsPdA9UvQiX5ESjEjy7HT3T/fyWOPLBbviNWbxt8q4z45ujUuK6AdNJH2OSvB5Qfh+PnComcWu2X40wf+zCzxYkngVMDrpUYrsJbVgRJIn18z2idtW5e7rw/dnmNl7BItPnUmQLEaG+6YQdAK438yuAzoTPNCfGtMq+hXwOzNbCzwLtANGu/tvk/WFRBpDCUYkeU4iWNkx1iqgd/h+IjCOYGnh9cD33P09AHffaWZjgDuBdwk6CzwJ/KymIne/x8wqgF8C/wNsAv6ZpO8i0mjqRSaSBmEPr2+7+2PpjkUkWfQMRkREkkIJRkREkkK3yEREJCnUghERkaRQghERkaRQghERkaRQghERkaRQghERkaT4/zHSvutP524QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "learning_rate = 0.01\n",
    "decay = 1e-4\n",
    "batch_size=32\n",
    "n_steps_per_epoch = math.ceil(len(X_train) / batch_size)\n",
    "epochs = np.arange(n_epochs)\n",
    "lrs = learning_rate / (1 + decay * epochs * n_steps_per_epoch)\n",
    "\n",
    "plt.plot(epochs, lrs,  \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.01])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Power Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-roommate",
   "metadata": {},
   "source": [
    "### Exponential Scheduling\n",
    "#### lr = lr0 * 0.1**(epoch / s)\n",
    "- The learning rate will gradually drop by a factor of 10 every s steps. \n",
    "- While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every s steps.\n",
    "- The LearningRateScheduler will update the optimizer’s learning_rate attribute at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "nutritional-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1**(epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "affected-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1**(epoch / s)\n",
    "    return exponential_decay_fn\n",
    "\n",
    "exponential_decay_fn = exponential_decay(lr0=0.01, s=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "laughing-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "million-street",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 1.1151 - accuracy: 0.7360 - val_loss: 0.8242 - val_accuracy: 0.7140\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.6714 - accuracy: 0.7964 - val_loss: 0.5488 - val_accuracy: 0.8464\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5694 - accuracy: 0.8229 - val_loss: 0.7165 - val_accuracy: 0.8026\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5708 - accuracy: 0.8286 - val_loss: 0.5962 - val_accuracy: 0.8322\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4951 - accuracy: 0.8474 - val_loss: 0.5765 - val_accuracy: 0.8458\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4518 - accuracy: 0.8599 - val_loss: 0.4802 - val_accuracy: 0.8562\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4218 - accuracy: 0.8689 - val_loss: 0.5218 - val_accuracy: 0.8524\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3726 - accuracy: 0.8781 - val_loss: 0.5421 - val_accuracy: 0.8454\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3475 - accuracy: 0.8831 - val_loss: 0.4824 - val_accuracy: 0.8572\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3159 - accuracy: 0.8930 - val_loss: 0.5088 - val_accuracy: 0.8754\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2856 - accuracy: 0.9013 - val_loss: 0.4439 - val_accuracy: 0.8758\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2750 - accuracy: 0.9056 - val_loss: 0.4433 - val_accuracy: 0.8736\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2552 - accuracy: 0.9116 - val_loss: 0.5177 - val_accuracy: 0.8684\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2376 - accuracy: 0.9179 - val_loss: 0.4894 - val_accuracy: 0.8720\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2197 - accuracy: 0.9242 - val_loss: 0.4649 - val_accuracy: 0.8790\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2051 - accuracy: 0.9276 - val_loss: 0.4652 - val_accuracy: 0.8884\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1862 - accuracy: 0.9360 - val_loss: 0.5010 - val_accuracy: 0.8822\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1743 - accuracy: 0.9394 - val_loss: 0.4983 - val_accuracy: 0.8834\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.1643 - accuracy: 0.9435 - val_loss: 0.5013 - val_accuracy: 0.8904\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1566 - accuracy: 0.9456 - val_loss: 0.5057 - val_accuracy: 0.8890\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1459 - accuracy: 0.9499 - val_loss: 0.5309 - val_accuracy: 0.8896\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.1367 - accuracy: 0.9518 - val_loss: 0.5364 - val_accuracy: 0.8902\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.1278 - accuracy: 0.9579 - val_loss: 0.5847 - val_accuracy: 0.8892\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1167 - accuracy: 0.9610 - val_loss: 0.5957 - val_accuracy: 0.8868\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1167 - accuracy: 0.9616 - val_loss: 0.5907 - val_accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                   validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "authorized-acrobat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-australian",
   "metadata": {},
   "source": [
    "The schedule function can take the current learning rate as a second argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "legal-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1**(1 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "equipped-haven",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 1.1227 - accuracy: 0.7344 - val_loss: 0.6928 - val_accuracy: 0.8000\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.6301 - accuracy: 0.8025 - val_loss: 0.5176 - val_accuracy: 0.8366\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5612 - accuracy: 0.8251 - val_loss: 0.7833 - val_accuracy: 0.8072\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.5196 - accuracy: 0.8369 - val_loss: 0.5696 - val_accuracy: 0.8378\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.4792 - accuracy: 0.8474 - val_loss: 0.4912 - val_accuracy: 0.8696\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4182 - accuracy: 0.8662 - val_loss: 0.5097 - val_accuracy: 0.8678\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.3924 - accuracy: 0.8740 - val_loss: 0.5099 - val_accuracy: 0.8668\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.3542 - accuracy: 0.8842 - val_loss: 0.4627 - val_accuracy: 0.8608\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3392 - accuracy: 0.8841 - val_loss: 0.4508 - val_accuracy: 0.8666\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 16s 10ms/step - loss: 0.3068 - accuracy: 0.8958 - val_loss: 0.3992 - val_accuracy: 0.8926\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2743 - accuracy: 0.9078 - val_loss: 0.4640 - val_accuracy: 0.8728\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2773 - accuracy: 0.9072 - val_loss: 0.4339 - val_accuracy: 0.8760\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.2451 - accuracy: 0.9140 - val_loss: 0.4338 - val_accuracy: 0.8850\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 18s 11ms/step - loss: 0.2244 - accuracy: 0.9224 - val_loss: 0.4988 - val_accuracy: 0.8742\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2155 - accuracy: 0.9263 - val_loss: 0.4375 - val_accuracy: 0.8880\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.1953 - accuracy: 0.9326 - val_loss: 0.4477 - val_accuracy: 0.8898\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1764 - accuracy: 0.9392 - val_loss: 0.4777 - val_accuracy: 0.8820\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.1684 - accuracy: 0.9435 - val_loss: 0.4907 - val_accuracy: 0.8872\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 25s 14ms/step - loss: 0.1618 - accuracy: 0.9453 - val_loss: 0.5071 - val_accuracy: 0.8920\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.1477 - accuracy: 0.9502 - val_loss: 0.4892 - val_accuracy: 0.8904\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 22s 13ms/step - loss: 0.1421 - accuracy: 0.9517 - val_loss: 0.5786 - val_accuracy: 0.8896\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.1329 - accuracy: 0.9547 - val_loss: 0.5832 - val_accuracy: 0.8912\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 27s 16ms/step - loss: 0.1207 - accuracy: 0.9606 - val_loss: 0.5889 - val_accuracy: 0.8910\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.1136 - accuracy: 0.9628 - val_loss: 0.6286 - val_accuracy: 0.8862\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 28s 16ms/step - loss: 0.1116 - accuracy: 0.9631 - val_loss: 0.6322 - val_accuracy: 0.8908\n"
     ]
    }
   ],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialDecay(keras.callbacks.Callback):\n",
    "    def __init__(self, s=40000):\n",
    "        super().__init__()\n",
    "        self.s = s\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # Note: the `batch` argument is reset at each epoch\n",
    "        lr = K.get_value(self.model.optimizer.lr)\n",
    "        K.set_value(self.model.optimizer.lr, lr * 0.1**(1 / s))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs['lr'] = K.get_value(self.model.optimizer.lr)\n",
    "        \n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "lr0 = 0.01\n",
    "optimizer = keras.optimizers.Nadam(lr=lr0)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "exp_decay = ExponentialDecay(s)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[exp_decay])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "phantom-affiliation",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = n_epochs * len(X_train) // 32\n",
    "steps = np.arange(n_steps)\n",
    "lrs = lr0 * 0.1**(steps / s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "silent-fraction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(steps, lrs, \"-\", linewidth=2)\n",
    "plt.axis([0, n_steps - 1, 0, lr0 * 1.1])\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Exponential Scheduling (per batch)\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-pioneer",
   "metadata": {},
   "source": [
    "### Piecewise Constant Scheduling\n",
    "- Use a constant learning rate for a number of epochs (e.g., lr0=0.1 for 5 epochs), then a smaller learning rate for another number of epochs (e.g., lr1=0.001 for 50 epochs), and so on. \n",
    "    - Although this solution can work very well, it requires fiddling around to figure out the right sequence of learning rates and how long to use each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "narrow-negative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "indirect-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant(boundaries, values):\n",
    "    boundaries = np.array([0] + boundaries)\n",
    "    values = np.array(values)\n",
    "    def piecewise_constant_fn(epoch):\n",
    "        return values[np.argmax(boundaries > epoch) - 1]\n",
    "    return piecewise_constant_fn\n",
    "\n",
    "piecewise_constant_fn = piecewise_constant([5, 15], [0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "greenhouse-netscape",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 26s 14ms/step - loss: 1.1574 - accuracy: 0.7315 - val_loss: 0.7580 - val_accuracy: 0.7702\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.7431 - accuracy: 0.7791 - val_loss: 0.7069 - val_accuracy: 0.7744\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.8114 - accuracy: 0.7666 - val_loss: 0.8354 - val_accuracy: 0.7200\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 22s 13ms/step - loss: 0.7857 - accuracy: 0.7773 - val_loss: 0.7282 - val_accuracy: 0.77842s - loss: 0.7796 - accura - ETA: 2s - loss: 0.7\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 23s 14ms/step - loss: 0.8087 - accuracy: 0.7805 - val_loss: 0.9467 - val_accuracy: 0.6994 - loss: 0.8086 - \n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.6022 - accuracy: 0.8109 - val_loss: 0.6328 - val_accuracy: 0.8520\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 22s 13ms/step - loss: 0.4864 - accuracy: 0.8533 - val_loss: 0.5552 - val_accuracy: 0.8446\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.4413 - accuracy: 0.8590 - val_loss: 0.5348 - val_accuracy: 0.8398\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.4321 - accuracy: 0.8605 - val_loss: 0.5855 - val_accuracy: 0.8174 17s - loss: 0.41\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.4475 - accuracy: 0.8677 - val_loss: 0.5219 - val_accuracy: 0.8620\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.3991 - accuracy: 0.8717 - val_loss: 0.5883 - val_accuracy: 0.8612\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 26s 15ms/step - loss: 0.4163 - accuracy: 0.8794 - val_loss: 0.6186 - val_accuracy: 0.8560\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.3818 - accuracy: 0.8799 - val_loss: 0.5830 - val_accuracy: 0.8536\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 21s 12ms/step - loss: 0.3929 - accuracy: 0.8805 - val_loss: 0.6628 - val_accuracy: 0.8634\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.4131 - accuracy: 0.8785 - val_loss: 0.7904 - val_accuracy: 0.8408\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.3478 - accuracy: 0.9050 - val_loss: 0.4505 - val_accuracy: 0.8786\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.2370 - accuracy: 0.9205 - val_loss: 0.4657 - val_accuracy: 0.87660s - loss: 0.2366  - ETA: 0s - los\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.2302 - accuracy: 0.9234 - val_loss: 0.4486 - val_accuracy: 0.8822\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.2156 - accuracy: 0.9274 - val_loss: 0.4597 - val_accuracy: 0.8872\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.2070 - accuracy: 0.9288 - val_loss: 0.4600 - val_accuracy: 0.8794\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 24s 14ms/step - loss: 0.2001 - accuracy: 0.9300 - val_loss: 0.4807 - val_accuracy: 0.8826\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.1928 - accuracy: 0.9331 - val_loss: 0.5191 - val_accuracy: 0.8870\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.1804 - accuracy: 0.9377 - val_loss: 0.4878 - val_accuracy: 0.8852\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 25s 14ms/step - loss: 0.1722 - accuracy: 0.9398 - val_loss: 0.5306 - val_accuracy: 0.8844\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 25s 15ms/step - loss: 0.1707 - accuracy: 0.9385 - val_loss: 0.5489 - val_accuracy: 0.8846\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(piecewise_constant_fn)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "lesbian-variable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, [piecewise_constant_fn(epoch) for epoch in history.epoch], \"o-\")\n",
    "plt.axis([0, n_epochs - 1, 0, 0.011])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.title(\"Piecewise Constant Scheduling\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-clerk",
   "metadata": {},
   "source": [
    "### Performance Scheduling\n",
    "- Measure the validation error every N steps (just like for early stopping), and reduce the learning rate by a factor of λ when the error stops dropping.\n",
    "- For performance scheduling, use the ReduceLROnPlateau callback. \n",
    "    - For example, if you pass the following callback to the fit() method, it will multiply the learning rate by 0.5 whenever the best validation loss does not improve for five consecutive epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "sophisticated-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "minus-newman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 17s 9ms/step - loss: 0.7110 - accuracy: 0.7761 - val_loss: 0.4654 - val_accuracy: 0.8532\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4818 - accuracy: 0.8410 - val_loss: 0.6064 - val_accuracy: 0.8254\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.5044 - accuracy: 0.8424 - val_loss: 0.5094 - val_accuracy: 0.8472\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.4849 - accuracy: 0.8501 - val_loss: 0.4738 - val_accuracy: 0.8618\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.5296 - accuracy: 0.8443 - val_loss: 0.5727 - val_accuracy: 0.8300\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.4907 - accuracy: 0.8572 - val_loss: 0.6577 - val_accuracy: 0.8470\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.3442 - accuracy: 0.8861 - val_loss: 0.4124 - val_accuracy: 0.8770\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 18s 11ms/step - loss: 0.2558 - accuracy: 0.9060 - val_loss: 0.4019 - val_accuracy: 0.8738\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2371 - accuracy: 0.9109 - val_loss: 0.3878 - val_accuracy: 0.8902\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 18s 10ms/step - loss: 0.2161 - accuracy: 0.9193 - val_loss: 0.4159 - val_accuracy: 0.8880\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.1994 - accuracy: 0.9258 - val_loss: 0.4363 - val_accuracy: 0.8822\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.1909 - accuracy: 0.9291 - val_loss: 0.4816 - val_accuracy: 0.8798\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.1807 - accuracy: 0.9312 - val_loss: 0.4396 - val_accuracy: 0.8866\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1663 - accuracy: 0.9368 - val_loss: 0.4738 - val_accuracy: 0.8796\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.1321 - accuracy: 0.9485 - val_loss: 0.4401 - val_accuracy: 0.8858\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.1047 - accuracy: 0.9590 - val_loss: 0.4568 - val_accuracy: 0.8936\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.0925 - accuracy: 0.9641 - val_loss: 0.4734 - val_accuracy: 0.8914\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.0860 - accuracy: 0.9671 - val_loss: 0.4952 - val_accuracy: 0.8888\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.0824 - accuracy: 0.9684 - val_loss: 0.5098 - val_accuracy: 0.8900\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.0691 - accuracy: 0.9739 - val_loss: 0.4988 - val_accuracy: 0.8914\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 17s 10ms/step - loss: 0.0587 - accuracy: 0.9787 - val_loss: 0.5127 - val_accuracy: 0.8936\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.0550 - accuracy: 0.9805 - val_loss: 0.5338 - val_accuracy: 0.8950\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.0515 - accuracy: 0.9817 - val_loss: 0.5384 - val_accuracy: 0.8956\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.0478 - accuracy: 0.9831 - val_loss: 0.5633 - val_accuracy: 0.8940\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.0429 - accuracy: 0.9863 - val_loss: 0.5708 - val_accuracy: 0.8918\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.02, momentum=0.9)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                   validation_data=(X_valid_scaled, y_valid),\n",
    "                   callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "domestic-shooting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.epoch, history.history[\"lr\"], \"bo-\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning Rate\", color='b')\n",
    "plt.tick_params('y', colors='b')\n",
    "plt.gca().set_xlim(0, n_epochs - 1)\n",
    "plt.grid(True)\n",
    "\n",
    "ax2 = plt.gca().twinx()\n",
    "ax2.plot(history.epoch, history.history[\"val_loss\"], \"r^-\")\n",
    "ax2.set_ylabel('Validation Loss', color='r')\n",
    "ax2.tick_params('y', colors='r')\n",
    "\n",
    "plt.title(\"Reduce LR on Plateau\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-convergence",
   "metadata": {},
   "source": [
    "### tf.keras schedulers\n",
    "- Lastly, tf.keras offers an alternative way to implement learning rate scheduling: define the learning rate using one of the schedules available in keras.optimizers.schedules, then pass this learning rate to any optimizer. \n",
    "    - This approach updates the learning rate at each step rather than at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "physical-kinase",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.5995 - accuracy: 0.7923 - val_loss: 0.4095 - val_accuracy: 0.8598\n",
      "Epoch 2/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3890 - accuracy: 0.8615 - val_loss: 0.3739 - val_accuracy: 0.8696\n",
      "Epoch 3/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.3530 - accuracy: 0.8774 - val_loss: 0.3732 - val_accuracy: 0.8688\n",
      "Epoch 4/25\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.3296 - accuracy: 0.8819 - val_loss: 0.3494 - val_accuracy: 0.8804\n",
      "Epoch 5/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3175 - accuracy: 0.8866 - val_loss: 0.3432 - val_accuracy: 0.8796\n",
      "Epoch 6/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2927 - accuracy: 0.8952 - val_loss: 0.3416 - val_accuracy: 0.8814\n",
      "Epoch 7/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2851 - accuracy: 0.8990 - val_loss: 0.3355 - val_accuracy: 0.8816\n",
      "Epoch 8/25\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.2713 - accuracy: 0.9039 - val_loss: 0.3366 - val_accuracy: 0.8816\n",
      "Epoch 9/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2712 - accuracy: 0.9045 - val_loss: 0.3265 - val_accuracy: 0.8858\n",
      "Epoch 10/25\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.2567 - accuracy: 0.9083 - val_loss: 0.3239 - val_accuracy: 0.8858\n",
      "Epoch 11/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2501 - accuracy: 0.9116 - val_loss: 0.3252 - val_accuracy: 0.8868\n",
      "Epoch 12/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2451 - accuracy: 0.9150 - val_loss: 0.3301 - val_accuracy: 0.8818\n",
      "Epoch 13/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2408 - accuracy: 0.9154 - val_loss: 0.3219 - val_accuracy: 0.8868\n",
      "Epoch 14/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2377 - accuracy: 0.9160 - val_loss: 0.3223 - val_accuracy: 0.8854\n",
      "Epoch 15/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2375 - accuracy: 0.9167 - val_loss: 0.3208 - val_accuracy: 0.8882\n",
      "Epoch 16/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2315 - accuracy: 0.9194 - val_loss: 0.3186 - val_accuracy: 0.8888\n",
      "Epoch 17/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2264 - accuracy: 0.9214 - val_loss: 0.3200 - val_accuracy: 0.8898\n",
      "Epoch 18/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2283 - accuracy: 0.9190 - val_loss: 0.3172 - val_accuracy: 0.8894\n",
      "Epoch 19/25\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.2283 - accuracy: 0.9205 - val_loss: 0.3201 - val_accuracy: 0.8894\n",
      "Epoch 20/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2287 - accuracy: 0.9216 - val_loss: 0.3173 - val_accuracy: 0.8902\n",
      "Epoch 21/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2264 - accuracy: 0.9210 - val_loss: 0.3183 - val_accuracy: 0.8908\n",
      "Epoch 22/25\n",
      "1719/1719 [==============================] - 13s 7ms/step - loss: 0.2256 - accuracy: 0.9204 - val_loss: 0.3167 - val_accuracy: 0.8912\n",
      "Epoch 23/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2222 - accuracy: 0.9231 - val_loss: 0.3175 - val_accuracy: 0.8900\n",
      "Epoch 24/25\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.2180 - accuracy: 0.9241 - val_loss: 0.3170 - val_accuracy: 0.8898\n",
      "Epoch 25/25\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.2222 - accuracy: 0.9226 - val_loss: 0.3169 - val_accuracy: 0.8910\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "s = 20 * len(X_train) // 32 # number of steps in 20 epochs (batch size = 32)\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 25\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-society",
   "metadata": {},
   "source": [
    "For piecewise constant scheduling, try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "muslim-spotlight",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    boundaries=[5. * n_steps_per_epoch, 15. * n_steps_per_epoch],\n",
    "    values=[0.01, 0.005, 0.001])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggregate-ladder",
   "metadata": {},
   "source": [
    "### 1Cycle scheduling\n",
    "- Contrary to the other approaches, 1cycle (introduced in a 2018 paper by Leslie Smith) starts by increasing the initial learning rate lr0 , growing linearly up to lr1 halfway through training.\n",
    "    - Then it decreases the learning rate linearly down to lr0 again during the second half of training, finishing the last few epochs by dropping the rate down by several orders of magnitude (still linearly).\n",
    "    - The maximum learning rate lr1 is chosen using the same approach we used to find the optimal learning rate, and the initial learning rate lr0 is chosen to be roughly 10 times lower.\n",
    "- When using a momentum, we start with a high momentum first (e.g., 0.95), then drop it down to a lower momentum during the first half of training (e.g., down to 0.85, linearly), and then bring it back up to the maximum value (e.g., 0.95) during the second half of training, finishing the last few epochs with that maximum value.\n",
    "- Smith did many experiments showing that this approach was often able to speed up training considerably and reach better performance. \n",
    "    - For example, on the popular CIFAR10 image dataset, this approach reached 91.9% validation accuracy in just 100 epochs, instead of 90.3% accuracy in 800 epochs through a standard approach (with the same neural network architecture).\n",
    "\n",
    "- A 2013 paper by Andrew Senior et al. compared the performance of some of the most popular learning schedules when using momentum optimization to train deep neural networks for speech recognition. \n",
    "    - The authors concluded that, in this setting, both performance scheduling and exponential scheduling performed well.\n",
    "    - **They favored exponential scheduling because it was easy to tune and it converged slightly faster to the optimal solution** (they also mentioned that it was easier to implement than performance scheduling, but in Keras both options are easy). \n",
    "    - That said, the **1cycle approach seems to perform even better.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "geological-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        K.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.lr)\n",
    "    K.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "political-stream",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.SGD(lr=1e-3),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "super-revolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "430/430 [==============================] - 6s 13ms/step - loss: nan - accuracy: 0.3122\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "willing-looking",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "graphic-theory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "430/430 [==============================] - 6s 14ms/step - loss: 0.6572 - accuracy: 0.7740 - val_loss: 0.4872 - val_accuracy: 0.8336\n",
      "Epoch 2/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.4581 - accuracy: 0.8396 - val_loss: 0.4275 - val_accuracy: 0.8524\n",
      "Epoch 3/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.4122 - accuracy: 0.8546 - val_loss: 0.4114 - val_accuracy: 0.8578\n",
      "Epoch 4/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.3837 - accuracy: 0.8642 - val_loss: 0.3868 - val_accuracy: 0.8686\n",
      "Epoch 5/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.3639 - accuracy: 0.8718 - val_loss: 0.3767 - val_accuracy: 0.8684\n",
      "Epoch 6/25\n",
      "430/430 [==============================] - 6s 14ms/step - loss: 0.3456 - accuracy: 0.8774 - val_loss: 0.3742 - val_accuracy: 0.8708\n",
      "Epoch 7/25\n",
      "430/430 [==============================] - 6s 14ms/step - loss: 0.3330 - accuracy: 0.8812 - val_loss: 0.3635 - val_accuracy: 0.8714\n",
      "Epoch 8/25\n",
      "430/430 [==============================] - 5s 12ms/step - loss: 0.3184 - accuracy: 0.8863 - val_loss: 0.3950 - val_accuracy: 0.8614\n",
      "Epoch 9/25\n",
      "430/430 [==============================] - 6s 14ms/step - loss: 0.3065 - accuracy: 0.8890 - val_loss: 0.3490 - val_accuracy: 0.8758\n",
      "Epoch 10/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.2944 - accuracy: 0.8925 - val_loss: 0.3393 - val_accuracy: 0.8796\n",
      "Epoch 11/25\n",
      "430/430 [==============================] - 6s 14ms/step - loss: 0.2838 - accuracy: 0.8963 - val_loss: 0.3459 - val_accuracy: 0.8820\n",
      "Epoch 12/25\n",
      "430/430 [==============================] - 7s 16ms/step - loss: 0.2709 - accuracy: 0.9026 - val_loss: 0.3660 - val_accuracy: 0.8684\n",
      "Epoch 13/25\n",
      "430/430 [==============================] - 6s 14ms/step - loss: 0.2538 - accuracy: 0.9080 - val_loss: 0.3355 - val_accuracy: 0.8832\n",
      "Epoch 14/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.2405 - accuracy: 0.9138 - val_loss: 0.3464 - val_accuracy: 0.8798\n",
      "Epoch 15/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.2281 - accuracy: 0.9185 - val_loss: 0.3259 - val_accuracy: 0.8840\n",
      "Epoch 16/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.2160 - accuracy: 0.9236 - val_loss: 0.3293 - val_accuracy: 0.8834\n",
      "Epoch 17/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.2063 - accuracy: 0.9260 - val_loss: 0.3349 - val_accuracy: 0.8876\n",
      "Epoch 18/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.1979 - accuracy: 0.9302 - val_loss: 0.3240 - val_accuracy: 0.8900\n",
      "Epoch 19/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.1892 - accuracy: 0.9339 - val_loss: 0.3236 - val_accuracy: 0.8900\n",
      "Epoch 20/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.1822 - accuracy: 0.9369 - val_loss: 0.3227 - val_accuracy: 0.8930\n",
      "Epoch 21/25\n",
      "430/430 [==============================] - 5s 12ms/step - loss: 0.1752 - accuracy: 0.9399 - val_loss: 0.3223 - val_accuracy: 0.8906\n",
      "Epoch 22/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.1700 - accuracy: 0.9419 - val_loss: 0.3186 - val_accuracy: 0.8938\n",
      "Epoch 23/25\n",
      "430/430 [==============================] - 6s 13ms/step - loss: 0.1654 - accuracy: 0.9440 - val_loss: 0.3190 - val_accuracy: 0.8944\n",
      "Epoch 24/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.1626 - accuracy: 0.9455 - val_loss: 0.3181 - val_accuracy: 0.8926\n",
      "Epoch 25/25\n",
      "430/430 [==============================] - 5s 13ms/step - loss: 0.1609 - accuracy: 0.9463 - val_loss: 0.3174 - val_accuracy: 0.8942\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-nation",
   "metadata": {},
   "source": [
    "## Avoiding Overfitting Through Regularization\n",
    "- Deep neural networks typically have tens of thousands of parameters, sometimes even millions. \n",
    "    - This gives them an incredible amount of freedom and means they can fit a huge variety of complex datasets. \n",
    "    - But this great flexibility also makes the network prone to overfitting the training set. We need regularization\n",
    "\n",
    "## $\\ell_1$ and $\\ell_2$ regularization\n",
    "- You can use $\\ell_2$ regularization to constrain a neural network’s connection weights, and/or $\\ell_1$ regularization if you want a sparse model (with many weights equal to 0).\n",
    "- Here is how to apply $\\ell_2$ regularization to a Keras layer’s connection weights, using a regularization factor of 0.01:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "julian-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"elu\", \n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "# or l1(0.1) for ℓ1 regularization with a factor or 0.1\n",
    "# or l1_l2(0.1, 0.01) for both ℓ1 and ℓ2 regularization, with factors 0.1 and 0.01 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "assigned-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 22s 11ms/step - loss: 3.2189 - accuracy: 0.7967 - val_loss: 0.7169 - val_accuracy: 0.8340\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 20s 11ms/step - loss: 0.7280 - accuracy: 0.8247 - val_loss: 0.6850 - val_accuracy: 0.8376\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(100, activation=\"elu\",\n",
    "                       kernel_initializer=\"he_normal\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    keras.layers.Dense(10, activation=\"softmax\",\n",
    "                       kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-transmission",
   "metadata": {},
   "source": [
    "- The l2() function returns a regularizer that will be called at each step during training to compute the regularization loss. This is then added to the final loss.\n",
    "    - As you might expect, you can just use keras.regularizers.l1() if you want ℓ regularization; if you want both $\\ell_1$ and $\\ell_2$ regularization, use keras.regularizers.l1_l2() (specifying both regularization factors).\n",
    "- Since you will typically want to apply the same regularizer to all layers in your network, as well as using the same activation function and the same initialization strategy in all hidden layers, you may find yourself repeating the same arguments.\n",
    "    - This makes the code ugly and error-prone.\n",
    "    - To avoid this, you can try refactoring your code to use loops.\n",
    "    - Another option is to use Python’s **functools.partial()** function, which lets you create a thin wrapper for any callable, with some default argument values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "continued-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 21s 10ms/step - loss: 3.2911 - accuracy: 0.7924 - val_loss: 0.7218 - val_accuracy: 0.8310\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 16s 9ms/step - loss: 0.7282 - accuracy: 0.8245 - val_loss: 0.6826 - val_accuracy: 0.8382\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation=\"elu\",\n",
    "                           kernel_initializer=\"he_normal\",\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs=2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-soccer",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- Dropout is one of the most popular regularization techniques for deep neural networks. It was proposed in a paper by Geoffrey Hinton in 2012 and further detailed in a 2014 paper by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks get a 1–2% accuracy boost simply by adding dropout. \n",
    "    - This may not sound like a lot, but when a model already has 95% accuracy, getting a 2% accuracy boost means dropping the error rate by almost 40% (going from 5% error to roughly 3%).\n",
    "- It is a fairly simple algorithm: **at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step.**\n",
    "- The hyperparameter p is called the dropout rate, and it is **typically set between 10% and 50%:** \n",
    "    - **closer to 20–30% in recurrent neural nets (RNNs)**, \n",
    "    - and **closer to 40–50% in convolutional neural networks (CNNs).**\n",
    "- After training, neurons don’t get dropped anymore. And that’s all\n",
    "\n",
    "<img src=\"11-9.png\">\n",
    "\n",
    "### TIP\n",
    "In practice, you can usually apply dropout only to the neurons in the top one to three layers (excluding the output layer).\n",
    "***\n",
    "\n",
    "- There is one small but important technical detail. Suppose p = 50%, in which case during testing a neuron would be connected to twice as many input neurons as it would be (on average) during training. \n",
    "    - To compensate for this fact, we need to multiply each neuron’s input connection weights by 0.5 after training. \n",
    "    - If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on and will be unlikely to perform well. \n",
    "    - More generally, we need to multiply each input connection weight by the **keep probability** (1 – p) after training.\n",
    "    - Alternatively, we can divide each neuron’s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well).\n",
    "- To implement dropout using Keras, you can use the **keras.layers.Dropout** layer.\n",
    "    - During training, it randomly drops some inputs (setting them to 0) and divides the remaining inputs by the keep probability.\n",
    "    - After training, it does nothing at all; it just passes the inputs to the next layer.\n",
    "\n",
    "The following code applies dropout regularization before every Dense layer, using a dropout rate of 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "tender-orientation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 25s 13ms/step - loss: 0.7611 - accuracy: 0.7576 - val_loss: 0.3730 - val_accuracy: 0.8644\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 23s 13ms/step - loss: 0.4306 - accuracy: 0.8401 - val_loss: 0.3397 - val_accuracy: 0.8720\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standard-summit",
   "metadata": {},
   "source": [
    "### WARNING\n",
    "- Since dropout is only active during training, comparing the training loss and the validation loss can be misleading. \n",
    "- In particular, a model may be overfitting the training set and yet have similar training and validation losses. \n",
    "    - So make sure to evaluate the training loss without dropout (e.g., after training).\n",
    "***\n",
    "- If you observe that **the model is overfitting, you can increase the dropout rate**. \n",
    "    - Conversely, you should **try decreasing the dropout rate if the model underfits the training set.**\n",
    "- It can also help to increase the dropout rate for large layers, and reduce it for small ones. \n",
    "- Moreover, many state-of-the-art architectures only use dropout after the last hidden layer, so you may want to try this if full dropout is too strong.\n",
    "- Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-blackjack",
   "metadata": {},
   "source": [
    "### TIP\n",
    "- If you want to regularize a self-normalizing network based on the SELU activation function (as discussed earlier), you should use **alpha dropout**: this is a variant of dropout that preserves the mean and standard deviation of its inputs (it was introduced in the same paper as SELU, as regular dropout would break self-normalization).\n",
    "\n",
    "## Alpha Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "educated-copyright",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "mathematical-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 22s 12ms/step - loss: 0.8023 - accuracy: 0.7146 - val_loss: 0.5778 - val_accuracy: 0.8446\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.5662 - accuracy: 0.7904 - val_loss: 0.5146 - val_accuracy: 0.8528\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.5260 - accuracy: 0.8063 - val_loss: 0.4882 - val_accuracy: 0.8612\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 19s 11ms/step - loss: 0.5124 - accuracy: 0.8098 - val_loss: 0.4795 - val_accuracy: 0.8588\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 15s 9ms/step - loss: 0.5077 - accuracy: 0.8128 - val_loss: 0.4264 - val_accuracy: 0.8704\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 13s 8ms/step - loss: 0.4793 - accuracy: 0.8202 - val_loss: 0.4614 - val_accuracy: 0.8624\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4717 - accuracy: 0.8275 - val_loss: 0.4703 - val_accuracy: 0.8638\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4570 - accuracy: 0.8300 - val_loss: 0.4248 - val_accuracy: 0.8674\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.4626 - accuracy: 0.8290 - val_loss: 0.4256 - val_accuracy: 0.8746\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4555 - accuracy: 0.8329 - val_loss: 0.4286 - val_accuracy: 0.8650\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4465 - accuracy: 0.8331 - val_loss: 0.4317 - val_accuracy: 0.8692\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 11s 6ms/step - loss: 0.4426 - accuracy: 0.8357 - val_loss: 0.5125 - val_accuracy: 0.8574\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4337 - accuracy: 0.8387 - val_loss: 0.4276 - val_accuracy: 0.8742\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4296 - accuracy: 0.8390 - val_loss: 0.4413 - val_accuracy: 0.8634\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 10s 6ms/step - loss: 0.4317 - accuracy: 0.8371 - val_loss: 0.4321 - val_accuracy: 0.8704\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4263 - accuracy: 0.8387 - val_loss: 0.4101 - val_accuracy: 0.8780\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 11s 7ms/step - loss: 0.4209 - accuracy: 0.8427 - val_loss: 0.5307 - val_accuracy: 0.8580\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4362 - accuracy: 0.8380 - val_loss: 0.4795 - val_accuracy: 0.8698\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4281 - accuracy: 0.8416 - val_loss: 0.4790 - val_accuracy: 0.8694\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 14s 8ms/step - loss: 0.4204 - accuracy: 0.8424 - val_loss: 0.4402 - val_accuracy: 0.8746\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(300, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n",
    "    keras.layers.AlphaDropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "n_epochs = 20\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "pressed-monday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.4756 - accuracy: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4755621552467346, 0.859499990940094]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "pretty-thumbnail",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 4s 2ms/step - loss: 0.3529 - accuracy: 0.8823\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3528751730918884, 0.8822545409202576]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "generic-mauritius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 9s 5ms/step - loss: 0.4231 - accuracy: 0.8421\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-priority",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) Dropout\n",
    "- In 2016, a paper by Yarin Gal and Zoubin Ghahramani added a few more good reasons to use dropout:\n",
    "    - First, the paper established a profound connection between dropout networks (i.e., neural networks containing a Dropout layer before every weight layer) and approximate Bayesian inference, giving dropout a solid mathematical justification.\n",
    "    - Second, the authors introduced a powerful technique called MC Dropout, which can boost the performance of any trained dropout model without having to retrain it or even modify it at all, provides a much better measure of the model’s uncertainty, and is also amazingly simple to implement.\n",
    "- Here is a full implementation of MC Dropout, boosting the dropout model we trained earlier without retraining it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "potential-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "differential-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test_scaled, training=True)\n",
    "                     for sample in range(100)])\n",
    "y_proba = y_probas.mean(axis=0)\n",
    "y_std = y_probas.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-nightlife",
   "metadata": {},
   "source": [
    "- We just make 100 predictions over the test set, setting **training=True to ensure that the Dropout layer is active, and stack the predictions.** \n",
    "- Since dropout is active, all the predictions will be different. \n",
    "- Recall that predict() returns a matrix with one row per instance and one column per class.\n",
    "    - Because there are 10,000 instances in the test set and 10 classes, this is a matrix of shape [10000, 10].\n",
    "    - We stack 100 such matrices, so y_probas is an array of shape [100, 10000, 10]. Once we average over the first dimension (axis=0), we get y_proba, an array of shape [10000, 10], like we would get with a single prediction.\n",
    "- That’s all! **Averaging over multiple predictions with dropout on gives us a Monte Carlo estimate** that is generally more reliable than the result of a single prediction with dropout off.\n",
    "\n",
    "For example, let’s look at the model’s prediction for the first instance in the test set, with dropout off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "detected-manhattan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test_scaled[:1]), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-disability",
   "metadata": {},
   "source": [
    "The model seems almost certain that this image belongs to class 9 (ankle boot). Should you trust it? Is there really so little room for doubt? Compare this with the predictions made when dropout is activated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "temporal-transparency",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.66, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.87, 0.  , 0.12]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.21, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.48, 0.  , 0.51]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.77, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.56, 0.  , 0.37]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.24, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.34, 0.  , 0.06, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.12, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.  , 0.25, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.22, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.14, 0.  , 0.33, 0.  , 0.53]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.34, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.18, 0.  , 0.19, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.  , 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.67, 0.  , 0.32]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46, 0.  , 0.53]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.65, 0.01, 0.01, 0.  , 0.33]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.02, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.22, 0.  , 0.78]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.04, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.77, 0.  , 0.19]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.27, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.42, 0.  , 0.45]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.08, 0.  , 0.9 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.31, 0.  , 0.51, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.48, 0.  , 0.04, 0.  , 0.48]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.13, 0.  , 0.81, 0.  , 0.05]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.14, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.62, 0.  , 0.34]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.1 , 0.  , 0.88]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.29, 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.79, 0.  , 0.03, 0.  , 0.18]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.2 , 0.  , 0.79]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.38, 0.  , 0.39, 0.  , 0.23]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.12, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.63]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.22, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.29, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.83, 0.  , 0.15]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.07, 0.  , 0.87]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.01, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.41, 0.  , 0.59]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.22, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.31, 0.  , 0.68]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.24, 0.  , 0.6 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.04, 0.  , 0.71]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.3 , 0.  , 0.7 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.02, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.41, 0.  , 0.57]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.28, 0.  , 0.66, 0.  , 0.06]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.39, 0.  , 0.52, 0.  , 0.09]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.41, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.52, 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.71, 0.  , 0.24]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.98]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.  , 0.16, 0.  , 0.8 ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.47, 0.  , 0.53]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.  , 0.36, 0.  , 0.58]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.21, 0.  , 0.04, 0.  , 0.75]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.07, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.64, 0.  , 0.34]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.44, 0.  , 0.54]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.34, 0.  , 0.65]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.89]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.03, 0.  , 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.05, 0.  , 0.14, 0.  , 0.82]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.35, 0.  , 0.64]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.9 , 0.  , 0.06, 0.  , 0.04]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.19, 0.  , 0.04, 0.  , 0.77]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.03, 0.  , 0.97]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.04, 0.  , 0.95]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.02, 0.  , 0.12, 0.  , 0.86]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.25, 0.  , 0.5 , 0.  , 0.25]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.1 , 0.  , 0.16, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.14, 0.  , 0.85]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.99]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.25, 0.  , 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.91]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.07, 0.  , 0.37, 0.  , 0.56]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.34, 0.  , 0.23, 0.  , 0.43]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-wrestling",
   "metadata": {},
   "source": [
    "- This tells a very different story: apparently, when we activate dropout, the model is not sure anymore.\n",
    "\n",
    "Once we average over the first dimension, we get the following MC Dropout predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "gothic-thong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.08, 0.  , 0.24, 0.  , 0.68]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-barrel",
   "metadata": {},
   "source": [
    "- The model still thinks this image belongs to class 9, but only with a 68% confidence, which seems much more reasonable than 99%. \n",
    "- Plus it’s useful to know exactly which other classes it thinks are likely. \n",
    "\n",
    "And you can also take a look at the standard deviation of the probability estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "israeli-congress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.16, 0.  , 0.23, 0.  , 0.28]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-controversy",
   "metadata": {},
   "source": [
    "- Apparently there’s quite a lot of variance in the probability estimates: if you were building a risk-sensitive system (e.g., a medical or financial system), you should probably treat such an uncertain prediction with extreme caution. \n",
    "- You definitely would not treat it like a 99% confident prediction. Moreover, the model’s accuracy got a small boost from 86.8 to 86.9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "temporal-tiffany",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_proba, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "revolutionary-principle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8672"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.sum(y_pred == y_test) / len(y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-supply",
   "metadata": {},
   "source": [
    "### NOTE\n",
    "- The number of Monte Carlo samples you use (100 in this example) is a hyperparameter you can tweak. \n",
    "    - **The higher it is, the more accurate the predictions and their uncertainty estimates will be. However, if you double it, inference time will also be doubled.**\n",
    "    - Moreover, above a certain number of samples, you will notice little improvement. So your job is to find the right trade-off between latency and accuracy, depending on your application.\n",
    "***\n",
    "- **If your model contains other layers that behave in a special way during training (such as BatchNormalization layers)**, then **you should not force training mode** like we just did. \n",
    "    - **Instead, you should replace the Dropout layers with the following MCDropout class:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "suited-philosophy",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)\n",
    "\n",
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-creativity",
   "metadata": {},
   "source": [
    "- If you are creating a model from scratch, it’s just a matter of using MCDropout rather than Dropout.\n",
    "    - But if you have a model that was already trained using Dropout, you need to create a new model that’s identical to the existing model except that it replaces the Dropout layers with MCDropout, then copy the existing model’s weights to your new model.\n",
    "- In short, MC Dropout is a fantastic technique that boosts dropout models and provides better uncertainty estimates. And of course, since it is just regular dropout during training, it also acts like a regularizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "supported-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "minimal-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "gross-shepherd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_18 (Flatten)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout (MCAlphaDro (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_1 (MCAlphaD (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "mc_alpha_dropout_2 (MCAlphaD (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "electrical-premises",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "mc_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "proud-receptor",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model.set_weights(model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-excerpt",
   "metadata": {},
   "source": [
    "Now we can use the model with MC Dropout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "statutory-weather",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.11, 0.  , 0.27, 0.  , 0.61]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(np.mean([mc_model.predict(X_test_scaled[:1]) for sample in range(100)], axis=0), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-chester",
   "metadata": {},
   "source": [
    "## Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "historical-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                           kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-devon",
   "metadata": {},
   "source": [
    "- After each training iteration, the model’s fit() method will call the object returned by max_norm(), passing it the layer’s weights and getting rescaled weights in return, which then replace the layer’s weights.\n",
    "- The max_norm() function has an axis argument that defaults to 0.\n",
    "    - A Dense layer usually has weights of shape [number of inputs, number of neurons], so using axis=0 means that the max-norm constraint will apply independently to each neuron’s weight vector.\n",
    "- If you want to use max-norm with convolutional layers, make sure to set the max_norm() constraint’s axis argument appropriately (usually axis=[0, 1, 2])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "binary-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1719/1719 [==============================] - 14s 7ms/step - loss: 0.5769 - accuracy: 0.8022 - val_loss: 0.3803 - val_accuracy: 0.8596\n",
      "Epoch 2/2\n",
      "1719/1719 [==============================] - 12s 7ms/step - loss: 0.3550 - accuracy: 0.8713 - val_loss: 0.3668 - val_accuracy: 0.8698\n"
     ]
    }
   ],
   "source": [
    "MaxNormDense = partial(keras.layers.Dense,\n",
    "                       activation=\"selu\", kernel_initializer=\"lecun_normal\",\n",
    "                       kernel_constraint=keras.constraints.max_norm(1.))\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    MaxNormDense(300),\n",
    "    MaxNormDense(100),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "n_epochs = 2\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs,\n",
    "                    validation_data=(X_valid_scaled, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-cylinder",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- We have covered a wide range of techniques, and you may be wondering which ones you should use.\n",
    "- This depends on the task, and there is no clear consensus yet, but I have found the configuration in Table 11-3 to work fine in most cases, without requiring much hyperparameter tuning.\n",
    "- That said, please do not consider these defaults as hard rules!\n",
    "\n",
    "<img src=\"t3.png\">\n",
    "\n",
    "- If the network is a simple stack of dense layers, then it can self-normalize, and you should use the configuration in Table 11-4 instead.\n",
    "\n",
    "<img src=\"t4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "novel-perception",
   "metadata": {},
   "source": [
    "- Don’t forget to normalize the input features! You should also try to reuse parts of a pretrained neural network if you can find one that solves a similar problem, or use unsupervised pretraining if you have a lot of unlabeled data, or use pretraining on an auxiliary task if you have a lot of labeled data for a similar task.\n",
    "- While the previous guidelines should cover most cases, here are some exceptions:\n",
    "    - **If you need a sparse model, you can use ℓ1 regularization (and optionally zero out the tiny weights after training).** \n",
    "        - If you need an even sparser model, you can use the TensorFlow Model Optimization Toolkit. \n",
    "        - This will break self-normalization, so you should use the default configuration in this case.\n",
    "    - **If you need a low-latency model (one that performs lightning-fast predictions), you may need to use fewer layers, fold the Batch Normalization layers into the previous layers, and possibly use a faster activation function such as leaky ReLU or just ReLU.**\n",
    "       - Having a sparse model will also help. Finally, you may want to reduce the float precision from 32 bits to 16 or even 8 bits. Again, check out TF-MOT.\n",
    "    - **If you are building a risk-sensitive application, or inference latency is not very important** in your application, you can use **MC Dropout to boost performance** and get more reliable probability estimates, along with uncertainty estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imposed-fortune",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "##  Deep Learning on CIFAR10\n",
    "### a.\n",
    "Exercise: Build a DNN with 20 hidden layers of 100 neurons each (that's too many, but it's the point of this exercise). Use He initialization and the ELU activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "orange-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-quick",
   "metadata": {},
   "source": [
    "### b.\n",
    "Exercise: Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you'll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model's architecture or hyperparameters.\n",
    "\n",
    "Let's add the output layer to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "explicit-persian",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-continuity",
   "metadata": {},
   "source": [
    "Let's use a Nadam optimizer with a learning rate of 5e-5. I tried learning rates 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3 and 1e-2, and I compared their learning curves for 10 epochs each (using the TensorBoard callback, below). The learning rates 3e-5 and 1e-4 were pretty good, so I tried 5e-5, which turned out to be slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "statewide-saturday",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-assessment",
   "metadata": {},
   "source": [
    "Let's load the CIFAR10 dataset. We also want to use early stopping, so we need a validation set. Let's use the first 5,000 images of the original training set as the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "welcome-chart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 100s 1us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hourly-outdoors",
   "metadata": {},
   "source": [
    "Now we can create the callbacks we need and train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "sophisticated-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "olive-evidence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 9404."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir=./my_cifar10_logs --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "civilian-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 34s 20ms/step - loss: 9.7350 - accuracy: 0.1397 - val_loss: 2.1328 - val_accuracy: 0.2306\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 2.0740 - accuracy: 0.2425 - val_loss: 2.0154 - val_accuracy: 0.2446\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.9617 - accuracy: 0.2785 - val_loss: 1.9702 - val_accuracy: 0.2850\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.8758 - accuracy: 0.3177 - val_loss: 1.8707 - val_accuracy: 0.3320\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.8140 - accuracy: 0.3415 - val_loss: 1.7675 - val_accuracy: 0.3508\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.7561 - accuracy: 0.3607 - val_loss: 1.7442 - val_accuracy: 0.3636\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.7133 - accuracy: 0.3777 - val_loss: 1.7050 - val_accuracy: 0.3730\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6680 - accuracy: 0.3967 - val_loss: 1.6871 - val_accuracy: 0.3974\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.6447 - accuracy: 0.4067 - val_loss: 1.6490 - val_accuracy: 0.4002\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.6096 - accuracy: 0.4180 - val_loss: 1.6978 - val_accuracy: 0.3796\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.5781 - accuracy: 0.4324 - val_loss: 1.6530 - val_accuracy: 0.3920\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.5625 - accuracy: 0.4406 - val_loss: 1.6467 - val_accuracy: 0.4088\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.5406 - accuracy: 0.4495 - val_loss: 1.6650 - val_accuracy: 0.4032\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 1.5180 - accuracy: 0.4536 - val_loss: 1.5829 - val_accuracy: 0.4314\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.5121 - accuracy: 0.4584 - val_loss: 1.5965 - val_accuracy: 0.4278\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.5007 - accuracy: 0.4625 - val_loss: 1.5785 - val_accuracy: 0.4384\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.4798 - accuracy: 0.4696 - val_loss: 1.5757 - val_accuracy: 0.4304s: 1.4798 - accuracy: \n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 34s 24ms/step - loss: 1.4654 - accuracy: 0.4761 - val_loss: 1.5731 - val_accuracy: 0.4432\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.4431 - accuracy: 0.4856 - val_loss: 1.5786 - val_accuracy: 0.4408\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.4386 - accuracy: 0.4838 - val_loss: 1.6125 - val_accuracy: 0.4340\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.4379 - accuracy: 0.4815 - val_loss: 1.5770 - val_accuracy: 0.4384\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.4161 - accuracy: 0.4894 - val_loss: 1.5682 - val_accuracy: 0.4436\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.4025 - accuracy: 0.4962 - val_loss: 1.5685 - val_accuracy: 0.4344\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3926 - accuracy: 0.4989 - val_loss: 1.5546 - val_accuracy: 0.4412\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.3816 - accuracy: 0.5022 - val_loss: 1.5393 - val_accuracy: 0.4506\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.3586 - accuracy: 0.5108 - val_loss: 1.5640 - val_accuracy: 0.4396\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.3618 - accuracy: 0.5128 - val_loss: 1.5419 - val_accuracy: 0.4548\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3503 - accuracy: 0.5173 - val_loss: 1.5610 - val_accuracy: 0.4400\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.3513 - accuracy: 0.5132 - val_loss: 1.5352 - val_accuracy: 0.4570\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3500 - accuracy: 0.5169 - val_loss: 1.6587 - val_accuracy: 0.4358\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.3488 - accuracy: 0.5209 - val_loss: 1.5420 - val_accuracy: 0.4504\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3129 - accuracy: 0.5302 - val_loss: 1.5296 - val_accuracy: 0.4612\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.3223 - accuracy: 0.5292 - val_loss: 1.5689 - val_accuracy: 0.4558\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2973 - accuracy: 0.5380 - val_loss: 1.5247 - val_accuracy: 0.4658\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2984 - accuracy: 0.5367 - val_loss: 1.5767 - val_accuracy: 0.4518\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2883 - accuracy: 0.5394 - val_loss: 1.5478 - val_accuracy: 0.4588\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.2795 - accuracy: 0.5459 - val_loss: 1.5394 - val_accuracy: 0.4664\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2726 - accuracy: 0.5453 - val_loss: 1.5586 - val_accuracy: 0.4614\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2693 - accuracy: 0.5430 - val_loss: 1.5410 - val_accuracy: 0.4586\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.2521 - accuracy: 0.5564 - val_loss: 1.5407 - val_accuracy: 0.4578\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2493 - accuracy: 0.5531 - val_loss: 1.5601 - val_accuracy: 0.4612\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.2446 - accuracy: 0.5555 - val_loss: 1.5572 - val_accuracy: 0.4614\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2407 - accuracy: 0.5552 - val_loss: 1.5491 - val_accuracy: 0.4636\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2247 - accuracy: 0.5634 - val_loss: 1.5606 - val_accuracy: 0.4514\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.2230 - accuracy: 0.5648 - val_loss: 1.5741 - val_accuracy: 0.4556\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 1.2079 - accuracy: 0.5699 - val_loss: 1.5378 - val_accuracy: 0.4654\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.2075 - accuracy: 0.5722 - val_loss: 1.5747 - val_accuracy: 0.4716\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.1994 - accuracy: 0.5704 - val_loss: 1.5571 - val_accuracy: 0.4582\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2008 - accuracy: 0.5715 - val_loss: 1.5610 - val_accuracy: 0.4726\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 26s 18ms/step - loss: 1.1846 - accuracy: 0.5779 - val_loss: 1.5734 - val_accuracy: 0.4624\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.1827 - accuracy: 0.5773 - val_loss: 1.6130 - val_accuracy: 0.4618\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.1769 - accuracy: 0.5785 - val_loss: 1.5872 - val_accuracy: 0.4614\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.1676 - accuracy: 0.5820 - val_loss: 1.5613 - val_accuracy: 0.4630\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 26s 19ms/step - loss: 1.1582 - accuracy: 0.5868 - val_loss: 1.5988 - val_accuracy: 0.4616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b914dc19c8>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "speaking-patrick",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 6ms/step - loss: 1.5247 - accuracy: 0.4658\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.52471923828125, 0.4657999873161316]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-algebra",
   "metadata": {},
   "source": [
    "### c.\n",
    "Exercise: Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
    "\n",
    "The code below is very similar to the code above, with a few changes:\n",
    "\n",
    "- I added a BN layer after every Dense layer (before the activation function), except for the output layer. I also added a BN layer before the first hidden layer.\n",
    "- I changed the learning rate to 5e-4. I experimented with 1e-5, 3e-5, 5e-5, 1e-4, 3e-4, 5e-4, 1e-3 and 3e-3, and I chose the one with the best validation performance after 20 epochs.\n",
    "- I renamed the run directories to runbn* and the model file name to my_cifar10_bn_model.h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "western-kuwait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 60s 32ms/step - loss: 1.9742 - accuracy: 0.2953 - val_loss: 1.6885 - val_accuracy: 0.3918\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.6818 - accuracy: 0.4001 - val_loss: 1.6009 - val_accuracy: 0.4318\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.6127 - accuracy: 0.4255 - val_loss: 1.5199 - val_accuracy: 0.4576\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 1.5518 - accuracy: 0.4473 - val_loss: 1.5225 - val_accuracy: 0.4586\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.5037 - accuracy: 0.4670 - val_loss: 1.4538 - val_accuracy: 0.4790\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4653 - accuracy: 0.4757 - val_loss: 1.4374 - val_accuracy: 0.4842\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 40s 28ms/step - loss: 1.4323 - accuracy: 0.4927 - val_loss: 1.4233 - val_accuracy: 0.4884\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.4043 - accuracy: 0.5023 - val_loss: 1.3987 - val_accuracy: 0.4996\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 43s 30ms/step - loss: 1.3818 - accuracy: 0.5104 - val_loss: 1.3679 - val_accuracy: 0.5124\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 1.3579 - accuracy: 0.5173 - val_loss: 1.3721 - val_accuracy: 0.5112\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 44s 32ms/step - loss: 1.3272 - accuracy: 0.5357 - val_loss: 1.3537 - val_accuracy: 0.5180\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.3103 - accuracy: 0.5338 - val_loss: 1.3854 - val_accuracy: 0.5034\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.2902 - accuracy: 0.5455 - val_loss: 1.3906 - val_accuracy: 0.5056\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 1.2701 - accuracy: 0.5522 - val_loss: 1.3488 - val_accuracy: 0.5240\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.2584 - accuracy: 0.5582 - val_loss: 1.3703 - val_accuracy: 0.5152\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.2521 - accuracy: 0.5567 - val_loss: 1.3488 - val_accuracy: 0.5240\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.2297 - accuracy: 0.5647 - val_loss: 1.3351 - val_accuracy: 0.5280\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.2073 - accuracy: 0.5741 - val_loss: 1.3235 - val_accuracy: 0.5348\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.1956 - accuracy: 0.5773 - val_loss: 1.3498 - val_accuracy: 0.5284\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.1824 - accuracy: 0.5844 - val_loss: 1.3494 - val_accuracy: 0.5290\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 44s 32ms/step - loss: 1.1660 - accuracy: 0.5867 - val_loss: 1.3488 - val_accuracy: 0.5320\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.1568 - accuracy: 0.5912 - val_loss: 1.3313 - val_accuracy: 0.5324\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 44s 31ms/step - loss: 1.1415 - accuracy: 0.5957 - val_loss: 1.3344 - val_accuracy: 0.5352\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.1291 - accuracy: 0.6000 - val_loss: 1.3130 - val_accuracy: 0.5430\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.1200 - accuracy: 0.6080 - val_loss: 1.3350 - val_accuracy: 0.5326\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.1044 - accuracy: 0.6066 - val_loss: 1.3375 - val_accuracy: 0.5386\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 45s 32ms/step - loss: 1.0896 - accuracy: 0.6176 - val_loss: 1.3466 - val_accuracy: 0.5332\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 48s 34ms/step - loss: 1.0837 - accuracy: 0.6199 - val_loss: 1.3444 - val_accuracy: 0.5364\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 49s 35ms/step - loss: 1.0818 - accuracy: 0.6182 - val_loss: 1.3296 - val_accuracy: 0.5402\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 47s 34ms/step - loss: 1.0735 - accuracy: 0.6250 - val_loss: 1.3616 - val_accuracy: 0.5344\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 48s 34ms/step - loss: 1.0669 - accuracy: 0.6232 - val_loss: 1.3645 - val_accuracy: 0.5362\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 48s 34ms/step - loss: 1.0397 - accuracy: 0.6332 - val_loss: 1.3957 - val_accuracy: 0.5284\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 47s 34ms/step - loss: 1.0366 - accuracy: 0.6350 - val_loss: 1.3417 - val_accuracy: 0.5460\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 51s 36ms/step - loss: 1.0219 - accuracy: 0.6393 - val_loss: 1.3364 - val_accuracy: 0.5452\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 49s 35ms/step - loss: 1.0186 - accuracy: 0.6425 - val_loss: 1.3617 - val_accuracy: 0.5444\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 1.0114 - accuracy: 0.6445 - val_loss: 1.3621 - val_accuracy: 0.5344\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 0.9908 - accuracy: 0.6515 - val_loss: 1.3518 - val_accuracy: 0.5430\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 43s 31ms/step - loss: 0.9871 - accuracy: 0.6527 - val_loss: 1.3719 - val_accuracy: 0.5394\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.9757 - accuracy: 0.6560 - val_loss: 1.3732 - val_accuracy: 0.5386\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.9686 - accuracy: 0.6629 - val_loss: 1.3907 - val_accuracy: 0.5294\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.9622 - accuracy: 0.6642 - val_loss: 1.3499 - val_accuracy: 0.5470\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.9543 - accuracy: 0.6682 - val_loss: 1.3959 - val_accuracy: 0.5398\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 42s 30ms/step - loss: 0.9524 - accuracy: 0.6673 - val_loss: 1.3895 - val_accuracy: 0.5444\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 41s 29ms/step - loss: 0.9418 - accuracy: 0.6691 - val_loss: 1.4222 - val_accuracy: 0.5314\n",
      "157/157 [==============================] - 2s 6ms/step - loss: 1.3130 - accuracy: 0.5430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3130398988723755, 0.5429999828338623]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-watts",
   "metadata": {},
   "source": [
    "- Is the model converging faster than before? Much faster! The previous model took 27 epochs to reach the lowest validation loss, while the new model achieved that same loss in just 5 epochs and continued to make progress until the 18th epoch. The BN layers stabilized training and allowed us to use a much larger learning rate, so convergence was faster.\n",
    "- Does BN produce a better model? Yes! The final model is also much better, with 54.0% accuracy instead of 47.6%. It's still not a very good model, but at least it's much better than before (a Convolutional Neural Network would do much better, but that's a different topic, see chapter 14).\n",
    "- How does BN affect training speed? Although the model converged much faster, each epoch took about 12s instead of 8s, because of the extra computations required by the BN layers. But overall the training time (wall time) was shortened significantly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-special",
   "metadata": {},
   "source": [
    "### d.\n",
    "Exercise: Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "australian-turner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 39s 22ms/step - loss: 2.0613 - accuracy: 0.2676 - val_loss: 1.8792 - val_accuracy: 0.3230\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.7308 - accuracy: 0.3805 - val_loss: 1.7332 - val_accuracy: 0.3790\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6281 - accuracy: 0.4276 - val_loss: 1.6625 - val_accuracy: 0.4044\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5393 - accuracy: 0.4586 - val_loss: 1.6203 - val_accuracy: 0.4432\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4842 - accuracy: 0.4814 - val_loss: 1.5983 - val_accuracy: 0.4248\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4447 - accuracy: 0.4944 - val_loss: 1.5023 - val_accuracy: 0.4646\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3915 - accuracy: 0.5165 - val_loss: 1.5350 - val_accuracy: 0.4680\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 27s 20ms/step - loss: 1.3503 - accuracy: 0.5256 - val_loss: 1.5265 - val_accuracy: 0.4622\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.3289 - accuracy: 0.5392 - val_loss: 1.5244 - val_accuracy: 0.4726\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2939 - accuracy: 0.5515 - val_loss: 1.5498 - val_accuracy: 0.4764\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2620 - accuracy: 0.5676 - val_loss: 1.5624 - val_accuracy: 0.4840\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2367 - accuracy: 0.5738 - val_loss: 1.4863 - val_accuracy: 0.4960\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2176 - accuracy: 0.5807 - val_loss: 1.4829 - val_accuracy: 0.4988\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1690 - accuracy: 0.5997 - val_loss: 1.5178 - val_accuracy: 0.4938\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1628 - accuracy: 0.6036 - val_loss: 1.5147 - val_accuracy: 0.4952\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.2866 - accuracy: 0.5554 - val_loss: 1.4950 - val_accuracy: 0.4968\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1931 - accuracy: 0.5949 - val_loss: 1.5266 - val_accuracy: 0.5024\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1219 - accuracy: 0.6170 - val_loss: 1.5066 - val_accuracy: 0.5062\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0913 - accuracy: 0.6243 - val_loss: 1.5670 - val_accuracy: 0.5020\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0894 - accuracy: 0.6285 - val_loss: 1.5292 - val_accuracy: 0.4888\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0667 - accuracy: 0.6338 - val_loss: 1.6128 - val_accuracy: 0.4980\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0679 - accuracy: 0.6324 - val_loss: 1.5465 - val_accuracy: 0.4986\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0317 - accuracy: 0.6502 - val_loss: 1.5836 - val_accuracy: 0.4932\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0228 - accuracy: 0.6505 - val_loss: 1.5641 - val_accuracy: 0.4990\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0098 - accuracy: 0.6610 - val_loss: 1.5522 - val_accuracy: 0.5056\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9710 - accuracy: 0.6698 - val_loss: 1.6168 - val_accuracy: 0.4986\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9708 - accuracy: 0.6695 - val_loss: 1.5976 - val_accuracy: 0.4990\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 0.9521 - accuracy: 0.6781 - val_loss: 1.5931 - val_accuracy: 0.4868\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9470 - accuracy: 0.6788 - val_loss: 1.5530 - val_accuracy: 0.5090\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9311 - accuracy: 0.6859 - val_loss: 1.6608 - val_accuracy: 0.4942\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.9248 - accuracy: 0.6864 - val_loss: 1.6054 - val_accuracy: 0.5026\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9037 - accuracy: 0.6963 - val_loss: 1.6425 - val_accuracy: 0.4942\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.8916 - accuracy: 0.6993 - val_loss: 1.6379 - val_accuracy: 0.5016\n",
      "157/157 [==============================] - 1s 5ms/step - loss: 1.4829 - accuracy: 0.4988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4828822612762451, 0.49880000948905945]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "grateful-recipient",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 6ms/step - loss: 1.4829 - accuracy: 0.4988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4828822612762451, 0.49880000948905945]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharp-minutes",
   "metadata": {},
   "source": [
    "We get 49.8% accuracy, which is not much better than the original model (47.6%), and not as good as the model using batch normalization (54.0%). However, convergence was almost as fast as with the BN model, plus each epoch took only 7 seconds. So it's by far the fastest model to train so far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-actor",
   "metadata": {},
   "source": [
    "### e.\n",
    "Exercise: Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "trained-disposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 59s 30ms/step - loss: 2.0557 - accuracy: 0.2807 - val_loss: 1.7831 - val_accuracy: 0.3776\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.6717 - accuracy: 0.4076 - val_loss: 1.7089 - val_accuracy: 0.3960\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.5776 - accuracy: 0.4486 - val_loss: 1.6406 - val_accuracy: 0.4324\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4946 - accuracy: 0.4766 - val_loss: 1.5477 - val_accuracy: 0.4596\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.4407 - accuracy: 0.4943 - val_loss: 1.6497 - val_accuracy: 0.4414\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3927 - accuracy: 0.5134 - val_loss: 1.5282 - val_accuracy: 0.4762\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 1.3452 - accuracy: 0.5303 - val_loss: 1.5490 - val_accuracy: 0.4692\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2988 - accuracy: 0.5441 - val_loss: 1.5115 - val_accuracy: 0.4850\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.2702 - accuracy: 0.5579 - val_loss: 1.5513 - val_accuracy: 0.4816\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2378 - accuracy: 0.5701 - val_loss: 1.5009 - val_accuracy: 0.4954\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1974 - accuracy: 0.5828 - val_loss: 1.5413 - val_accuracy: 0.5028\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1777 - accuracy: 0.5932 - val_loss: 1.5566 - val_accuracy: 0.4896\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.1407 - accuracy: 0.6033 - val_loss: 1.6082 - val_accuracy: 0.4936\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0956 - accuracy: 0.6216 - val_loss: 1.5983 - val_accuracy: 0.5004\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0853 - accuracy: 0.6231 - val_loss: 1.6099 - val_accuracy: 0.5102\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0672 - accuracy: 0.6323 - val_loss: 1.6183 - val_accuracy: 0.4958\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0482 - accuracy: 0.6390 - val_loss: 1.6666 - val_accuracy: 0.5070\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0205 - accuracy: 0.6456 - val_loss: 1.6277 - val_accuracy: 0.5104\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9920 - accuracy: 0.6652 - val_loss: 1.6775 - val_accuracy: 0.5074\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9682 - accuracy: 0.6700 - val_loss: 1.6648 - val_accuracy: 0.5058\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9597 - accuracy: 0.6707 - val_loss: 1.6936 - val_accuracy: 0.5012\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9350 - accuracy: 0.6796 - val_loss: 1.7349 - val_accuracy: 0.5110\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9090 - accuracy: 0.6888 - val_loss: 1.7123 - val_accuracy: 0.5016\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.8975 - accuracy: 0.6946 - val_loss: 1.7432 - val_accuracy: 0.5064\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 0.8933 - accuracy: 0.6946 - val_loss: 1.7931 - val_accuracy: 0.4982\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.8564 - accuracy: 0.7116 - val_loss: 1.7471 - val_accuracy: 0.5012\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.8453 - accuracy: 0.7157 - val_loss: 1.8183 - val_accuracy: 0.4954\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.8392 - accuracy: 0.7132 - val_loss: 1.8226 - val_accuracy: 0.4926\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.8136 - accuracy: 0.7214 - val_loss: 1.9056 - val_accuracy: 0.5016\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.8115 - accuracy: 0.7244 - val_loss: 1.8688 - val_accuracy: 0.5008\n",
      "157/157 [==============================] - 1s 6ms/step - loss: 1.5009 - accuracy: 0.4954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.500890851020813, 0.49540001153945923]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-mileage",
   "metadata": {},
   "source": [
    "Let's use MC Dropout now. We will need the MCAlphaDropout class we used earlier, so let's just copy it here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "accessible-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-score",
   "metadata": {},
   "source": [
    "Now let's create a new model, identical to the one we just trained (with the same weights), but with MCAlphaDropout dropout layers instead of AlphaDropout layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "burning-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-specialist",
   "metadata": {},
   "source": [
    "Then let's add a couple utility functions. The first will run the model many times (10 by default) and it will return the mean predicted class probabilities. The second will use these mean probabilities to predict the most likely class for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fancy-october",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-destination",
   "metadata": {},
   "source": [
    "Now let's make predictions for all the instances in the validation set, and compute the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "creative-customs",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4932"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exceptional-child",
   "metadata": {},
   "source": [
    "We get no accuracy improvement in this case (we're still at 49% accuracy).\n",
    "\n",
    "So the best model we got in this exercise is the Batch Normalization model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-monday",
   "metadata": {},
   "source": [
    "### f.\n",
    "Exercise: Retrain your model using 1cycle scheduling and see if it improves training speed and model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "scientific-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "conscious-perfume",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 11s 26ms/step - loss: nan - accuracy: 0.1252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.6187450885772705,\n",
       " 3.9368624346596857)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "protective-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-2)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "seven-barrier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 11s 27ms/step - loss: 2.2315 - accuracy: 0.2381 - val_loss: 1.8069 - val_accuracy: 0.3794\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 1.7926 - accuracy: 0.3699 - val_loss: 1.6318 - val_accuracy: 0.4166\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 1.6395 - accuracy: 0.4185 - val_loss: 1.6519 - val_accuracy: 0.4196\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 1.5448 - accuracy: 0.4532 - val_loss: 1.6267 - val_accuracy: 0.4330\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 1.4861 - accuracy: 0.4737 - val_loss: 1.5406 - val_accuracy: 0.4676\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.4361 - accuracy: 0.4894 - val_loss: 1.5364 - val_accuracy: 0.4660\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 1.4040 - accuracy: 0.4998 - val_loss: 1.6452 - val_accuracy: 0.4402\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.3441 - accuracy: 0.5222 - val_loss: 1.4777 - val_accuracy: 0.4990\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.2689 - accuracy: 0.5464 - val_loss: 1.5298 - val_accuracy: 0.4848\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.1947 - accuracy: 0.5711 - val_loss: 1.5063 - val_accuracy: 0.5026\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 1.1216 - accuracy: 0.6000 - val_loss: 1.5581 - val_accuracy: 0.5042\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 9s 26ms/step - loss: 1.0657 - accuracy: 0.6188 - val_loss: 1.5349 - val_accuracy: 0.5054\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 0.9920 - accuracy: 0.6454 - val_loss: 1.5390 - val_accuracy: 0.5176\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 0.9130 - accuracy: 0.6738 - val_loss: 1.5634 - val_accuracy: 0.5222\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 9s 25ms/step - loss: 0.8857 - accuracy: 0.6846 - val_loss: 1.5951 - val_accuracy: 0.5236\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-tribune",
   "metadata": {},
   "source": [
    "One cycle allowed us to train the model in just 15 epochs, each taking only 2 seconds (thanks to the larger batch size). This is several times faster than the fastest model we trained so far. Moreover, we improved the model's performance (from 47.6% to 52.0%). The batch normalized model reaches a slightly better performance (54%), but it's much slower to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-camera",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
