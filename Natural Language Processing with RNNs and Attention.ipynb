{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cultural-feelings",
   "metadata": {
    "executionInfo": {
     "elapsed": 740,
     "status": "ok",
     "timestamp": 1618258486980,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "cultural-feelings"
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PoAHbbrUvnEP",
   "metadata": {
    "id": "PoAHbbrUvnEP"
   },
   "source": [
    "## Char-RNN\n",
    "- In a famous 2015 blog post titled “The Unreasonable Effectiveness of Recurrent Neural Networks,” Andrej Karpathy showed how to train an RNN to predict the next character in a sentence. This Char-RNN can then be used to generate novel text, one character at a time.\n",
    "\n",
    "### Splitting a sequence into batches of shuffled windows\n",
    "For example, let's split the sequence 0 to 14 into windows of length 5, each shifted by 2 (e.g.,[0, 1, 2, 3, 4], [2, 3, 4, 5, 6], etc.), then shuffle them, and split them into inputs (the first 4 steps) and targets (the last 4 steps) (e.g., [2, 3, 4, 5, 6] would be split into [[2, 3, 4, 5], [3, 4, 5, 6]]), then \n",
    "create batches of 3 such input/target pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pBRREpeawnKe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6733,
     "status": "ok",
     "timestamp": 1618258492986,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "pBRREpeawnKe",
    "outputId": "aca000df-7eef-483f-9b78-22f420bf1ba1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________ Batch 0 \n",
      "X_batch\n",
      "[[6 7 8 9]\n",
      " [2 3 4 5]\n",
      " [4 5 6 7]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 7  8  9 10]\n",
      " [ 3  4  5  6]\n",
      " [ 5  6  7  8]]\n",
      "____________________ Batch 1 \n",
      "X_batch\n",
      "[[ 0  1  2  3]\n",
      " [ 8  9 10 11]\n",
      " [10 11 12 13]]\n",
      "===== \n",
      "Y_batch\n",
      "[[ 1  2  3  4]\n",
      " [ 9 10 11 12]\n",
      " [11 12 13 14]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "n_steps = 5\n",
    "dataset = tf.data.Dataset.from_tensor_slices(tf.range(15))\n",
    "dataset = dataset.window(n_steps, shift=2, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(n_steps))\n",
    "dataset = dataset.shuffle(10).map(lambda window: (window[:-1], window[1:]))\n",
    "dataset = dataset.batch(3).prefetch(1)\n",
    "for index, (X_batch, Y_batch) in enumerate(dataset):\n",
    "    print(\"_\" * 20, \"Batch\", index, \"\\nX_batch\")\n",
    "    print(X_batch.numpy())\n",
    "    print(\"=\" * 5, \"\\nY_batch\")\n",
    "    print(Y_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S6axTeI08Nqe",
   "metadata": {
    "id": "S6axTeI08Nqe"
   },
   "source": [
    "### Creating the Training Dataset\n",
    "First, let’s download all of Shakespeare’s work, using Keras’s handy **get_file()** function and downloading the data from Andrej Karpathy’s Char-RNN project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acting-check",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6722,
     "status": "ok",
     "timestamp": 1618258492987,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "acting-check",
    "outputId": "22ad3e03-cf84-4680-f29c-07d85a289e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "1122304/1115394 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "filepath = keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "  shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "BP1I1jok83LW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6713,
     "status": "ok",
     "timestamp": 1618258492989,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "BP1I1jok83LW",
    "outputId": "fb7702a0-d571-4571-c9ee-f2fd6945b5f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:148])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O9wOOpGi9php",
   "metadata": {
    "id": "O9wOOpGi9php"
   },
   "source": [
    "Next, we must encode every character as an integer. \n",
    "- One option is to create a custom preprocessing layer.\n",
    "- But in this case, it will be simpler to use Keras’s **Tokenizer** class.\n",
    "- First we need to fit a tokenizer to the text: it will find all the characters used in the text and map each of them to a different character ID, from 1 to the number of distinct characters (it does not start at 0, so we can use that value for masking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0JCWJKC88-nu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 6701,
     "status": "ok",
     "timestamp": 1618258492990,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "0JCWJKC88-nu",
    "outputId": "bfb6c859-9597-4d65-f255-0c39a64aedaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(shakespeare_text.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "FiXPRv7Z_zSU",
   "metadata": {
    "executionInfo": {
     "elapsed": 8018,
     "status": "ok",
     "timestamp": 1618258494320,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "FiXPRv7Z_zSU"
   },
   "outputs": [],
   "source": [
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8hfgVKd2AVKz",
   "metadata": {
    "id": "8hfgVKd2AVKz"
   },
   "source": [
    "- We set **char_level=True** to get character-level encoding rather than the default word-level encoding. \n",
    "- **Note that this tokenizer converts the text to lowercase by default**(but you can set lower=False if you do not want that).\n",
    "- Now the tokenizer can encode a sentence (or a list of sentences) to a list of character IDs and back, and it tells us how many distinct characters there are and the total number of characters in the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bWdq1gJCARc5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8010,
     "status": "ok",
     "timestamp": 1618258494322,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "bWdq1gJCARc5",
    "outputId": "7a8703a5-f604-4224-8c68-dd4ee3f7ed80"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences([\"First\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ulbx9EBBiC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7998,
     "status": "ok",
     "timestamp": 1618258494323,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "e3ulbx9EBBiC",
    "outputId": "7483afec-4a7d-4938-c20a-8b974a7c7326"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "XMSoAJNgBMlC",
   "metadata": {
    "executionInfo": {
     "elapsed": 7987,
     "status": "ok",
     "timestamp": 1618258494324,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "XMSoAJNgBMlC"
   },
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index) # number of distinct characters\n",
    "dataset_size = tokenizer.document_count # total number of characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mn8-VOumBu5l",
   "metadata": {
    "id": "Mn8-VOumBu5l"
   },
   "source": [
    "Let’s encode the full text so each character is represented by its ID **(we subtract 1 to get IDs from 0 to 38, rather than from 1 to 39)**:\n",
    "\n",
    "Before we continue, we need to split the dataset into a training set, a validation set, and a test set. We can’t just shuffle all the characters in the text, so how do you split a sequential dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hCILdOSnEQxz",
   "metadata": {
    "id": "hCILdOSnEQxz"
   },
   "source": [
    "### How to Split a Sequential Dataset\n",
    "- It is very important to avoid any overlap between the training set, the\n",
    "validation set, and the test set. \n",
    "- For example, we can take the first 90% of the text for the training set, then the next 5% for the validation set, and the final 5% for the test set. \n",
    "- It would also be a good idea to leave a gap between these sets to avoid the risk of a paragraph overlapping over two sets.\n",
    "***\n",
    "- When dealing with time series, you would in general split across time,: for\n",
    "example, you might take the years 2000 to 2012 for the training set, the years 2013 to 2015 for the validation set, and the years 2016 to 2018 for the test set.\n",
    "- However, in some cases you may be able to split along other dimensions, which will give you a longer time period to train on.\n",
    "> - For example, if you have data about the financial health of 10,000 companies from 2000 to 2018, you might be able to split this data across the different companies.\n",
    "> - It’s very likely that many of these companies will be strongly correlated, though (e.g., whole economic sectors may go up or down\n",
    "jointly), and if you have correlated companies across the training set and the\n",
    "test set your test set will not be as useful, as its measure of the\n",
    "generalization error will be optimistically biased.\n",
    "- So, it is often safer to split across time—but this implicitly assumes that the patterns the RNN can learn in the past (in the training set) will still exist in the future. \n",
    "> - In other words, we assume that the time series is stationary (at least in a wide sense).\n",
    "> - For many time series this assumption is reasonable (e.g., chemical reactions should be fine, since the laws of chemistry don’t change every day), but for many others it is not (e.g., financial markets are notoriously not stationary since patterns disappear as soon as traders spot them and start exploiting them).\n",
    "> - To make sure the time series is indeed sufficiently stationary, you can plot the model’s errors on the validation set across time: if the model performs much better on the first part of the validation set than on the last part, then the time series may not be stationary enough, and you might be better off training the model on a shorter time span.\n",
    "- In short, splitting a time series into a training set, a validation set, and a test set is not a trivial task, and how it’s done will depend strongly on the task at hand.\n",
    "\n",
    "Now back to Shakespeare! Let’s take the first 90% of the text for the training set (keeping the rest for the validation set and the test set), and create a tf.data.Dataset that will return each character one by one from this set:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3WaG7PpKByIL",
   "metadata": {
    "executionInfo": {
     "elapsed": 8349,
     "status": "ok",
     "timestamp": 1618258494695,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "3WaG7PpKByIL"
   },
   "outputs": [],
   "source": [
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1\n",
    "train_size = dataset_size * 90 // 100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MPnnICNdGjYM",
   "metadata": {
    "id": "MPnnICNdGjYM"
   },
   "source": [
    "### Chopping the Sequential Dataset into Multiple Windows\n",
    "- The training set now consists of a single sequence of over a million\n",
    "characters, so we can’t just train the neural network directly on it: the RNN would be equivalent to a deep net with over a million layers, and we would have a single (very long) instance to train it.\n",
    "- **Instead, we will use the dataset’s window() method to convert this long sequence of characters into many smaller windows of text.**\n",
    "- Every instance in the dataset will be a fairly short substring of the whole text, and the RNN will be unrolled only over the length of these substrings.\n",
    "> - This is called **truncated backpropagation through time.**\n",
    "- Let’s call the window() method to create a dataset of short text windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "WA-AU9dFOGli",
   "metadata": {
    "executionInfo": {
     "elapsed": 8353,
     "status": "ok",
     "timestamp": 1618258494707,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "WA-AU9dFOGli"
   },
   "outputs": [],
   "source": [
    "n_steps = 100\n",
    "window_length = n_steps + 1 # target = input shifted 1 character ahead\n",
    "dataset = dataset.repeat().window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RyUCBH3-lPMh",
   "metadata": {
    "id": "RyUCBH3-lPMh"
   },
   "source": [
    "#### TIP\n",
    "You can try tuning n_steps: it is easier to train RNNs on shorter input sequences, but of course the RNN will not be able to learn any pattern longer than n_steps, so don’t make it too small.\n",
    "***\n",
    "- **By default, the window() method creates nonoverlapping windows, but to get the largest possible training set we use shift=1 so that the first window contains characters 0 to 100, the second contains characters 1 to 101, and so on.**\n",
    "- **To ensure that all windows are exactly 101 characters long (which will allow us to create batches without having to do any padding), we set drop_remainder=True (otherwise the last 100 windows will contain 100 characters, 99 characters, and so on down to 1 character).**\n",
    "- The window() method creates a dataset that contains windows, each of\n",
    "which is also represented as a dataset.\n",
    "> - It’s a **nested dataset**, analogous to a list of lists.\n",
    "> - This is useful when you want to transform each window by calling its dataset methods (e.g., to shuffle them or batch them).\n",
    "- However, we cannot use a nested dataset directly for training, as our model will expect tensors as input, not datasets.\n",
    "- So, we must call the **flat_map()** method: **it converts a nested dataset into a flat dataset (one that does not contain datasets).**\n",
    "> - For example, suppose {1, 2, 3} represents a dataset containing the sequence of tensors 1, 2, and 3.\n",
    "> - If you flatten the nested dataset {{1, 2}, {3, 4, 5, 6}}, you get back the flat dataset {1, 2, 3, 4, 5, 6}.\n",
    "- Moreover, the **flat_map() method takes a function as an argument, which\n",
    "allows you to transform each dataset in the nested dataset before flattening.**\n",
    "> - For example, if you pass the function lambda ds: ds.batch(2) to flat_map(), then it will transform the nested dataset {{1, 2}, {3, 4, 5, 6}} into the flat dataset {[1, 2], [3, 4], [5, 6]}: it’s a dataset of tensors of size 2.\n",
    "- With that in mind, we are ready to flatten our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "qBdx27nxFfDs",
   "metadata": {
    "executionInfo": {
     "elapsed": 8345,
     "status": "ok",
     "timestamp": 1618258494708,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "qBdx27nxFfDs"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zO_SzD8KF1fQ",
   "metadata": {
    "id": "zO_SzD8KF1fQ"
   },
   "source": [
    "- Notice that we call batch(window_length) on each window: since all windows have exactly that length, we will get a single tensor for each of them.\n",
    "- Now the dataset contains consecutive windows of 101 characters each.\n",
    "- Since Gradient Descent works best when the instances in the training set are independent and identically distributed, we need to shuffle these windows.\n",
    "- Then we can batch the windows and separate the inputs (the first 100 characters) from the target (the last character):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "OdIgAM1fGPr6",
   "metadata": {
    "executionInfo": {
     "elapsed": 8336,
     "status": "ok",
     "timestamp": 1618258494710,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "OdIgAM1fGPr6"
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "AuKiyUzEGpnO",
   "metadata": {
    "executionInfo": {
     "elapsed": 8328,
     "status": "ok",
     "timestamp": 1618258494711,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "AuKiyUzEGpnO"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R2FeXnyPHRZc",
   "metadata": {
    "id": "R2FeXnyPHRZc"
   },
   "source": [
    "Figure 16-1 summarizes the dataset preparation steps discussed so far (showing windows of length 11 rather than 101, and a batch size of 3 instead of 32).\n",
    "\n",
    "![16-1.png](data:image/png;base64,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)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "POGJ205zIFqM",
   "metadata": {
    "id": "POGJ205zIFqM"
   },
   "source": [
    "- **Categorical input features should generally be encoded, usually as one-hot vectors or as embeddings.** \n",
    "> - Here, we will encode each character using a **one-hot vector** because there are fairly few distinct characters (only 39):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hwfT_zn0HzDB",
   "metadata": {
    "executionInfo": {
     "elapsed": 8321,
     "status": "ok",
     "timestamp": 1618258494712,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "hwfT_zn0HzDB"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bpXR7lMX-I",
   "metadata": {
    "id": "52bpXR7lMX-I"
   },
   "source": [
    "Finally, we just need to add prefetching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "USYWYo4SMYzn",
   "metadata": {
    "executionInfo": {
     "elapsed": 8314,
     "status": "ok",
     "timestamp": 1618258494713,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "USYWYo4SMYzn"
   },
   "outputs": [],
   "source": [
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "xp2pEakMYBqb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9373,
     "status": "ok",
     "timestamp": 1618258495782,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "xp2pEakMYBqb",
    "outputId": "93850c0d-98bd-4f22-e2d4-7d4846c25c89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "matYmpyaMcNU",
   "metadata": {
    "id": "matYmpyaMcNU"
   },
   "source": [
    "That’s it! Preparing the dataset was the hardest part. Now let’s create the\n",
    "model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IX_ToFiNMdMF",
   "metadata": {
    "id": "IX_ToFiNMdMF"
   },
   "source": [
    "### Building and Training the Char-RNN Model\n",
    "- **To predict the next character based on the previous 100 characters, we can use an RNN with 2 GRU layers of 128 units each and 20% dropout on both the inputs (dropout) and the hidden states (recurrent_dropout).**\n",
    "> - We can tweak these hyperparameters later, if needed.\n",
    "- **The output layer is a time distributed Dense layer.**\n",
    "> - **This time this layer must have 39 units (max_id) because there are 39 distinct characters in the text**, and we want to output a probability for each possible character (at each time step).\n",
    "> - The output probabilities should sum up to 1 at each time step, so we apply the **softmax activation function to the outputs of the Dense layer.**\n",
    "- We can then compile this model, using the **\"sparse_categorical_crossentropy\" loss** and an **Adam optimizer**.\n",
    "- Finally, we are ready to train the model for several epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "TA9d38yqEYFY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16297401,
     "status": "ok",
     "timestamp": 1618274783822,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "TA9d38yqEYFY",
    "outputId": "2a6aa840-4604-4ec3-fe59-78ec6629fd97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6095/31370 [====>.........................] - ETA: 3:14:53 - loss: 1.9049"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-c9fa952ce1b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"sparse_categorical_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id],\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                     dropout=0.2, recurrent_dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation=\"softmax\"))\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "history = model.fit(dataset, steps_per_epoch=train_size // batch_size, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W-j8yuRrsKiB",
   "metadata": {
    "id": "W-j8yuRrsKiB"
   },
   "source": [
    "### Using the Char-RNN Model\n",
    "- Now we have a model that can predict the next character in text written by Shakespeare. \n",
    "> - To feed it some text, we first need to preprocess it like we did\n",
    "earlier, so let’s create a little function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "TuOTY7AUsbzz",
   "metadata": {
    "executionInfo": {
     "elapsed": 16297397,
     "status": "ok",
     "timestamp": 1618274783830,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "TuOTY7AUsbzz"
   },
   "outputs": [],
   "source": [
    "def preprocess(texts):\n",
    "  X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "  return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2hNQ_S9etDFT",
   "metadata": {
    "id": "2hNQ_S9etDFT"
   },
   "source": [
    "Now let’s use the model to predict the next letter in some text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ENCyJwAPs80J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 16298038,
     "status": "ok",
     "timestamp": 1618274784481,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "ENCyJwAPs80J",
    "outputId": "0a6e76cc-1011-4405-9f9d-9725eb3c4cf0"
   },
   "outputs": [],
   "source": [
    "X_new = preprocess([\"How are yo\"])\n",
    "Y_pred = model.predict_classes(X_new)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1] # 1st sentence, last char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ehEMrd_csp4u",
   "metadata": {
    "id": "ehEMrd_csp4u"
   },
   "source": [
    "Success! The model guessed right. Now let’s use this model to generate\n",
    "new text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1Y1B-tHss2v",
   "metadata": {
    "id": "n1Y1B-tHss2v"
   },
   "source": [
    "### Generating Fake Shakespearean Text\n",
    "- To generate new text using the Char-RNN model, we could feed it some text, make the model predict the most likely next letter, add it at the end of the text, then give the extended text to the model to guess the next letter, and so on.\n",
    "> - But in practice this often leads to the same words being repeated over and over again.\n",
    "> - Instead, we can pick the next character randomly, with a probability equal to the estimated probability, using TensorFlow’s **tf.random.categorical()** function.\n",
    "> - This will generate more diverse and interesting text.\n",
    "- The categorical() function samples random class indices, given the class log probabilities (logits).\n",
    "- To have more control over the diversity of the generated text, we can divide the logits by a number called the **temperature**, which we can tweak as we wish: **a temperature close to 0 will favor the high-probability characters, while a very high temperature will give all characters an equal probability.**\n",
    "\n",
    "The following **next_char()** function uses this approach to pick the next character to add to the input text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tgjK6ZSDsrsb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 745,
     "status": "ok",
     "timestamp": 1618275192025,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "tgjK6ZSDsrsb",
    "outputId": "65b444a0-694b-4ece-b9df-06f5b63c135d"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "tf.random.categorical([[np.log(0.5), np.log(0.4), np.log(0.1)]], num_samples=40).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "rQQYMux3t9O1",
   "metadata": {
    "executionInfo": {
     "elapsed": 668,
     "status": "ok",
     "timestamp": 1618275259458,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "rQQYMux3t9O1"
   },
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "  X_new = preprocess([text])\n",
    "  y_proba = model(X_new)[0, -1:, :]\n",
    "  rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "  char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "  return tokenizer.sequences_to_texts(char_id.numpy())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ww8jgolGuQnA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 794,
     "status": "ok",
     "timestamp": 1618275276685,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "Ww8jgolGuQnA",
    "outputId": "077de0df-15c1-48c0-a514-1596e1fe4064"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "next_char(\"How are yo\", temperature=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b_9_awKMuW5L",
   "metadata": {
    "id": "b_9_awKMuW5L"
   },
   "source": [
    "Next, we can write a small function that will repeatedly call next_char() to get the next character and append it to the given text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "vDSduIKAuYiY",
   "metadata": {
    "executionInfo": {
     "elapsed": 616,
     "status": "ok",
     "timestamp": 1618275400545,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "vDSduIKAuYiY"
   },
   "outputs": [],
   "source": [
    "def complete_text(text, n_chars=50, temperature=1):\n",
    "  for _ in range(n_chars):\n",
    "    text += next_char(text, temperature)\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YWGdOy2Gu3OR",
   "metadata": {
    "id": "YWGdOy2Gu3OR"
   },
   "source": [
    "We are now ready to generate some text! Let’s try with different temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lbhj3X3-u4MS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9341,
     "status": "ok",
     "timestamp": 1618275483352,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "lbhj3X3-u4MS",
    "outputId": "3f513e85-0bd7-4008-fc62-f258b8cffa04"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\", temperature=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5nG1IhvMvJAt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8965,
     "status": "ok",
     "timestamp": 1618275519609,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "5nG1IhvMvJAt",
    "outputId": "9621237b-8b86-4eb6-f305-d2998ab10113"
   },
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xI8S1aZ5vRN-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9018,
     "status": "ok",
     "timestamp": 1618275549101,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "xI8S1aZ5vRN-",
    "outputId": "bf059e86-5ded-472b-9449-c667e4dbd5cc"
   },
   "outputs": [],
   "source": [
    "print(complete_text(\"t\", temperature=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ryFLxhJevcRJ",
   "metadata": {
    "id": "ryFLxhJevcRJ"
   },
   "source": [
    "- Apparently our Shakespeare model works best at a temperature close to 1.\n",
    "- To generate more convincing text, you could try using more GRU layers and more neurons per layer, train for longer, and add some regularization (for example, you could set recurrent_dropout=0.3 in the GRU layers).\n",
    "- Moreover, the model is currently incapable of learning patterns longer than n_steps, which is just 100 characters. \n",
    "> - You could try making this window larger, but it will also make training harder, and even LSTM and GRU cells cannot handle very long sequences. \n",
    "- Alternatively, you could use a **stateful RNN**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QNO0ZmvWv-Ph",
   "metadata": {
    "id": "QNO0ZmvWv-Ph"
   },
   "source": [
    "## Stateful RNN\n",
    "- Until now, we have used only stateless RNNs: at each training iteration the\n",
    "model starts with a hidden state full of zeros, then it updates this state at\n",
    "each time step, and after the last time step, it throws it away, as it is not\n",
    "needed anymore.\n",
    "- What if we told the RNN to preserve this final state after processing one training batch and use it as the initial state for the next training batch? \n",
    "> - This way the model can learn long-term patterns despite only backpropagating through short sequences. \n",
    "> - This is called a **stateful RNN**. \n",
    "\n",
    "Let’s see how to build one.\n",
    "\n",
    "- First, note that a stateful RNN only makes sense if each input sequence in a\n",
    "batch starts exactly where the corresponding sequence in the previous batch\n",
    "left off.\n",
    "> - **So the first thing we need to do to build a stateful RNN is to use sequential and nonoverlapping input sequences (rather than the shuffled and overlapping sequences we used to train stateless RNNs).**\n",
    "- When creating the Dataset, we must therefore use **shift=n_steps** (instead of shift=1) when calling the window() method.\n",
    "> - Moreover, **we must obviously not call the shuffle() method.**\n",
    "- Unfortunately, batching is much harder when preparing a dataset for a stateful RNN than it is for a stateless RNN.\n",
    "- Indeed, if we were to call batch(32), then 32 consecutive windows would be put in the same batch, and the following batch would not continue each of these\n",
    "window where it left off.\n",
    "> - The first batch would contain windows 1 to 32 and the second batch would contain windows 33 to 64, so if you consider, say, the first window of each batch (i.e., windows 1 and 33), you can see that they are not consecutive. \n",
    "- The simplest solution to this problem is to just **use “batches” containing a single window:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sBTW2eYPxnsg",
   "metadata": {
    "executionInfo": {
     "elapsed": 863,
     "status": "ok",
     "timestamp": 1618276171635,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "sBTW2eYPxnsg"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpBAuSA8xsP5",
   "metadata": {
    "executionInfo": {
     "elapsed": 626,
     "status": "ok",
     "timestamp": 1618276384862,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "bpBAuSA8xsP5"
   },
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])\n",
    "dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "dataset = dataset.repeat().batch(1)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m_3a46qiziCo",
   "metadata": {
    "id": "m_3a46qiziCo"
   },
   "source": [
    "Figure 16-2 summarizes the first steps.\n",
    "\n",
    "![16-2.png](data:image/png;base64,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)\n",
    "\n",
    "- Batching is harder, but it is not impossible. \n",
    "- For example, we could chop Shakespeare’s text into 32 texts of equal length, create one dataset of consecutive input sequences for each of them, and finally use tf.train.Dataset.zip(datasets).map(lambda *windows: tf.stack(windows)) to create proper consecutive batches, where the n\n",
    "input sequence in a batch starts off exactly where the n input sequence\n",
    "ended in the previous batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NfjFc2iT2vkx",
   "metadata": {
    "executionInfo": {
     "elapsed": 1007,
     "status": "ok",
     "timestamp": 1618277828151,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "NfjFc2iT2vkx"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "encoded_parts = np.array_split(encoded[:train_size], batch_size)\n",
    "datasets = []\n",
    "for encoded_part in encoded_parts:\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(encoded_part)\n",
    "    dataset = dataset.window(window_length, shift=n_steps, drop_remainder=True)\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_length))\n",
    "    datasets.append(dataset)\n",
    "dataset = tf.data.Dataset.zip(tuple(datasets)).map(lambda *windows: tf.stack(windows))\n",
    "dataset = dataset.repeat().map(lambda windows: (windows[:, :-1], windows[:, 1:]))\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TDnfJx-J0cdl",
   "metadata": {
    "id": "TDnfJx-J0cdl"
   },
   "source": [
    "Now let’s create the stateful RNN.\n",
    "- First, we need to set **stateful=True** when creating every recurrent layer.\n",
    "- Second, the stateful RNN needs to know the batch size (since it will preserve a state for each input sequence in the batch), so **we must set the batch_input_shape argument in the first layer.**\n",
    "- Note that **we can leave the second dimension unspecified, since the inputs could have any length:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oiUeYIyz0y-Z",
   "metadata": {
    "executionInfo": {
     "elapsed": 1121,
     "status": "ok",
     "timestamp": 1618277886014,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "oiUeYIyz0y-Z"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2,\n",
    "                     dropout=0.2,\n",
    "                     batch_input_shape=[batch_size, None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True, stateful=True,\n",
    "                     #dropout=0.2, recurrent_dropout=0.2),\n",
    "                     dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wybG3BO14Q7n",
   "metadata": {
    "id": "wybG3BO14Q7n"
   },
   "source": [
    "At the end of each epoch, we need to reset the states before we go back to\n",
    "the beginning of the text. For this, we can use a small callback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4sxXb1nu4ckt",
   "metadata": {
    "executionInfo": {
     "elapsed": 845,
     "status": "ok",
     "timestamp": 1618278019145,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "4sxXb1nu4ckt"
   },
   "outputs": [],
   "source": [
    "class ResetStatesCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_begin(self, epoch, logs):\n",
    "    self.model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gf5Mkl-44y0A",
   "metadata": {
    "id": "gf5Mkl-44y0A"
   },
   "source": [
    "And now we can compile and fit the model (for more epochs, because each epoch is much shorter than earlier, and there is only one instance per batch):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fNDcp40G9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215395,
     "status": "ok",
     "timestamp": 1618278374406,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "6b2fNDcp40G9",
    "outputId": "e7092efd-7796-4ccc-9da1-8afecc383d0d"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "steps_per_epoch = train_size // batch_size // n_steps\n",
    "history = model.fit(dataset, steps_per_epoch=steps_per_epoch, epochs=50,\n",
    "                    callbacks=[ResetStatesCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4PLq1S7L6WWB",
   "metadata": {
    "id": "4PLq1S7L6WWB"
   },
   "source": [
    "#### TIP\n",
    "- After this model is trained, it will only be possible to use it to make predictions for batches of the same size as were used during training. \n",
    "- To avoid this restriction, **create an identical stateless model, and copy the stateful model’s weights to this model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tvKXvoU56kNR",
   "metadata": {
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1618278571922,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "tvKXvoU56kNR"
   },
   "outputs": [],
   "source": [
    "stateless_model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None, max_id]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id,\n",
    "                                                    activation=\"softmax\"))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hXv_aDHe62Q7",
   "metadata": {
    "id": "hXv_aDHe62Q7"
   },
   "source": [
    "To set the weights, we first need to build the model (so the weights get created):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r-mkawMc63Uw",
   "metadata": {
    "executionInfo": {
     "elapsed": 980,
     "status": "ok",
     "timestamp": 1618278642738,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "r-mkawMc63Uw"
   },
   "outputs": [],
   "source": [
    "stateless_model.build(tf.TensorShape([None, None, max_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uULwv3FV7HwI",
   "metadata": {
    "executionInfo": {
     "elapsed": 797,
     "status": "ok",
     "timestamp": 1618278699487,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "uULwv3FV7HwI"
   },
   "outputs": [],
   "source": [
    "stateless_model.set_weights(model.get_weights())\n",
    "model = stateless_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVm8d6Bw7V7V",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1100,
     "status": "ok",
     "timestamp": 1618278712444,
     "user": {
      "displayName": "Batuhan Yılmaz",
      "photoUrl": "",
      "userId": "14419533306292507032"
     },
     "user_tz": -180
    },
    "id": "rVm8d6Bw7V7V",
    "outputId": "de032e03-bccb-43ee-ef99-4aacce171496"
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "print(complete_text(\"t\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "avlf9bUu7u2Y",
   "metadata": {
    "id": "avlf9bUu7u2Y"
   },
   "source": [
    "- Now that we have built a character-level model, it’s time to look at wordlevel\n",
    "models and tackle a common natural language processing task: **sentiment analysis.** \n",
    "- In the process we will learn how to handle sequences of variable lengths using masking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A35VDpdg8BmQ",
   "metadata": {
    "id": "A35VDpdg8BmQ"
   },
   "source": [
    "## Sentiment Analysis\n",
    "- If MNIST is the “hello world” of computer vision, then the **IMDb reviews dataset is the “hello world” of natural language processing: it consists of 50,000 movie reviews in English (25,000 for training, 25,000 for testing)** extracted from the famous Internet Movie Database, along with a simple binary target for each review indicating whether it is **negative (0)** or **positive (1)**.\n",
    "- Just like MNIST, the IMDb reviews dataset is popular for good reasons: it is simple enough to be tackled on a laptop in a reasonable amount of time, but challenging enough to be fun and rewarding.\n",
    "\n",
    "Keras provides a simple function to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "located-mount",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-ultimate",
   "metadata": {},
   "source": [
    "Where are the movie reviews? \n",
    "- Well, as you can see, the dataset is already preprocessed for you: X_train consists of a list of reviews, each of which is represented as a NumPy array of integers, **where each integer represents a word**.\n",
    "- All punctuation was removed, and then words were converted to lowercase, split by spaces, and finally indexed by frequency (so low integers correspond to frequent words).\n",
    "- **The integers 0, 1, and 2 are special: they represent the padding token, the start-of-sequence (SOS) token, and unknown words, respectively.**\n",
    "- If you want to visualize a review, you can decode it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = keras.datasets.imdb.get_word_index()\n",
    "id_to_word = {id_ + 3:word for word, id_ in word_index.items()}\n",
    "for id_, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "    id_to_word[id_] = token\n",
    "\" \".join([id_to_word[id_] for id_ in X_train[0][:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regular-curve",
   "metadata": {},
   "source": [
    "- In a real project, you will have to preprocess the text yourself. \n",
    "    - You can do that using the same **Tokenizer** class we used earlier, but this time setting **char_level=False**(which is the default).\n",
    "- When encoding words, it filters out a lot of characters, including most punctuation, line breaks, and tabs (but you can change this by setting the **filters** argument).\n",
    "- Most importantly, it uses spaces to identify word boundaries.\n",
    "    - This is OK for English and many other scripts (written languages) that use spaces between words, but not all scripts use spaces this way.\n",
    "    - Chinese does not use spaces between words, Vietnamese uses spaces even within words, and languages such as German often attach multiple words together, without spaces.\n",
    "    - Even in English, spaces are not always the best way to tokenize text: think of “San Francisco” or “#ILoveDeepLearning.”\n",
    "- Fortunately, there are better options! The 2018 paper by Taku Kudo introduced an unsupervised learning technique to tokenize and detokenize text at the subword level in a language-independent way, treating spaces like other characters.\n",
    "    - With this approach, even if your model encounters a word it has never seen before, it can still reasonably guess what it means.\n",
    "        - For example, it may never have seen the word “smartest” during training, but perhaps it learned the word “smart” and it also learned that the suffix “est” means “the most,” so it can infer the meaning of “smartest.”\n",
    "    - Google’s **SentencePiece** project provides an open source implementation, described in a paper by Taku Kudo and John Richardson.\n",
    "- Another option was proposed in an earlier paper by Rico Sennrich et al. that explored other ways of creating subword encodings (e.g., using byte pair encoding).\n",
    "- Last but not least, the TensorFlow team released the **TF.Text** library in June 2019, which implements various tokenization strategies, including WordPiece (a variant of byte pair encoding).\n",
    "***\n",
    "- If you want to deploy your model to a mobile device or a web browser, and you don’t want to have to write a different preprocessing function every time, then you will want to handle preprocessing using only TensorFlow operations, so it can be included in the model itself.\n",
    "\n",
    "Let’s see how.\n",
    "- First, let’s load the original IMDb reviews, as text (byte strings), using TensorFlow Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "regular-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets, info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "south-inflation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'unsupervised'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "medium-friendly",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = info.splits[\"train\"].num_examples\n",
    "test_size = info.splits[\"test\"].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "toxic-increase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 25000)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size, test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "level-evidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting  ...\n",
      "Label: 0 = Negative\n",
      "\n",
      "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However  ...\n",
      "Label: 0 = Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in datasets[\"train\"].batch(2).take(1):\n",
    "    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n",
    "        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n",
    "        print(\"Label:\", label, \"= Positive\" if label else \"= Negative\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "attached-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X_batch, y_batch):\n",
    "    X_batch = tf.strings.substr(X_batch, 0, 300)\n",
    "    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*/?>\", b\" \")\n",
    "    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n",
    "    X_batch = tf.strings.split(X_batch)\n",
    "    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "secondary-provincial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 53), dtype=string, numpy=\n",
       " array([[b'This', b'was', b'an', b'absolutely', b'terrible', b'movie',\n",
       "         b\"Don't\", b'be', b'lured', b'in', b'by', b'Christopher',\n",
       "         b'Walken', b'or', b'Michael', b'Ironside', b'Both', b'are',\n",
       "         b'great', b'actors', b'but', b'this', b'must', b'simply', b'be',\n",
       "         b'their', b'worst', b'role', b'in', b'history', b'Even',\n",
       "         b'their', b'great', b'acting', b'could', b'not', b'redeem',\n",
       "         b'this', b\"movie's\", b'ridiculous', b'storyline', b'This',\n",
       "         b'movie', b'is', b'an', b'early', b'nineties', b'US',\n",
       "         b'propaganda', b'pi', b'<pad>', b'<pad>', b'<pad>'],\n",
       "        [b'I', b'have', b'been', b'known', b'to', b'fall', b'asleep',\n",
       "         b'during', b'films', b'but', b'this', b'is', b'usually', b'due',\n",
       "         b'to', b'a', b'combination', b'of', b'things', b'including',\n",
       "         b'really', b'tired', b'being', b'warm', b'and', b'comfortable',\n",
       "         b'on', b'the', b'sette', b'and', b'having', b'just', b'eaten',\n",
       "         b'a', b'lot', b'However', b'on', b'this', b'occasion', b'I',\n",
       "         b'fell', b'asleep', b'because', b'the', b'film', b'was',\n",
       "         b'rubbish', b'The', b'plot', b'development', b'was', b'constant',\n",
       "         b'Cons']], dtype=object)>,\n",
       " <tf.Tensor: shape=(2,), dtype=int64, numpy=array([0, 0], dtype=int64)>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustained-detector",
   "metadata": {},
   "source": [
    "- It starts by truncating the reviews, **keeping only the first 300 characters of each:** this will speed up training, and it won’t impact performance too much because you can generally tell whether a review is positive or not in the first sentence or two.\n",
    "- Then it uses regular expressions to replace <br /> tags with spaces, and to replace any characters other than letters and quotes with spaces.\n",
    "    - **For example, the text \"Well, I can't< br />\" will become \"Well I can't\".**\n",
    "- Finally, the preprocess() function splits the reviews by the spaces, which returns a ragged tensor, and it converts this ragged tensor to a dense tensor, padding all reviews with the padding token \"< pad >\" so that they all have the same length.\n",
    "\n",
    "Next, we need to construct the vocabulary.\n",
    "- This requires going through the whole training set once, applying our preprocess() function, and using a Counter to count the number of occurrences of each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "neutral-steps",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocabulary = Counter()\n",
    "for X_batch, y_batch in datasets[\"train\"].batch(32).map(preprocess):\n",
    "    for review in X_batch:\n",
    "        vocabulary.update(list(review.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-communist",
   "metadata": {},
   "source": [
    "Let’s look at the three most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "periodic-jewelry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b'<pad>', 214309), (b'the', 61137), (b'a', 38564)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary.most_common()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "joint-dispute",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53893"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-cycle",
   "metadata": {},
   "source": [
    "Great! **We probably don’t need our model to know all the words in the dictionary to get good performance**, though, so let’s truncate the vocabulary, keeping only the 10,000 most common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aggressive-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "truncated_vocabulary = [\n",
    "    word for word, count in vocabulary.most_common()[:vocab_size]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "negative-diagram",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "12\n",
      "11\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}\n",
    "for word in b\"This movie was faaaaaantastic\".split():\n",
    "    print(word_to_id.get(word) or vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-minnesota",
   "metadata": {},
   "source": [
    "**Now we need to add a preprocessing step to replace each word with its ID (i.e., its index in the vocabulary).**\n",
    "- We will create a lookup table for this, using 1,000 out-of-vocabulary (oov) buckets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "senior-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.constant(truncated_vocabulary)\n",
    "word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\n",
    "vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\n",
    "num_oov_buckets = 1000\n",
    "table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-cabin",
   "metadata": {},
   "source": [
    "We can then use this table to look up the IDs of a few words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "exclusive-walker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4), dtype=int64, numpy=array([[   22,    12,    11, 10053]], dtype=int64)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.lookup(tf.constant([b\"This movie was faaaaaantastic\".split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-samoa",
   "metadata": {},
   "source": [
    "Note that **the words “this,” “movie,” and “was” were found in the table, so their IDs are lower than 10,000**, while the word **“faaaaaantastic” was not found, so it was mapped to one of the oov buckets, with an ID greater than or equal to 10,000.**\n",
    "#### TIP\n",
    "TF Transform provides some useful functions to handle such vocabularies. \n",
    "- For example, check out the **tft.compute_and_apply_vocabulary()** function: **it will go through the dataset to find all distinct words and build the vocabulary, and it will generate the TF operations required to encode each word using this vocabulary.**\n",
    "***\n",
    "- Now we are ready to create the final training set.\n",
    "- **We batch the reviews, then convert them to short sequences of words using the preprocess() function, then encode these words using a simple encode_words() function that uses the table we just built, and finally prefetch the next batch:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "marked-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_words(X_batch, y_batch):\n",
    "    return table.lookup(X_batch), y_batch\n",
    "\n",
    "train_set = datasets[\"train\"].repeat().batch(32).map(preprocess)\n",
    "train_set = train_set.map(encode_words).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "historical-version",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  22   11   28 ...    0    0    0]\n",
      " [   6   21   70 ...    0    0    0]\n",
      " [4099 6881    1 ...    0    0    0]\n",
      " ...\n",
      " [  22   12  118 ...  331 1047    0]\n",
      " [1757 4101  451 ...    0    0    0]\n",
      " [3365 4392    6 ...    0    0    0]], shape=(32, 60), dtype=int64)\n",
      "tf.Tensor([0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 0 0 0], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch)\n",
    "    print(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inclusive-quantity",
   "metadata": {},
   "source": [
    "At last we can create the model and train it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "informed-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 23/781 [..............................] - ETA: 2:17 - loss: 0.6933 - accuracy: 0.4911"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-f2ca9d0f8cc3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"binary_crossentropy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"adam\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 2943\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2945\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1919\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 560\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embed_size = 128\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n",
    "                           mask_zero=True,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.GRU(128, return_sequences=True),\n",
    "    keras.layers.GRU(128),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banner-attraction",
   "metadata": {},
   "source": [
    "- The **first layer is an Embedding layer**, which will **convert word IDs into embeddings**.\n",
    "    - **The embedding matrix needs to have one row per word ID (vocab_size + num_oov_buckets) and one column per embedding dimension** (this example uses 128 dimensions, but this is a hyperparameter you could tune).\n",
    "    - *Whereas the inputs of the model will be 2D tensors of shape [batch size, time steps], the output of the Embedding layer will be a 3D tensor of shape [batch size, time steps, embedding size].**\n",
    "- The rest of the model is fairly straightforward: it is composed of two GRU layers, with the second one returning only the output of the last time step.\n",
    "- **The output layer is just a single neuron using the sigmoid activation function to output the estimated probability that the review expresses a positive sentiment regarding the movie.**\n",
    "- We then compile the model quite simply, and we fit it on the dataset we prepared earlier, for a few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-disco",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Embeddings\n",
    "- The TensorFlow Hub project makes it easy to reuse pretrained model components in your own models.\n",
    "    - These model components are called **modules**.\n",
    "- Simply browse the TF Hub repository, find the one you need, and copy the code example into your project, and the module will be automatically downloaded, along with its pretrained weights, and included in your model. Easy!\n",
    "- For example, let’s use the **nnlm-en-dim50** sentence embedding module, version 1, in our sentiment analysis model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-perth",
   "metadata": {},
   "outputs": [],
   "source": [
    "TFHUB_CACHE_DIR = os.path.join(os.curdir, \"my_tfhub_cache\")\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = TFHUB_CACHE_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "animated-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thermal-genealogy",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1\",\n",
    "                   dtype=tf.string, input_shape=[], output_shape=[50]),\n",
    "    keras.layers.Dense(128, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirpath, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-vacation",
   "metadata": {},
   "source": [
    "- **The hub.KerasLayer layer downloads the module from the given URL.**\n",
    "    - This particular module is a **sentence encoder**: **it takes strings as input and encodes each one as a single vector (in this case, a 50-dimensional vector).**\n",
    "    - Internally, **it parses the string (splitting words on spaces) and embeds each word using an embedding matrix that was pretrained on a huge corpus**: the Google News 7B corpus (seven billion words long!).\n",
    "    - **Then it computes the mean of all the word embeddings, and the result is the sentence embedding.**\n",
    "- We can then add two simple Dense layers to create a good sentiment analysis model.\n",
    "- **By default, a hub.KerasLayer is not trainable**, but you can set trainable=True when creating it to change that so that you can fine-tune it for your task.\n",
    "\n",
    "Next, we can just load the IMDb reviews dataset,  no need to preprocess it (except for batching and prefetching) and directly train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "datasets ,info = tfds.load(\"imdb_reviews\", as_supervised=True, with_info=True)\n",
    "train_size = info.splits[\"train\"].num_examples\n",
    "batch_size = 32\n",
    "train_set = datasets[\"train\"].repeat().batch(batch_size).prefetch(1)\n",
    "history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-blackjack",
   "metadata": {},
   "source": [
    "- Note that the last part of the TF Hub module URL specified that we wanted version 1 of the model. \n",
    "    - This versioning ensures that if a new module version is released, it will not break our model.\n",
    "    - Conveniently, if you just enter this URL in a web browser, you will get the documentation for this module.\n",
    "- By default, TF Hub will cache the downloaded files into the local system’s temporary directory.\n",
    "    - You may prefer to download them into a more permanent directory to avoid having to download them again after every system cleanup.\n",
    "        - To do that, set the TFHUB_CACHE_DIR environment variable to the directory of your choice (e.g., os.environ[\"TFHUB_CACHE_DIR\"] = \"./my_tfhub_cache\").\n",
    "\n",
    "So far, we have looked at time series, text generation using Char-RNN, and sentiment analysis using word-level RNN models, training our own word embeddings or reusing pretrained embeddings.\n",
    "- Let’s now look at another important NLP task: **neural machine translation(NMT)**, first using a pure Encoder–Decoder model, then improving it with attention mechanisms, and finally looking the extraordinary Transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-capitol",
   "metadata": {},
   "source": [
    "## An Encoder–Decoder Network for Neural Machine Translation\n",
    "Let’s take a look at a simple neural machine translation model that will translate English sentences to French (see Figure 16-3).\n",
    "- In short, **the English sentences are fed to the encoder, and the decoder outputs the French translations.** \n",
    "    - Note that the **French translations are also used as inputs to the decoder, but shifted back by one step.**\n",
    "- In other words, **the decoder is given as input the word that it should have output at the previous step** (regardless of what it actually output).\n",
    "- **For the very first word,it is given the start-of-sequence (SOS) token.**\n",
    "- **The decoder is expected to end the sentence with an end-of-sequence (EOS) token.**\n",
    "- Note that the **English sentences are reversed before they are fed to the encoder.** \n",
    "    - For example, “I drink milk” is reversed to “milk drink I.”\n",
    "    - **This ensures that the beginning of the English sentence will be fed last to the encoder, which is useful because that’s generally the first thing that the decoder needs to translate.**\n",
    "- **Each word is initially represented by its ID** (e.g., 288 for the word “milk”).\n",
    "- Next, an **embedding layer** returns the word embedding.\n",
    "    - **These word embeddings are what is actually fed to the encoder and the decoder.**\n",
    "\n",
    "<img src=\"16-3.png\">\n",
    "\n",
    "- **At each step, the decoder outputs a score for each word in the output vocabulary (i.e., French), and then the softmax layer turns these scores into probabilities.**\n",
    "    - For example, at the first step the word “Je” may have a probability of 20%, “Tu” may have a probability of 1%, and so on.\n",
    "- **The word with the highest probability is output.**\n",
    "- This is very much like a regular classification task, so you can train the model using the **\"sparse_categorical_crossentropy\" loss**, much like we did in the Char-RNN model.\n",
    "- Note that **at inference time (after training), you will not have the target sentence to feed to the decoder.**\n",
    "    - **Instead, simply feed the decoder the word that it output at the previous step**, as shown in Figure 16-4 (this will require an embedding lookup that is not shown in the diagram).\n",
    "\n",
    "<img src=\"16-4.png\">\n",
    "\n",
    "OK, now you have the big picture. Still, there are a few more details to handle if you implement this model:\n",
    "- So far we have assumed that all input sequences (to the encoder and to the decoder) have a constant length. But obviously sentence lengths vary.\n",
    "    - Since regular tensors have fixed shapes, **they can only contain sentences of the same length.**\n",
    "        - **You can use masking to handle this**, as discussed earlier.\n",
    "    - **However, if the sentences have very different lengths, you can’t just crop them like we did for sentiment analysis** (because we want full translations, not cropped translations).\n",
    "    - **Instead, group sentences into buckets of similar lengths** (e.g., a bucket for the 1- to 6-word sentences, another for the 7- to 12-word sentences, and so on), using padding for the shorter sequences to ensure all sentences in a bucket have the same length.\n",
    "        - For example, “I drink milk” becomes “< pad > < pad > < pad > milk drink I.”\n",
    "- **We want to ignore any output past the EOS token, so these tokens should not contribute to the loss (they must be masked out).**\n",
    "    - For example, if the model outputs “Je bois du lait < eos > oui,” the loss for the last word should be ignored.\n",
    "- **When the output vocabulary is large (which is the case here), outputting a probability for each and every possible word would be terribly slow.**\n",
    "    - If the target vocabulary contains, say, 50,000 French words, then the decoder would output 50,000-dimensional vectors, and then computing the softmax function over such a large vector would be very computationally intensive.\n",
    "    - **To avoid this, one solution is to look only at the logits output by the model for the correct word and for a random sample of incorrect words, then compute an approximation of the loss based only on these logits.**\n",
    "        - This **sampled softmax technique** was introduced in 2015 by Sébastien Jean et al.\n",
    "    - In TensorFlow you can use the **tf.nn.sampled_softmax_loss() function for this during training** and use **the normal softmax function at inference time** (sampled softmax cannot be used at inference time because it requires knowing the target).\n",
    "    \n",
    "***\n",
    "- The **TensorFlow Addons** project includes many sequence-to-sequence tools to let you easily build production-ready Encoder–Decoders.\n",
    "    - For example, the following code creates a basic Encoder–Decoder model, similar to the one represented in Figure 16-3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "digital-framework",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "tested-remainder",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "daily-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(512, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(512)\n",
    "output_layer = keras.layers.Dense(vocab_size)\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lenghts = decoder(\n",
    "    decoder_embeddings, initial_state=encoder_state,\n",
    "    sequence_length=sequence_lengths)\n",
    "Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(\n",
    "    inputs=[encoder_inputs, decoder_inputs, sequence_lengths],\n",
    "    outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "spanish-plain",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "viral-smile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 [==============================] - 15s 268ms/step - loss: 4.6050\n",
      "Epoch 2/2\n",
      "32/32 [==============================] - 9s 294ms/step - loss: 4.6035\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randint(100, size=10*1000).reshape(1000, 10)\n",
    "Y = np.random.randint(100, size=15*1000).reshape(1000, 15)\n",
    "X_decoder = np.c_[np.zeros((1000, 1)), Y[:, :-1]]\n",
    "seq_lengths = np.full([1000], 15)\n",
    "\n",
    "history = model.fit([X, X_decoder, seq_lengths], Y, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nearby-hollywood",
   "metadata": {},
   "source": [
    "The code is mostly self-explanatory, but there are a few points to note.\n",
    "- First, we set **return_state=True when creating the LSTM layer so that we can get its final hidden state and pass it to the decoder.**\n",
    "    - Since we are using an LSTM cell, it actually returns two hidden states (short term and long term).\n",
    "- The **TrainingSampler** is one of several samplers available in TensorFlow Addons: **their role is to tell the decoder at each step what it should pretend the previous output was.**\n",
    "    - **During inference**, this should be the embedding of the token that was actually output.\n",
    "    - **During training**, it should be the embedding of the previous target token: this is why we used the TrainingSampler.\n",
    "- **In practice, it is often a good idea to start training with the embedding of the target of the previous time step and gradually transition to using the embedding of the actual token that was output at the previous step.**\n",
    "    - This idea was introduced in a 2015 paper by Samy Bengio et al.\n",
    "- **The ScheduledEmbeddingTrainingSampler will randomly choose between the target or the actual output, with a probability that you can gradually change during training.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-license",
   "metadata": {},
   "source": [
    "## Bidirectional RNNs\n",
    "- **At each time step, a regular recurrent layer only looks at past and present inputs before generating its output.** \n",
    "    - In other words, it is **“causal”**, meaning **it cannot look into the future.**\n",
    "- This type of RNN makes sense when forecasting time series, but for many NLP tasks, such as Neural Machine Translation, it is often preferable to look ahead at the next words before encoding a given word.\n",
    "    - For example, consider the phrases “the Queen of the United Kingdom,” “the queen of hearts,” and “the queen bee”: to properly encode the word “queen,” you need to look ahead.\n",
    "- To implement this, **run two recurrent layers on the same inputs, one reading the words from left to right and the other reading them from right to left.**\n",
    "- **Then simply combine their outputs at each time step, typically by concatenating them.**\n",
    "    - This is called a **bidirectional recurrent layer.**\n",
    "- To implement a bidirectional recurrent layer in Keras, wrap a recurrent layer in a keras.layers.Bidirectional layer.\n",
    "\n",
    "For example, the following code creates a bidirectional GRU layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(10, return_sequences=True, input_shape=[None, 10]),\n",
    "    keras.layers.Bidirectional(keras.layers.GRU(10, return_sequences=True))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adjustable-subsection",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "- **The Bidirectional layer will create a clone of the GRU layer (but in the reverse direction), and it will run both and concatenate their outputs.** \n",
    "    - So although the GRU layer has 10 units, the Bidirectional layer will output 20 values per time step.\n",
    "\n",
    "<img src=\"16-5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-divorce",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "- Suppose you train an Encoder–Decoder model, and use it to translate the French sentence “Comment vas-tu?” to English. \n",
    "- You are hoping that it will output the proper translation (“How are you?”), but unfortunately it outputs “How will you?” Looking at the training set, you notice many sentences such as “Comment vas-tu jouer?” which translates to “How will you play?” So it wasn’t absurd for the model to output “How will” after seeing “Comment vas.” \n",
    "    - Unfortunately, in this case it was a mistake, and the model could not go back and fix it, so it tried to complete the sentence as best it could. \n",
    "    - By greedily outputting the most likely word at every step, it ended up with a suboptimal translation.\n",
    "- How can we give the model a chance to go back and fix mistakes it made earlier?\n",
    "- One of the most common solutions is **beam search**: it keeps track of a short list of the k most promising sentences (say, the top three), and at each decoder step it tries to extend them by one word, keeping only the k most likely sentences. \n",
    "    - The parameter k is called the **beam width**.\n",
    "\n",
    "For example, suppose you use the model to translate the sentence “Comment vas-tu?” using beam search with a beam width of 3.\n",
    "- At the first decoder step, the model will output an estimated probability for each possible word. \n",
    "    - Suppose the top three words are “How” (75% estimated probability), “What” (3%), and “You” (1%). That’s our short list so far.\n",
    "- Next, we create three copies of our model and use them to find the next word for each sentence.\n",
    "    - Each model will output one estimated probability per word in the vocabulary.\n",
    "    - The first model will try to find the next word in the sentence “How,” and perhaps it will output a probability of 36% for the word “will,” 32% for the word “are,” 16% for the word “do,” and so on.\n",
    "        - Note that these are actually **conditional probabilities**, given that the sentence starts with “How.”\n",
    "    - The second model will try to complete the sentence “What”; it might output a conditional probability of 50% for the word “are,” and so on. \n",
    "        - Assuming the vocabulary has 10,000 words, each model will output 10,000 probabilities.\n",
    "    - Next, we compute the probabilities of each of the 30,000 two-word sentences that these models considered (3 × 10,000). \n",
    "        - We do this by multiplying the estimated conditional probability of each word by the estimated probability of the sentence it completes.\n",
    "- For example, the estimated probability of the sentence “How” was 75%, while the estimated conditional probability of the word “will” (given that the first word is “How”) was 36%, so the estimated probability of the sentence “How will” is 75% × 36% = 27%.\n",
    "- After computing the probabilities of all 30,000 two word sentences, we keep only the top 3.\n",
    "    - Perhaps they all start with the word “How”: “How will” (27%), “How are” (24%), and “How do” (12%).\n",
    "    - Right now, the sentence “How will” is winning, but “How are” has not been eliminated.\n",
    "- Then we repeat the same process: we use three models to predict the next word in each of these three sentences, and we compute the probabilities of all 30,000 three-word sentences we considered.\n",
    "- Perhaps the top three are now “How are you” (10%), “How do you” (8%), and “How will you” (2%). \n",
    "    - At the next step we may get “How do you do” (7%), “How are you < eos >” (6%), and “How are you doing” (3%).\n",
    "        - Notice that “How will” was eliminated, and we now have three perfectly reasonable translations.\n",
    "- We boosted our Encoder–Decoder model’s performance without any extra training, simply by using it more wisely.\n",
    "\n",
    "You can implement beam search fairly easily using TensorFlow Addons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "large-wales",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "beam_width = 10\n",
    "decoder = tfa.seq2seq.beam_search_decoder.BeamSearchDecoder(\n",
    "    cell=decoder_cell, beam_width=beam_width, output_layer=output_layer)\n",
    "decoder_initial_state = tfa.seq2seq.beam_search_decoder.tile_batch(\n",
    "    encoder_state, multiplier=beam_width)\n",
    "outputs, _, _ = decoder(\n",
    "    embedding_decoder, start_tokens=start_tokens, end_token=end_token,\n",
    "    initial_state=decoder_initial_state)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atmospheric-retirement",
   "metadata": {},
   "source": [
    "- We first create a BeamSearchDecoder, which wraps all the decoder clones (in this case 10 clones). \n",
    "    - Then we create one copy of the encoder’s final state for each decoder clone, and we pass these states to the decoder, along with the start and end tokens.\n",
    "- With all this, you can get good translations for fairly short sentences (especially if you use pretrained word embeddings).\n",
    "    - Unfortunately, this model will be really bad at translating long sentences.\n",
    "- Once again, the problem comes from the limited short-term memory of RNNs.\n",
    "- **Attention mechanisms** are the game-changing innovation that addressed this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-optimum",
   "metadata": {},
   "source": [
    "## Attention Mechanisms\n",
    "Consider the path from the word “milk” to its translation “lait” in Figure 16-3: it is quite long! This means that a representation of this word (along with all the other words) needs to be carried over many steps before it is actually used. Can’t we make this path shorter?\n",
    "- This was the core idea in a groundbreaking 2014 paper by Dzmitry Bahdanau et al. \n",
    "- **They introduced a technique that allowed the decoder to focus on the appropriate words (as encoded by the encoder) at each time step.**\n",
    "    - For example, at the time step where the decoder needs to output the word “lait,” it will focus its attention on the word “milk.”\n",
    "        - **This means that the path from an input word to its translation is now much shorter**, so the short-term memory limitations of RNNs have much less impact.\n",
    "- **Attention mechanisms revolutionized neural machine translation (and NLP in general), allowing a significant improvement in the state of the art, especially for long sentences (over 30 words).**\n",
    "\n",
    "Figure 16-6 shows this model’s architecture (slightly simplified, as we will see).\n",
    "- On the left, you have the encoder and the decoder. \n",
    "    - **Instead of just sending the encoder’s final hidden state to the decoder** (which is still done, although it is not shown in the figure), **we now send all of its outputs to the decoder.**\n",
    "- At each time step, the decoder’s memory cell computes a weighted sum of all these encoder outputs: this determines which words it will focus on at this step.\n",
    "- The weight α is the weight of the ith encoder output at the ti, decoder time step.\n",
    "    - For example, **if the weight α(3,2) is much larger than the weights α(3,0) and α(3,1), then the decoder will pay much more attention to word number 2 (“milk”) than to the other two words, at least at this time step.**\n",
    "- The rest of the decoder works just like earlier: at each time step the memory cell receives the inputs we just discussed, plus the hidden state from the previous time step, and finally (although it is not represented in the diagram) it receives the target word from the previous time step (or at inference time, the output from the previous time step).\n",
    "<img src=\"16-6.png\">\n",
    "\n",
    "- But where do these α weights come from? It’s actually pretty simple: they are generated by a type of small neural network called an **alignment model (or an attention layer)**, which is trained jointly with the rest of the Encoder–Decoder model.\n",
    "    - This alignment model is illustrated on the righthand side of Figure 16-6.\n",
    "- **It starts with a time-distributed Dense layer with a single neuron, which receives as input all the encoder outputs, concatenated with the decoder’s previous hidden state** (e.g., h(2)).\n",
    "    - This layer outputs a score (or energy) for each encoder output (e.g., e(3,2)): this score measures how well each output is aligned with the decoder’s previous hidden state.\n",
    "- **Finally, all the scores go through a softmax layer to get a final weight for each encoder output**(e.g., α(3,2)).\n",
    "    - All the weights for a given decoder time step add up to 1 (since the softmax layer is not time-distributed).\n",
    "- This particular attention mechanism is called **Bahdanau attention**(named after the paper’s first author).\n",
    "    - Since it concatenates the encoder output with the decoder’s previous hidden state, it is sometimes called **concatenative attention (or additive attention)**.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-instrument",
   "metadata": {},
   "source": [
    "Another common attention mechanism was proposed shortly after, in a 2015 paper by Minh-Thang Luong et al.\n",
    "- Because the goal of the attention mechanism is to measure the similarity between one of the encoder’s outputs and the decoder’s previous hidden state, **the authors proposed to simply compute the dot product of these two vectors**, as this is often a fairly good similarity measure, and modern hardware can compute it much faster.\n",
    "    - For this to be possible, both vectors must have the same dimensionality.\n",
    "- This is called **Luong attention**(again, after the paper’s first author), or sometimes **multiplicative attention**.\n",
    "- **The dot product gives a score, and all the scores (at a given decoder time step) go through a softmax layer to give the final weights**, just like in Bahdanau attention.\n",
    "- Another simplification they proposed was to **use the decoder’s hidden state at the current time step rather than at the previous time step** (i.e., h(t)) rather than h(t-1)), **then to use the output of the attention mechanism (noted ˜h(t)) directly to compute the decoder’s predictions**(rather than using it to compute the decoder’s current hidden state).\n",
    "- They also proposed a variant of the dot product mechanism where **the encoder outputs first go through a linear transformation (i.e., a time-distributed Dense layer without a bias term) before the dot products are computed.**\n",
    "    - This is called **the “general” dot product approach.**\n",
    "- They compared both dot product approaches to the concatenative attention mechanism (adding a rescaling parameter vector v), and **they observed that the dot product variants performed better than concatenative attention.**\n",
    "    - **For this reason, concatenative attention is much less used now.**\n",
    "\n",
    "Here is how you can add Luong attention to an Encoder–Decoder model using TensorFlow Addons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "attention_mechanism = tfa.seq2seq.attention_wrapper.LuongAttention(\n",
    "    units, encoder_state, memory_sequence_length=encoder_sequence_length)\n",
    "attention_decoder_cell = tfa.seq2seq.attention_wrapper.AttentionWrapper(\n",
    "    decoder_cell, attention_mechanism, attention_layer_size=n_units)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-casting",
   "metadata": {},
   "source": [
    "We simply wrap the decoder cell in an AttentionWrapper, and we provide the desired attention mechanism (Luong attention in this example)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-satin",
   "metadata": {},
   "source": [
    "### Visual Attention\n",
    "Attention mechanisms are now used for a variety of purposes. \n",
    "- One of their first applications beyond NMT was in generating image captions using visual attention: a convolutional neural network first processes the image and outputs some feature maps, then a decoder RNN equipped with an attention mechanism generates the caption, one word at a time. \n",
    "    - At each decoder time step (each word), the decoder uses the attention model to focus on just the right part of the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-friendship",
   "metadata": {},
   "source": [
    "## Attention Is All You Need: The Transformer Architecture\n",
    "- In a groundbreaking 2017 paper, a team of Google researchers suggested that **“Attention Is All You Need.”**\n",
    "- They managed to create an architecture called **the Transformer**, which significantly improved the state of the art in NMT **without using any recurrent or convolutional layers, just attention mechanisms (plus embedding layers, dense layers, normalization layers, and a few other bits and pieces).**\n",
    "    - As an extra bonus, **this architecture was also much faster to train and easier to parallelize**, so they managed to train it at a fraction of the time and cost of the previous state-of-the-art models.\n",
    "- The Transformer architecture is represented below.\n",
    "\n",
    "<img src=\"16-8.png\">\n",
    "\n",
    "Let’s walk through this figure:\n",
    "- The lefthand part is the **encoder**. \n",
    "    - Just like earlier, **it takes as input a batch of sentences represented as sequences of word IDs** (the input shape is [batch size, max input sentence length]), and it encodes each word into a 512-dimensional representation (so the encoder’s output shape is [batch size, max input sentence length, 512]). \n",
    "    - Note that the top part of the encoder is stacked N times (in the paper, N = 6).\n",
    "- The righthand part is the **decoder**. \n",
    "    - **During training, it takes the target sentence as input** (also represented as a sequence of word IDs), shifted one time step to the right (i.e., a start-of-sequence token is inserted at the beginning).\n",
    "    - **It also receives the outputs of the encoder** (i.e., the arrows coming from the left side).\n",
    "    - Note that the top part of the decoder is also stacked N times, and the encoder stack’s final outputs are fed to the decoder at each of these N levels.\n",
    "    - Just like earlier, the decoder outputs a probability for each possible next word, at each time step (its output shape is [batch size, max output sentence length, vocabulary length]).\n",
    "- During inference, the decoder cannot be fed targets, so we feed it the previously output words (starting with a start-of-sequence token).\n",
    "    - **So the model needs to be called repeatedly, predicting one more word at every round** (which is fed to the decoder at the next round, until the end-of-sequence token is output).\n",
    "- Looking more closely, you can see that you are already familiar with most components: there are **two embedding layers, 5 × N skip connections, each of them followed by a layer normalization layer, 2 × N “Feed Forward” modules that are composed of two dense layers each (the first one using the ReLU activation function, the second with no activation function), and finally the output layer is a dense layer using the softmax activation function.**\n",
    "    - **All of these layers are time-distributed, so each word is treated independently of all the others.**\n",
    "\n",
    "But how can we translate a sentence by only looking at one word at a time? Well, that’s where the new components come in:\n",
    "- The encoder’s **Multi-Head Attention layer** encodes each word’s relationship with every other word in the same sentence, paying more attention to the most relevant ones.\n",
    "    - For example, the output of this layer for the word “Queen” in the sentence “They welcomed the Queen of the United Kingdom” will depend on all the words in the sentence, but it will probably pay more attention to the words “United” and “Kingdom” than to the words “They” or “welcomed.”\n",
    "        - This attention mechanism is called **self attention** (the sentence is paying attention to itself). We will discuss exactly how it works shortly.\n",
    "    - **The decoder’s Masked Multi-Head Attention layer does the same thing, but each word is only allowed to attend to words located before it.**\n",
    "    - Finally, the decoder’s upper Multi-Head Attention layer is where the decoder pays attention to the words in the input sentence.\n",
    "        - For example, the decoder will probably pay close attention to the word “Queen” in the input sentence when it is about to output this word’s translation\n",
    "- The **positional embeddings** are simply dense vectors (much like word embeddings) that represent the position of a word in the sentence.\n",
    "    - The n positional embedding is added to the word embedding of the $n^{th}$ word in each sentence.\n",
    "    - This gives the model access to each word’s position, which is needed because **the Multi-Head Attention layers do not consider the order or the position of the words; they only look at their relationships.**\n",
    "    - Since all the other layers are time-distributed, they have no way of knowing the position of each word (either relative or absolute).\n",
    "        - Obviously, the relative and absolute word positions are important, so we need to give this information to the Transformer somehow, and **positional embeddings** are a good way to do this.\n",
    "\n",
    "Let’s look a bit closer at both these novel components of the Transformer architecture, starting with the positional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-london",
   "metadata": {},
   "source": [
    "## Positional embeddings\n",
    "- A positional embedding is a dense vector that encodes the position of a word within a sentence: the $i^{th}$ positional embedding is simply added to the word embedding of the $i^{th}$ word in the sentence.\n",
    "    - These positional embeddings can be learned by the model, but in the paper the authors preferred to use fixed positional embeddings, defined using the sine and cosine functions of different frequencies.\n",
    "- There is no PositionalEmbedding layer in TensorFlow, but it is easy to create one. \n",
    "    - For efficiency reasons, we precompute the positional embedding matrix in the constructor (so we need to know the maximum sentence length, max_steps, and the number of dimensions for each word representation, max_dims).\n",
    "    - Then the call() method crops this embedding matrix to the size of the inputs, and it adds it to the inputs.\n",
    "    - Since we added an extra first dimension of size 1 when creating the positional embedding matrix, the rules of broadcasting will ensure that the matrix gets added to every sentence in the inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(keras.layers.Layer):\n",
    "    def __init__(self, max_steps, max_dims, dtype=tf.float32, **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        if max_dims % 2 == 1: max_dims += 1 # max_dims must be even\n",
    "        p, i = np.meshgrid(np.arange(max_steps), np.arange(max_dims // 2))\n",
    "        pos_emb = np.empty((1, max_steps, max_dims))\n",
    "        pos_emb[0, :, ::2] = np.sin(p / 10000**(2 * i / max_dims)).T\n",
    "        pos_emb[0, :, 1::2] = np.cos(p / 10000**(2 * i / max_dims)).T\n",
    "        self.positional_embedding = tf.constant(pos_emb.astype(self.dtype))\n",
    "    def call(self, inputs):\n",
    "        shape = tf.shape(inputs)\n",
    "        return inputs + self.positional_embedding[:, :shape[-2], :shape[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-hopkins",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 201\n",
    "max_dims = 512\n",
    "pos_emb = PositionalEncoding(max_steps, max_dims)\n",
    "PE = pos_emb(np.zeros((1, max_steps, max_dims), np.float32))[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1, i2, crop_i = 100, 101, 150\n",
    "p1, p2, p3 = 22, 60, 35\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(9, 5))\n",
    "ax1.plot([p1, p1], [-1, 1], \"k--\", label=\"$p = {}$\".format(p1))\n",
    "ax1.plot([p2, p2], [-1, 1], \"k--\", label=\"$p = {}$\".format(p2), alpha=0.5)\n",
    "ax1.plot(p3, PE[p3, i1], \"bx\", label=\"$p = {}$\".format(p3))\n",
    "ax1.plot(PE[:,i1], \"b-\", label=\"$i = {}$\".format(i1))\n",
    "ax1.plot(PE[:,i2], \"r-\", label=\"$i = {}$\".format(i2))\n",
    "ax1.plot([p1, p2], [PE[p1, i1], PE[p2, i1]], \"bo\")\n",
    "ax1.plot([p1, p2], [PE[p1, i2], PE[p2, i2]], \"ro\")\n",
    "ax1.legend(loc=\"center right\", fontsize=14, framealpha=0.95)\n",
    "ax1.set_ylabel(\"$P_{(p,i)}$\", rotation=0, fontsize=16)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.hlines(0, 0, max_steps - 1, color=\"k\", linewidth=1, alpha=0.3)\n",
    "ax1.axis([0, max_steps - 1, -1, 1])\n",
    "ax2.imshow(PE.T[:crop_i], cmap=\"gray\", interpolation=\"bilinear\", aspect=\"auto\")\n",
    "ax2.hlines(i1, 0, max_steps - 1, color=\"b\")\n",
    "cheat = 2 # need to raise the red line a bit, or else it hides the blue one\n",
    "ax2.hlines(i2+cheat, 0, max_steps - 1, color=\"r\")\n",
    "ax2.plot([p1, p1], [0, crop_i], \"k--\")\n",
    "ax2.plot([p2, p2], [0, crop_i], \"k--\", alpha=0.5)\n",
    "ax2.plot([p1, p2], [i2+cheat, i2+cheat], \"ro\")\n",
    "ax2.plot([p1, p2], [i1, i1], \"bo\")\n",
    "ax2.axis([0, max_steps - 1, 0, crop_i])\n",
    "ax2.set_xlabel(\"$p$\", fontsize=16)\n",
    "ax2.set_ylabel(\"$i$\", rotation=0, fontsize=16)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-copyright",
   "metadata": {},
   "source": [
    "#### Sine/cosine positional embedding matrix (transposed, top) with a focus on two values of i (bottom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-edition",
   "metadata": {},
   "source": [
    "- This solution gives the same performance as learned positional embeddings do, **but it can extend to arbitrarily long sentences**, which is why it’s favored.\n",
    "- After the positional embeddings are added to the word embeddings, the rest of the model has access to the absolute position of each word in the sentence because there is a unique positional embedding for each position (e.g., the positional embedding for the word located at the 22nd position in a sentence is represented by the vertical dashed line at the bottom left of the plot above, and you can see that it is unique to that position).\n",
    "- Moreover, the choice of oscillating functions (sine and cosine) makes it possible for the model to learn relative positions as well.\n",
    "    - For example, words located 38 words apart (e.g., at positions p = 22 and p = 60) always have the same positional embedding values in the embedding dimensions i = 100 and i = 101, as you can see in the plots above.\n",
    "- This explains why we need both the sine and the cosine for each frequency: if we only used the sine (the blue wave at i = 100), the model would not be able to distinguish positions p = 25 and p = 35 (marked by a cross).\n",
    "\n",
    "**Then we can create the first layers of the Transformer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 512; max_steps = 500; vocab_size = 10000\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "embeddings = keras.layers.Embedding(vocab_size, embed_size)\n",
    "encoder_embeddings = embeddings(encoder_inputs)\n",
    "decoder_embeddings = embeddings(decoder_inputs)\n",
    "positional_encoding = PositionalEncoding(max_steps, max_dims=embed_size)\n",
    "encoder_in = positional_encoding(encoder_embeddings)\n",
    "decoder_in = positional_encoding(decoder_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-exception",
   "metadata": {},
   "source": [
    "Now let’s look deeper into the heart of the Transformer model: the Multi- Head Attention layer.\n",
    "## Multi-Head Attention\n",
    "- To understand how a **Multi-Head Attention layer** works, we must first understand the **Scaled Dot-Product Attention layer**, which it is based on.\n",
    "- Let’s suppose the encoder analyzed the input sentence “They played chess,” and it managed to understand that the word “They” is the subject and the word “played” is the verb, so it encoded this information in the representations of these words.\n",
    "- Now suppose the decoder has already translated the subject, and it thinks that it should translate the verb next. For this, it needs to fetch the verb from the input sentence.\n",
    "    - This is analog to a dictionary lookup: it’s as if the encoder created a dictionary {“subject”: “They”, “verb”: “played”, …} and the decoder wanted to look up the value that corresponds to the key “verb.”\n",
    "- However, the model does not have discrete tokens to represent the keys (like “subject” or “verb”); it has vectorized representations of these concepts (which it learned during training), so the key it will use for the lookup (called the query) will not perfectly match any key in the dictionary.\n",
    "    - The solution is to compute a similarity measure between the query and each key in the dictionary, and then use the softmax function to convert these similarity scores to weights that add up to 1.\n",
    "    - If the key that represents the verb is by far the most similar to the query, then that key’s weight will be close to 1.\n",
    "    - Then the model can compute a weighted sum of the corresponding values, so if the weight of the “verb” key is close to 1, then the weighted sum will be very close to the representation of the word “played.”\n",
    "- In short, you can think of this whole process as a differentiable dictionary lookup. \n",
    "    - The similarity measure used by the Transformer is just the dot product, like in Luong attention.\n",
    "\n",
    "Here is a (very) simplified Transformer (the actual architecture has skip connections, layer norm, dense nets, and most importantly it uses Multi-Head Attention instead of regular Attention):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verified-nylon",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = encoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, Z])\n",
    "\n",
    "encoder_outputs = Z\n",
    "Z = decoder_in\n",
    "for N in range(6):\n",
    "    Z = keras.layers.Attention(use_scale=True, causal=True)([Z, Z])\n",
    "    Z = keras.layers.Attention(use_scale=True)([Z, encoder_outputs])\n",
    "\n",
    "outputs = keras.layers.TimeDistributed(\n",
    "    keras.layers.Dense(vocab_size, activation=\"softmax\"))(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portuguese-guess",
   "metadata": {},
   "source": [
    "- The **use_scale=True** argument creates an additional parameter that lets the layer learn how to properly downscale the similarity scores. \n",
    "    - This is a bit different from the Transformer model, which always downscales the similarity scores by the same factor (√dkeys).\n",
    "- The **causal=True** argument when creating the second attention layer ensures that each output token only attends to previous output tokens, not future ones.\n",
    "\n",
    "Now it’s time to look at the final piece of the puzzle: what is a **Multi-Head Attention layer**? Its architecture is shown in Figure 16-10.\n",
    "\n",
    "<img src=\"16-10.png\">\n",
    "\n",
    "- As you can see, it is just a bunch of **Scaled Dot-Product Attention layers**, each preceded by a linear transformation of the values, keys, and queries (i.e., **a time-distributed Dense layer with no activation function**).\n",
    "- **All the outputs are simply concatenated, and they go through a final linear transformation (again, time-distributed).** But why? What is the intuition behind this architecture?\n",
    "- Well, consider the word “played” we discussed earlier (in the sentence “They played chess”).\n",
    "    - The encoder was smart enough to encode the fact that it is a verb. \n",
    "    - But the word representation also includes its position in the text, thanks to the positional encodings, and it probably includes many other features that are useful for its translation, such as the fact that it is in the past tense.\n",
    "- In short, the word representation encodes many different characteristics of the word.\n",
    "- **If we just used a single Scaled Dot-Product Attention layer, we would only be able to query all of these characteristics in one shot.**\n",
    "- **This is why the Multi-Head Attention layer applies multiple different linear transformations of the values, keys, and queries: this allows the model to apply many different projections of the word representation into different subspaces, each focusing on a subset of the word’s characteristics.**\n",
    "    - Perhaps one of the linear layers will project the word representation into a subspace where all that remains is the information that the word is a verb, another linear layer will extract just the fact that it is past tense, and so on.\n",
    "    - Then the Scaled Dot-Product Attention layers implement the lookup phase, and finally we concatenate all the results and project them back to the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-history",
   "metadata": {},
   "source": [
    "- Here's a basic implementation of the MultiHeadAttention layer.\n",
    "    -  Note that Conv1D layers with kernel_size=1 (and the default padding=\"valid\" and strides=1) is equivalent to a TimeDistributed(Dense(...)) layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "municipal-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class MultiHeadAttention(keras.layers.Layer):\n",
    "    def __init__(self, n_heads, causal=False, use_scale=False, **kwargs):\n",
    "        self.n_heads = n_heads\n",
    "        self.causal = causal\n",
    "        self.use_scale = use_scale\n",
    "        super().__init__(**kwargs)\n",
    "    def build(self, batch_input_shape):\n",
    "        self.dims = batch_input_shape[0][-1]\n",
    "        self.q_dims, self.v_dims, self.k_dims = [self.dims // self.n_heads] * 3 # could be hyperparameters instead\n",
    "        self.q_linear = keras.layers.Conv1D(self.n_heads * self.q_dims, kernel_size=1, use_bias=False)\n",
    "        self.v_linear = keras.layers.Conv1D(self.n_heads * self.v_dims, kernel_size=1, use_bias=False)\n",
    "        self.k_linear = keras.layers.Conv1D(self.n_heads * self.k_dims, kernel_size=1, use_bias=False)\n",
    "        self.attention = keras.layers.Attention(causal=self.causal, use_scale=self.use_scale)\n",
    "        self.out_linear = keras.layers.Conv1D(self.dims, kernel_size=1, use_bias=False)\n",
    "        super().build(batch_input_shape)\n",
    "    def _multi_head_linear(self, inputs, linear):\n",
    "        shape = K.concatenate([K.shape(inputs)[:-1], [self.n_heads, -1]])\n",
    "        projected = K.reshape(linear(inputs), shape)\n",
    "        perm = K.permute_dimensions(projected, [0, 2, 1, 3])\n",
    "        return K.reshape(perm, [shape[0] * self.n_heads, shape[1], -1])\n",
    "    def call(self, inputs):\n",
    "        q = inputs[0]\n",
    "        v = inputs[1]\n",
    "        k = inputs[2] if len(inputs) > 2 else v\n",
    "        shape = K.shape(q)\n",
    "        q_proj = self._multi_head_linear(q, self.q_linear)\n",
    "        v_proj = self._multi_head_linear(v, self.v_linear)\n",
    "        k_proj = self._multi_head_linear(k, self.k_linear)\n",
    "        multi_attended = self.attention([q_proj, v_proj, k_proj])\n",
    "        shape_attended = K.shape(multi_attended)\n",
    "        reshaped_attended = K.reshape(multi_attended, [shape[0], self.n_heads, shape_attended[1], shape_attended[2]])\n",
    "        perm = K.permute_dimensions(reshaped_attended, [0, 2, 1, 3])\n",
    "        concat = K.reshape(perm, [shape[0], shape_attended[1], -1])\n",
    "        return self.out_linear(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-drink",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.random.rand(2, 50, 512)\n",
    "V = np.random.rand(2, 80, 512)\n",
    "multi_attn = MultiHeadAttention(8)\n",
    "multi_attn([Q, V]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-addition",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "#### Exercise: Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from \"April 22, 2019\" to \"2019-04-22\").\n",
    "\n",
    "Let's start by creating the dataset. We will use random days between 1000-01-01 and 9999-12-31:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "excited-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "MONTHS = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\",\n",
    "         \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "\n",
    "def random_dates(n_dates):\n",
    "    min_date = date(1000, 1, 1).toordinal()\n",
    "    max_date = date(9999, 12, 31).toordinal()\n",
    "    \n",
    "    ordinals = np.random.randint(max_date - min_date, size=n_dates) + min_date\n",
    "    dates = [date.fromordinal(ordinal) for ordinal in ordinals]\n",
    "    \n",
    "    x = [MONTHS[dt.month - 1] + \" \" + dt.strftime(\"%d, %Y\") for dt in dates]\n",
    "    y = [dt.isoformat() for dt in dates]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-belarus",
   "metadata": {},
   "source": [
    "Here are a few random dates, displayed in both the input format and the target format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "indonesian-seminar",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input                    Target                   \n",
      "--------------------------------------------------\n",
      "September 20, 7075       7075-09-20               \n",
      "May 15, 8579             8579-05-15               \n",
      "January 11, 7103         7103-01-11               \n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "n_dates = 3\n",
    "x_example, y_example = random_dates(n_dates)\n",
    "print(\"{:25s}{:25s}\".format(\"Input\", \"Target\"))\n",
    "print(\"-\" * 50)\n",
    "for idx in range(n_dates):\n",
    "    print(\"{:25s}{:25s}\".format(x_example[idx], y_example[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "prescribed-operations",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ,0123456789ADFJMNOSabceghilmnoprstuvy'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_CHARS = \"\".join(sorted(set(\"\".join(MONTHS) + \"0123456789, \")))\n",
    "INPUT_CHARS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-romantic",
   "metadata": {},
   "source": [
    "And here's the list of possible characters in the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "computational-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHARS = \"0123456789-\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-deadline",
   "metadata": {},
   "source": [
    "Let's write a function to convert a string to a list of character IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "going-solution",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_str_to_ids(date_str, chars=INPUT_CHARS):\n",
    "    return [chars.index(c) for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "gentle-staff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19, 23, 31, 34, 23, 28, 21, 23, 32, 0, 4, 2, 1, 0, 9, 2, 9, 7]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(x_example[0], INPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "military-washer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 0, 7, 5, 10, 0, 9, 10, 2, 0]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_str_to_ids(y_example[0], OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "stock-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_date_strs(date_strs, chars=INPUT_CHARS):\n",
    "    X_ids = [date_str_to_ids(dt, chars) for dt in date_strs]\n",
    "    X = tf.ragged.constant(X_ids, ragged_rank=1)\n",
    "    return (X + 1).to_tensor() # using 0 as the padding token ID\n",
    "\n",
    "def create_dataset(n_dates):\n",
    "    x, y = random_dates(n_dates)\n",
    "    return prepare_date_strs(x, INPUT_CHARS), prepare_date_strs(y, OUTPUT_CHARS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "elect-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "X_train, Y_train = create_dataset(10000)\n",
    "X_valid, Y_valid = create_dataset(2000)\n",
    "X_test, Y_test = create_dataset(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "together-baghdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([ 8,  1,  8,  6, 11,  1, 10, 11,  3,  1])>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-surprise",
   "metadata": {},
   "source": [
    "### First version: a very basic seq2seq model\n",
    "Let's first try the simplest possible model: we feed in the input sequence, which first goes through the encoder (an embedding layer followed by a single LSTM layer), which outputs a vector, then it goes through a decoder (a single LSTM layer, followed by a dense output layer), which outputs a sequence of vectors, each representing the estimated probabilities for all possible output character.\n",
    "\n",
    "**Since the decoder expects a sequence as input, we repeat the vector (which is output by the decoder) as many times as the longest possible output sequence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "severe-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 32\n",
    "max_output_length = Y_train.shape[1]\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder = keras.models.Sequential([\n",
    "    keras.layers.Embedding(input_dim=len(INPUT_CHARS) + 1,\n",
    "                           output_dim=embedding_size,\n",
    "                           input_shape=[None]),\n",
    "    keras.layers.LSTM(128)\n",
    "])\n",
    "\n",
    "decoder = keras.models.Sequential([\n",
    "    keras.layers.LSTM(128, return_sequences=True),\n",
    "    keras.layers.Dense(len(OUTPUT_CHARS) + 1, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "automated-debate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "313/313 [==============================] - 23s 53ms/step - loss: 2.0809 - accuracy: 0.2640 - val_loss: 1.3734 - val_accuracy: 0.4943\n",
      "Epoch 2/20\n",
      "313/313 [==============================] - 14s 45ms/step - loss: 1.3178 - accuracy: 0.5161 - val_loss: 1.1401 - val_accuracy: 0.5863\n",
      "Epoch 3/20\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 1.0614 - accuracy: 0.6163 - val_loss: 0.9267 - val_accuracy: 0.6612\n",
      "Epoch 4/20\n",
      "313/313 [==============================] - 18s 59ms/step - loss: 0.9199 - accuracy: 0.6683 - val_loss: 0.7030 - val_accuracy: 0.7358\n",
      "Epoch 5/20\n",
      "313/313 [==============================] - 15s 47ms/step - loss: 0.7627 - accuracy: 0.7218 - val_loss: 0.5766 - val_accuracy: 0.7741\n",
      "Epoch 6/20\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 0.5083 - accuracy: 0.8003 - val_loss: 0.3713 - val_accuracy: 0.8566\n",
      "Epoch 7/20\n",
      "313/313 [==============================] - 17s 53ms/step - loss: 0.6312 - accuracy: 0.7784 - val_loss: 0.4727 - val_accuracy: 0.8357\n",
      "Epoch 8/20\n",
      "313/313 [==============================] - 19s 61ms/step - loss: 0.3840 - accuracy: 0.8669 - val_loss: 0.2545 - val_accuracy: 0.9133\n",
      "Epoch 9/20\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.2495 - accuracy: 0.9231 - val_loss: 0.1627 - val_accuracy: 0.9568\n",
      "Epoch 10/20\n",
      "313/313 [==============================] - 16s 50ms/step - loss: 0.1338 - accuracy: 0.9673 - val_loss: 0.0957 - val_accuracy: 0.9791\n",
      "Epoch 11/20\n",
      "313/313 [==============================] - 19s 60ms/step - loss: 0.0782 - accuracy: 0.9853 - val_loss: 0.0564 - val_accuracy: 0.9910\n",
      "Epoch 12/20\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0768 - accuracy: 0.9847 - val_loss: 0.0372 - val_accuracy: 0.9948\n",
      "Epoch 13/20\n",
      "313/313 [==============================] - 20s 62ms/step - loss: 0.0305 - accuracy: 0.9967 - val_loss: 0.0240 - val_accuracy: 0.9980\n",
      "Epoch 14/20\n",
      "313/313 [==============================] - 19s 62ms/step - loss: 0.0196 - accuracy: 0.9984 - val_loss: 0.0157 - val_accuracy: 0.9988\n",
      "Epoch 15/20\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.0172 - accuracy: 0.9983 - val_loss: 0.0197 - val_accuracy: 0.9981\n",
      "Epoch 16/20\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0127 - accuracy: 0.9995 - val_loss: 0.0085 - val_accuracy: 0.9996\n",
      "Epoch 17/20\n",
      "313/313 [==============================] - 20s 63ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 0.9999\n",
      "Epoch 18/20\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "313/313 [==============================] - 19s 59ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.RepeatVector(max_output_length),\n",
    "    decoder\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "                   validation_data=(X_valid, Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-closure",
   "metadata": {},
   "source": [
    "Looks great, we reach 100% validation accuracy! Let's use the model to make some predictions. We will need to be able to convert a sequence of character IDs to a readable string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "featured-imperial",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_date_strs(ids, chars=OUTPUT_CHARS):\n",
    "    return [\"\".join([(\"?\" + chars)[index] for index in sequence])\n",
    "            for sequence in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-vancouver",
   "metadata": {},
   "source": [
    "Now we can use the model to convert some dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "express-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"September 17, 2009\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "impressed-enlargement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009-09-17\n",
      "1789-07-14\n"
     ]
    }
   ],
   "source": [
    "#ids = model.predict_classes(X_new)\n",
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-while",
   "metadata": {},
   "source": [
    "Perfect! :)\n",
    "\n",
    "However, since the model was only trained on input strings of length 18 (which is the length of the longest date), it does not perform well if we try to use it to make predictions on shorter sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "harmful-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = prepare_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "swiss-canvas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-02-02\n",
      "1789-01-14\n"
     ]
    }
   ],
   "source": [
    "#ids = model.predict_classes(X_new)\n",
    "ids = np.argmax(model.predict(X_new), axis=-1)\n",
    "for date_str in ids_to_date_strs(ids):\n",
    "    print(date_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incoming-amount",
   "metadata": {},
   "source": [
    "Oops! We need to ensure that we always pass sequences of the same length as during training, using padding if necessary. Let's write a little helper function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "acute-westminster",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = X_train.shape[1]\n",
    "\n",
    "def prepare_date_strs_padded(date_strs):\n",
    "    X = prepare_date_strs(date_strs)\n",
    "    if X.shape[1] < max_input_length:\n",
    "        X = tf.pad(X, [[0, 0], [0, max_input_length - X.shape[1]]])\n",
    "    return X\n",
    "\n",
    "def convert_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    #ids = model.predict_classes(X)\n",
    "    ids = np.argmax(model.predict(X), axis=-1)\n",
    "    return ids_to_date_strs(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "corresponding-lewis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2020-05-02', '1789-07-14']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_date_strs([\"May 02, 2020\", \"July 14, 1789\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-bandwidth",
   "metadata": {},
   "source": [
    "Cool! Granted, there are certainly much easier ways to write a date conversion tool (e.g., using regular expressions or even basic string manipulation), but you have to admit that using neural networks is way cooler. ;-)\n",
    "\n",
    "However, real-life sequence-to-sequence problems will usually be harder, so for the sake of completeness, let's build a more powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-basement",
   "metadata": {},
   "source": [
    "### Second version: feeding the shifted targets to the decoder (teacher forcing)\n",
    "- Instead of feeding the decoder a simple repetition of the encoder's output vector, we can **feed it the target sequence, shifted by one time step to the right.** \n",
    "    - This way, at each time step the decoder will know what the previous target character was. \n",
    "    - This should help is tackle more complex sequence-to-sequence problems.\n",
    "- Since the first output character of each target sequence has no previous character, **we will need a new token to represent the start-of-sequence (sos).**\n",
    "- During inference, we won't know the target, so what will we feed the decoder? \n",
    "    - **We can just predict one character at a time, starting with an sos token, then feeding the decoder all the characters that were predicted so far.**\n",
    "- But if the decoder's LSTM expects to get the previous target as input at each step, how shall we pass it it the vector output by the encoder? \n",
    "    - Well, one option is to ignore the output vector, and instead use the encoder's LSTM state as the initial state of the decoder's LSTM (which requires that encoder's LSTM must have the same number of units as the decoder's LSTM).\n",
    "    \n",
    "Now let's create the decoder's inputs (for training, validation and testing). The sos token will be represented using the last possible output character's ID + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "automated-intent",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def shifted_output_sequences(Y):\n",
    "    sos_tokens = tf.fill(dims=(len(Y), 1), value=sos_id)\n",
    "    return tf.concat([sos_tokens, Y[:, :-1]], axis=1)\n",
    "\n",
    "X_train_decoder = shifted_output_sequences(Y_train)\n",
    "X_valid_decoder = shifted_output_sequences(Y_valid)\n",
    "X_test_decoder = shifted_output_sequences(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-victoria",
   "metadata": {},
   "source": [
    "Let's take a look at the decoder's training inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "conservative-imperial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000, 10), dtype=int32, numpy=\n",
       "array([[12,  8,  1, ..., 10, 11,  3],\n",
       "       [12,  9,  6, ...,  6, 11,  2],\n",
       "       [12,  8,  2, ...,  2, 11,  2],\n",
       "       ...,\n",
       "       [12, 10,  8, ...,  2, 11,  4],\n",
       "       [12,  2,  2, ...,  3, 11,  3],\n",
       "       [12,  8,  9, ...,  8, 11,  3]])>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-packaging",
   "metadata": {},
   "source": [
    "Now let's build the model. It's not a simple sequential model anymore, so let's use the **functional API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "offshore-atlantic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 22s 48ms/step - loss: 1.9374 - accuracy: 0.3113 - val_loss: 1.4042 - val_accuracy: 0.4663\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 12s 39ms/step - loss: 1.3114 - accuracy: 0.5053 - val_loss: 0.9136 - val_accuracy: 0.6647\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.7811 - accuracy: 0.7148 - val_loss: 0.3804 - val_accuracy: 0.8841\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.2892 - accuracy: 0.9189 - val_loss: 0.1546 - val_accuracy: 0.9653\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 12s 38ms/step - loss: 0.0936 - accuracy: 0.9868 - val_loss: 0.0441 - val_accuracy: 0.9984\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 0.0365 - accuracy: 0.9982 - val_loss: 0.1302 - val_accuracy: 0.9702\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 16s 51ms/step - loss: 0.0414 - accuracy: 0.9956 - val_loss: 0.0149 - val_accuracy: 0.9998\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 15s 48ms/step - loss: 0.0170 - accuracy: 0.9987 - val_loss: 0.0125 - val_accuracy: 0.9998\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.0095 - accuracy: 0.9999 - val_loss: 0.0072 - val_accuracy: 0.9999\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.0058 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 0.9999\n"
     ]
    }
   ],
   "source": [
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "lstm_units = 128\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "encoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(INPUT_CHARS) + 1,\n",
    "    output_dim=encoder_embedding_size)(encoder_input)\n",
    "_, encoder_state_h, encoder_state_c = keras.layers.LSTM(\n",
    "    lstm_units, return_state=True)(encoder_embedding)\n",
    "encoder_state = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_input = keras.layers.Input(shape=[None], dtype=tf.int32)\n",
    "decoder_embedding = keras.layers.Embedding(\n",
    "    input_dim=len(OUTPUT_CHARS) + 2,\n",
    "    output_dim=decoder_embedding_size)(decoder_input)\n",
    "decoder_lstm_output = keras.layers.LSTM(lstm_units, return_sequences=True)(\n",
    "    decoder_embedding, initial_state=encoder_state)\n",
    "decoder_output = keras.layers.Dense(len(OUTPUT_CHARS) + 1,\n",
    "                                    activation=\"softmax\")(decoder_lstm_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_input, decoder_input],\n",
    "                           outputs=[decoder_output])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=10,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-anderson",
   "metadata": {},
   "source": [
    "Let's once again use the model to make some predictions. This time we need to predict characters one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "convertible-guess",
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_id = len(OUTPUT_CHARS) + 1\n",
    "\n",
    "def predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = tf.fill(dims=(len(X), 1), value=sos_id)\n",
    "    for index in range(max_output_length):\n",
    "        pad_size = max_output_length - Y_pred.shape[1]\n",
    "        X_decoder = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n",
    "        Y_probas_next = model.predict([X, X_decoder])[:, index:index+1]\n",
    "        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n",
    "        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n",
    "    return ids_to_date_strs(Y_pred[:, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "hearing-armstrong",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-culture",
   "metadata": {},
   "source": [
    "Works fine!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weird-chain",
   "metadata": {},
   "source": [
    "### Third version: using TF-Addons's seq2seq implementation\n",
    "- Let's build exactly the same model, but using **TF-Addon's seq2seq API.** \n",
    "- The implementation below is almost very similar to the TFA example higher in this notebook, except without the model input to specify the output sequence length, for simplicity (but you can easily add it back in if you need it for your projects, when the output sequences have very different lengths)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "deadly-ecology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 22s 49ms/step - loss: 1.9192 - accuracy: 0.3118 - val_loss: 1.4655 - val_accuracy: 0.4414\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 16s 53ms/step - loss: 1.4373 - accuracy: 0.4338 - val_loss: 1.2389 - val_accuracy: 0.5117\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 18s 59ms/step - loss: 1.0971 - accuracy: 0.5844 - val_loss: 0.8649 - val_accuracy: 0.6661\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 20s 64ms/step - loss: 0.5611 - accuracy: 0.8031 - val_loss: 0.2242 - val_accuracy: 0.9461\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 19s 60ms/step - loss: 0.1810 - accuracy: 0.9636 - val_loss: 0.0713 - val_accuracy: 0.9945\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 0.0586 - accuracy: 0.9959 - val_loss: 0.0416 - val_accuracy: 0.9974\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 18s 58ms/step - loss: 0.0299 - accuracy: 0.9990 - val_loss: 0.0188 - val_accuracy: 0.9997\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 17s 56ms/step - loss: 0.0157 - accuracy: 0.9999 - val_loss: 0.0119 - val_accuracy: 0.9998\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 16s 52ms/step - loss: 0.0099 - accuracy: 1.0000 - val_loss: 1.5205 - val_accuracy: 0.7030\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 0.1868 - accuracy: 0.9619 - val_loss: 0.0078 - val_accuracy: 0.9999\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 19s 60ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0052 - val_accuracy: 0.9999\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 18s 57ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 18s 59ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0030 - val_accuracy: 1.0000\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 17s 54ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 17s 55ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "encoder_embedding_size = 32\n",
    "decoder_embedding_size = 32\n",
    "units = 128\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(\n",
    "    len(INPUT_CHARS) + 1, encoder_embedding_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(\n",
    "    len(OUTPUT_CHARS) + 2, decoder_embedding_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(len(OUTPUT_CHARS) + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,\n",
    "                                                 sampler,\n",
    "                                                 output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(\n",
    "    decoder_embeddings,\n",
    "    initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)\n",
    "\n",
    "model = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],\n",
    "                           outputs=[Y_proba])\n",
    "optimizer = keras.optimizers.Nadam()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "history = model.fit([X_train, X_train_decoder], Y_train, epochs=15,\n",
    "                    validation_data=([X_valid, X_valid_decoder], Y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-compensation",
   "metadata": {},
   "source": [
    "And once again, 100% validation accuracy! To use the model, we can just reuse the predict_date_strs() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "focused-delta",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-elephant",
   "metadata": {},
   "source": [
    "- However, there's a much more efficient way to perform inference. Until now, during inference, we've run the model once for each new character. \n",
    "- **Instead, we can create a new decoder, based on the previously trained layers, but using a GreedyEmbeddingSampler instead of a TrainingSampler.**\n",
    "    - At each time step, the GreedyEmbeddingSampler will compute the argmax of the decoder's outputs, and run the resulting token IDs through the decoder's embedding layer. \n",
    "    - Then it will feed the resulting embeddings to the decoder's LSTM cell at the next time step. \n",
    "    - This way, we only need to run the decoder once to get the full prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "final-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_sampler = tfa.seq2seq.sampler.GreedyEmbeddingSampler(\n",
    "    embedding_fn=decoder_embedding_layer)\n",
    "inference_decoder = tfa.seq2seq.basic_decoder.BasicDecoder(\n",
    "    decoder_cell, inference_sampler, output_layer=output_layer,\n",
    "    maximum_iterations=max_output_length)\n",
    "batch_size = tf.shape(encoder_inputs)[:1]\n",
    "start_tokens = tf.fill(dims=batch_size, value=sos_id)\n",
    "final_outputs, final_state, final_sequence_lengths = inference_decoder(\n",
    "    start_tokens,\n",
    "    initial_state=encoder_state,\n",
    "    start_tokens=start_tokens,\n",
    "    end_token=0)\n",
    "\n",
    "inference_model = keras.models.Model(inputs=[encoder_inputs],\n",
    "                                     outputs=[final_outputs.sample_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-reflection",
   "metadata": {},
   "source": [
    "- The GreedyEmbeddingSampler needs the start_tokens (a vector containing the start-of-sequence ID for each decoder sequence), and the end_token (the decoder will stop decoding a sequence once the model outputs this token).\n",
    "- We must set maximum_iterations when creating the BasicDecoder, or else it may run into an infinite loop (if the model never outputs the end token for at least one of the sequences). This would force you would to restart the Jupyter kernel.\n",
    "- The decoder inputs are not needed anymore, since all the decoder inputs are generated dynamically based on the outputs from the previous time step.\n",
    "- The model's outputs are **final_outputs.sample_id** instead of the softmax of final_outputs.rnn_outputs. \n",
    "    - This allows us to directly get the argmax of the model's outputs. \n",
    "    - If you prefer to have access to the logits, you can replace final_outputs.sample_id with final_outputs.rnn_outputs.\n",
    "    \n",
    "\n",
    "Now we can write a simple function that uses the model to perform the date format conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "polished-permit",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_predict_date_strs(date_strs):\n",
    "    X = prepare_date_strs_padded(date_strs)\n",
    "    Y_pred = inference_model.predict(X)\n",
    "    return ids_to_date_strs(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "impossible-greenhouse",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-07-14', '2020-05-01']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-tennis",
   "metadata": {},
   "source": [
    "Let's check that it really is faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "expressed-marks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714 ms ± 49.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "christian-turkey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61.8 ms ± 827 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_predict_date_strs([\"July 14, 1789\", \"May 01, 2020\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-circle",
   "metadata": {},
   "source": [
    "That's more than a 10x speedup! And it would be even more if we were handling longer sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-champagne",
   "metadata": {},
   "source": [
    "### Exercise: Use one of the recent language models (e.g., GPT) to generate more convincing Shakespearean text.\n",
    "\n",
    "The simplest way to use recent language models is to use the excellent transformers library, open sourced by Hugging Face. It provides many modern neural net architectures (including BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet and more) for Natural Language Processing (NLP), including many pretrained models. It relies on either TensorFlow or PyTorch. Best of all: it's amazingly simple to use.\n",
    "\n",
    "First, let's load a pretrained model. In this example, we will use OpenAI's GPT model, with an additional Language Model on top (just a linear layer with weights tied to the input embeddings). Let's import it and load the pretrained weights (this will download about 445MB of data to ~/.cache/torch/transformers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "assumed-processor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b4f76d4ee14e51b2c3b33a0cbd61d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/656 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ab060bdd28643e8b46b93bec84ca549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/466M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFOpenAIGPTLMHeadModel.\n",
      "\n",
      "All the layers of TFOpenAIGPTLMHeadModel were initialized from the model checkpoint at openai-gpt.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFOpenAIGPTLMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFOpenAIGPTLMHeadModel\n",
    "\n",
    "model = TFOpenAIGPTLMHeadModel.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-showcase",
   "metadata": {},
   "source": [
    "Next we will need a specialized tokenizer for this model. This one will try to use the spaCy and ftfy libraries if they are installed, or else it will fall back to BERT's BasicTokenizer followed by Byte-Pair Encoding (which should be fine for most use cases)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "buried-platinum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbcaff70b5b461496d54d9a93547c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/816k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d8f0aced5d439ea83ecf551d6fd71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/458k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c91a51fea4343c6ab05a85e714cad2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.27M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "from transformers import OpenAIGPTTokenizer\n",
    "\n",
    "tokenizer = OpenAIGPTTokenizer.from_pretrained(\"openai-gpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "surrounded-knock",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187]])>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_text = \"This royal throne of kings, this sceptred isle\"\n",
    "encoded_prompt = tokenizer.encode(prompt_text,\n",
    "                                  add_special_tokens=False,\n",
    "                                  return_tensors=\"tf\")\n",
    "encoded_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "split-indiana",
   "metadata": {},
   "source": [
    "Next, let's use the model to generate text after the prompt. We will generate 5 different sentences, each starting with the prompt text, followed by 40 additional tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "correct-november",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 50), dtype=int32, numpy=\n",
       "array([[  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   256,   603,  6151, 39276,   239,   256,  1168,\n",
       "          768,   481,  9897,   498,   481,  7352,   498,   481,  2761,\n",
       "          260,  9606,   240,   488,   606,   812,  1700,   704,  2436,\n",
       "          239,   562,   718,  1272,  1272,   896,   550,   643,   500,\n",
       "          509,   246,  7285,   500,   616],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   240,   568,  1359,   246,  6903,  6260,   815,   520,\n",
       "          488,  1158,   239, 40477,   806,   481, 17135, 11547,   999,\n",
       "          754,  4868,   485,   481,  4187,   240,   481, 33063,   641,\n",
       "          999,  1552,   485, 18471,   562,  2646,   239,   568,   645,\n",
       "          655,   509,   775,  1889,   485],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,  1598,  4103,   557,   246, 14973,  7346,   500,   481,\n",
       "          835,   498,  1615,  1668,   594,   240,   547,   945,   240,\n",
       "          488,  1076,  9690,  1218,   994, 37361,   481,  9823,   498,\n",
       "          481,  7455,   239,   256, 40477,   568,  8071,   558,   848,\n",
       "          485,   524,  5309,   500,   720],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,   239, 40477,   806,   544,   481,   618,   257,   895,\n",
       "          544,   487,   595,   793,   257,   256, 40477,   256,   487,\n",
       "         1040,   666, 18050,   240,   249,  1441,   239,   249,  3083,\n",
       "          487,   812,  2294,   240,   645,   487,   980,   694,  4457,\n",
       "          562,   239,   645,   620,   240],\n",
       "       [  616,  5751,  6404,   498,  9606,   240,   616, 26271,  7428,\n",
       "        16187,  1259,  1359,   604,   246,  7047,   239,   244, 40477,\n",
       "          244,   547,   618,   544,   770,   240,   244,   246,  1002,\n",
       "         3397,   551,   488, 29579,   535,   756,  3879,  2738,  1990,\n",
       "          481,   737,   498,   481,  2264,   239,   618, 17950,   249,\n",
       "         1137,   655,   240,  1081,   491]])>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_sequences = 5\n",
    "length = 40\n",
    "\n",
    "generated_sequences = model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    do_sample=True,\n",
    "    max_length=length + len(encoded_prompt[0]),\n",
    "    temperature=1.0,\n",
    "    top_k=0,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.0,\n",
    "    num_return_sequences=num_sequences,\n",
    ")\n",
    "\n",
    "generated_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "veterinary-cholesterol",
   "metadata": {},
   "source": [
    "Now let's decode the generated sequences and print them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "chemical-gentleman",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this royal throne of kings, this sceptred isle,'said sherzad.'give us the spells of the prisoner of the sea - kings, and we will answer your questions. for how many many dunedain was a wizard in this\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle, but also a realm greater than she and father. \n",
      " where the ruling fathers left their fate to the gods, the courtiers were left alone to fend for themselves. but if there was any chance to\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle may serve as a fortification in the face of velocent, my son, and those glorious years should precede the collapse of the empire.'\n",
      " but sparhawk had come to his senses in time\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle. \n",
      " where is the king? why is he not here?'\n",
      "'he went into exile, i believe. i expect he will return, if he has been cared for. if so,\n",
      "--------------------------------------------------------------------------------\n",
      "this royal throne of kings, this sceptred isle must also have a crown. \" \n",
      " \" my king is right, \" a voice rang out and halfdan's head jerked round towards the side of the hall. king lambi stood there, looking at\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for sequence in generated_sequences:\n",
    "    text = tokenizer.decode(sequence, clean_up_tokenization_spaces=True)\n",
    "    print(text)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-factory",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Natural Language Processing with RNNs and Attention.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
